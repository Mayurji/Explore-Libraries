{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Observability and evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arize-phoenix\n",
      "  Downloading arize_phoenix-3.16.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting ddsketch (from arize-phoenix)\n",
      "  Downloading ddsketch-2.0.4-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting hdbscan>=0.8.33 (from arize-phoenix)\n",
      "  Using cached hdbscan-0.8.33-cp39-cp39-linux_x86_64.whl\n",
      "Requirement already satisfied: jinja2 in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (3.0.1)\n",
      "Requirement already satisfied: numpy in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.22.4)\n",
      "Collecting openinference-instrumentation-langchain>=0.1.12 (from arize-phoenix)\n",
      "  Downloading openinference_instrumentation_langchain-0.1.12-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting openinference-instrumentation-llama-index>=1.2.0 (from arize-phoenix)\n",
      "  Downloading openinference_instrumentation_llama_index-1.2.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting openinference-instrumentation-openai>=0.1.4 (from arize-phoenix)\n",
      "  Downloading openinference_instrumentation_openai-0.1.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting openinference-semantic-conventions>=0.1.5 (from arize-phoenix)\n",
      "  Downloading openinference_semantic_conventions-0.1.5-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-proto in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.20.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.20.0)\n",
      "Requirement already satisfied: pandas in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.3.4)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.20 in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (3.20.3)\n",
      "Requirement already satisfied: psutil in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (14.0.2)\n",
      "Requirement already satisfied: requests in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (2.31.0)\n",
      "Requirement already satisfied: scikit-learn in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (0.24.2)\n",
      "Requirement already satisfied: scipy in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (1.7.1)\n",
      "Requirement already satisfied: sortedcontainers in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (2.4.0)\n",
      "Requirement already satisfied: starlette in /home/mayur/anaconda3/lib/python3.9/site-packages (from arize-phoenix) (0.27.0)\n",
      "Collecting strawberry-graphql==0.208.2 (from arize-phoenix)\n",
      "  Downloading strawberry_graphql-0.208.2-py3-none-any.whl.metadata (7.5 kB)\n"
     ]
    }
   ],
   "source": [
    "!pip install arize-phoenix\n",
    "#!pip install --upgrade scikit-learn\n",
    "#!pip install langchain\n",
    "#!pip install transformers\n",
    "#!pip install torch\n",
    "#!pip install pydantic==1.10.8\n",
    "#!pip install typing-inspect==0.8.0 typing_extensions==4.5.0\n",
    "!pip -q install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'phoenix'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20822/3089668015.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mphoenix\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mphoenix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLangChainInstrumentor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'phoenix'"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import langchain\n",
    "import transformers\n",
    "import os\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "import cohere\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "# Launch phoenix\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "# # By default, the traces will be exported to the locally running Phoenix server.\n",
    "LangChainInstrumentor().instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 64},\n",
    "    device_map='auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " president of the United States\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | hf\n",
    "\n",
    "question = \"Who is Obama?\"\n",
    "\n",
    "print(chain.invoke({\"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('all-data.csv', encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response                                             review\n",
       "0   neutral  According to Gran , the company has no plans t...\n",
       "1   neutral  Technopolis plans to develop in stages an area...\n",
       "2  negative  The international electronic industry company ...\n",
       "3  positive  With the new production plant the company woul...\n",
       "4  positive  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_CUSTOM_TEMPLATE = '''\n",
    "    You are evaluating the positive, negative and neutral of the responses to query.\n",
    "    [BEGIN DATA]\n",
    "    ************\n",
    "    [review]: {review}\n",
    "    ************\n",
    "    [Response]: {response}\n",
    "    [END DATA]\n",
    "\n",
    "\n",
    "    Please focus on the tone of the response.\n",
    "    Your answer must be single word, either \"positive\" or \"negative\"\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HuggingFacePipeline' object has no attribute 'default_concurrency'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9370/55303035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Will ensure the binary value expected from the template is returned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m relevance_classifications = llm_classify(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdataframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMY_CUSTOM_TEMPLATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ubuntu/Downloads/software/yes/lib/python3.9/site-packages/phoenix/experimental/evals/functions/classify.py\u001b[0m in \u001b[0;36mllm_classify\u001b[0;34m(dataframe, model, template, rails, system_instruction, verbose, use_function_calling_if_available, provide_explanation, include_prompt, include_response, run_sync, concurrency)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mparsed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mconcurrency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcurrency\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_concurrency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;31m# clients need to be reloaded to ensure that async evals work properly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HuggingFacePipeline' object has no attribute 'default_concurrency'"
     ]
    }
   ],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "    llm_classify\n",
    ")\n",
    "\n",
    "rails = ['neutral', 'positive', 'negative']\n",
    "#The rails is used to hold the output to specific values based on the template\n",
    "#It will remove text such as \",,,\" or \"...\"\n",
    "#Will ensure the binary value expected from the template is returned\n",
    "#rails = list(RAG_RELEVANCY_PROMPT_RAILS_MAP.values())\n",
    "relevance_classifications = llm_classify(\n",
    "    dataframe=df,\n",
    "    template=MY_CUSTOM_TEMPLATE,\n",
    "    model=hf,\n",
    "    rails=rails,\n",
    "    provide_explanation=True\n",
    ")\n",
    "#relevance_classifications is a Dataframe with columns 'label' and 'explanation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import llm_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_generate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
