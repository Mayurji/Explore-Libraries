{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is LLMs?\n",
    "\n",
    "- Sophisticated Deep Learning Models that excels in Natural Language Processing tasks.\n",
    "- It understands the complex patterns from the vast amount of training data to generate human like text.\n",
    "\n",
    "Words that goes along with LLMs\n",
    "- Encoder-Decoder Model (Transformer Model)\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*376uJu_fc_uR8H3X.png\" alt=\"encoder-decoder\" width=\"600\"/>\n",
    "\n",
    "\n",
    "- Autoregressive Model\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/371123751/figure/fig1/AS:11431281162474767@1685329470503/Autoregressive-sampling-The-LLM-is-sampled-to-generate-a-single-token-continuation-of.png\" alt=\"autoregressive\" width=\"600\">\n",
    "\n",
    "- Pretrained/Fine Tuned Models\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1102/1*hb6tJWEeeiwnVmzxMOPlrQ.png\" alt=\"pretrained\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Text Summarizer\n",
    "\n",
    "Facebook's BART:\n",
    "\n",
    "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and \\\n",
    "an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary \\\n",
    "noising function, and (2) learning a model to reconstruct the original text.\n",
    "\n",
    "BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation)\\\n",
    "but also works well for comprehension tasks (e.g. text classification, question answering). This \\\n",
    "particular checkpoint has been fine-tuned on CNN Daily Mail, a large collection of text-summary pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-01 11:59:15.725530: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-01 11:59:18.551447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def load_model():\n",
    "    model = pipeline(\"summarization\", device=0, model='facebook/bart-large-cnn')\n",
    "    return model\n",
    "\n",
    "def generate_chunks(inp_str):\n",
    "    max_chunk = 500\n",
    "    inp_str = inp_str.replace('.', '.<eos>')\n",
    "    inp_str = inp_str.replace('?', '?<eos>')\n",
    "    inp_str = inp_str.replace('!', '!<eos>')\n",
    "    \n",
    "    sentences = inp_str.split('<eos>')\n",
    "    current_chunk = 0 \n",
    "    chunks = []\n",
    "    for sentence in sentences:\n",
    "        if len(chunks) == current_chunk + 1: \n",
    "            if len(chunks[current_chunk]) + len(sentence.split(' ')) <= max_chunk:\n",
    "                chunks[current_chunk].extend(sentence.split(' '))\n",
    "            else:\n",
    "                current_chunk += 1\n",
    "                chunks.append(sentence.split(' '))\n",
    "        else:\n",
    "            chunks.append(sentence.split(' '))\n",
    "\n",
    "    for chunk_id in range(len(chunks)):\n",
    "        chunks[chunk_id] = ' '.join(chunks[chunk_id])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def text_summarizer(text):\n",
    "    updated_text = generate_chunks(text)\n",
    "    model = load_model()\n",
    "    summary = model(updated_text, max_length=750, min_length=30, do_sample=True)[0]['summary_text']\n",
    "    return summary\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 750, but your input_length is only 156. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=78)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Large Language Models are the key component behind text generation. They consist of large pretrained transformer models trained to predict the next word (or, more precisely, token) given some input text. Since they predict  one token at a time, you need to do something more elaborate to generate new sentences.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summarizer(\"\"\"LLMs, or Large Language Models, are the key component behind text generation. \n",
    "                In a nutshell, they consist of large pretrained transformer models trained to predict \n",
    "                the next word (or, more precisely, token) given some input text. Since they predict \n",
    "                one token at a time, you need to do something more elaborate to generate new sentences \n",
    "                other than just calling the model â€” you need to do autoregressive generation.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
