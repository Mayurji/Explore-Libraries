{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demystifying LLMs\n",
    "\n",
    "- RoBerta Model\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/358563215/figure/fig1/AS:1148929689305141@1650937592915/RoBERTa-masked-language-modeling-with-the-input-sentence-The-cat-is-eating-some-food.png\" alt=\"MLM\" width=\"700\">\n",
    "\n",
    "- It is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. \n",
    "- It was pretrained on the raw texts only, with no humans labelling them in any way with an automatic process to generate inputs and labels from those texts.\n",
    "- Masked Language Modeling\n",
    "- It uses MLM objective.\n",
    "    - It randomly masks 15% of the words in the input, then run the entire masked sentence through the model and has to predict the masked words.\n",
    "    - This is different from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n",
    "    - This way, the model learns an inner representation of the English language that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled sentences for instance, you can train a standard classifier using the features produced by the BERT model as inputs.\n",
    "\n",
    "- roberta-base-squad2 model\n",
    "    - It fine-tuned using the SQuAD2.0 dataset. It's been trained on question-answer pairs, including unanswerable questions, for the task of Question Answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.33065298199653625,\n",
       "  'token': 2943,\n",
       "  'token_str': ' male',\n",
       "  'sequence': \"Hello I'm a male model.\"},\n",
       " {'score': 0.0465540736913681,\n",
       "  'token': 2182,\n",
       "  'token_str': ' female',\n",
       "  'sequence': \"Hello I'm a female model.\"},\n",
       " {'score': 0.042330000549554825,\n",
       "  'token': 2038,\n",
       "  'token_str': ' professional',\n",
       "  'sequence': \"Hello I'm a professional model.\"},\n",
       " {'score': 0.03721684217453003,\n",
       "  'token': 2734,\n",
       "  'token_str': ' fashion',\n",
       "  'sequence': \"Hello I'm a fashion model.\"},\n",
       " {'score': 0.03253638744354248,\n",
       "  'token': 1083,\n",
       "  'token_str': ' Russian',\n",
       "  'sequence': \"Hello I'm a Russian model.\"}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='roberta-base')\n",
    "unmasker(\"Hello I'm a <mask> model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.2117147445678711, 'start': 59, 'end': 84, 'answer': 'gives freedom to the user'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "qa_input_1 = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(qa_input_1)\n",
    "print(res)\n",
    "# # b) Load model & tokenizer\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_input_2 = {'context' : \"\"\"\n",
    "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\",\n",
    "'question' : \"Which deep learning libraries back ü§ó Transformers?\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9416548013687134,\n",
       " 'start': 78,\n",
       " 'end': 105,\n",
       " 'answer': 'Jax, PyTorch and TensorFlow'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = nlp(qa_input_2)\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.5.5.tar.gz (2.5 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/mayur/anaconda3/lib/python3.9/site-packages (from flash-attn) (1.13.0)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /home/mayur/anaconda3/lib/python3.9/site-packages (from flash-attn) (23.2)\n",
      "Requirement already satisfied: ninja in /home/mayur/anaconda3/lib/python3.9/site-packages (from flash-attn) (1.11.1)\n",
      "Requirement already satisfied: typing-extensions in /home/mayur/anaconda3/lib/python3.9/site-packages (from torch->flash-attn) (4.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/mayur/anaconda3/lib/python3.9/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/mayur/anaconda3/lib/python3.9/site-packages (from torch->flash-attn) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/mayur/anaconda3/lib/python3.9/site-packages (from torch->flash-attn) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/mayur/anaconda3/lib/python3.9/site-packages (from torch->flash-attn) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/mayur/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (58.0.4)\n",
      "Requirement already satisfied: wheel in /home/mayur/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->flash-attn) (0.37.0)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m271.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.5.5-cp39-cp39-linux_x86_64.whl size=121949693 sha256=e44a2d0ca01cc68ce84596f869f70479c268f27ac078486c6c02cfcfb17c1697\n",
      "  Stored in directory: /home/mayur/.cache/pip/wheels/34/2c/5c/5eead9cd104af1ced6f6fa8c3b3b54d5369d5e832e5d724681\n",
      "Successfully built flash-attn\n",
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.7.0 flash-attn-2.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
