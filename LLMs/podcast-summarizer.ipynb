{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573d9036",
   "metadata": {},
   "source": [
    "### Build a Podcast Summarizer Using Python\n",
    "\n",
    "- Download Video\n",
    "- Extract Audio segment and save as MP3\n",
    "- Transcribe the Audio to text\n",
    "- Summarize the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d17fed",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f17cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install yt_dlp pydub transformers datasets torch\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcc216",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ddc474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayur/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-03 12:43:57.515776: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759475638.026323   66592 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1759475638.163727   66592 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1759475639.210084   66592 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759475639.210135   66592 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759475639.210137   66592 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1759475639.210139   66592 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-03 12:43:59.349104: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/mayur/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    }
   ],
   "source": [
    "import yt_dlp\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd514da",
   "metadata": {},
   "source": [
    "### Loading env variables into runtime environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8d10b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44675c4",
   "metadata": {},
   "source": [
    "### Connecting to Huggingface Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "023278db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# If running in a notebook environment\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "your_token = os.getenv(\"HUGGINGFACE_API_KEY\")  # Get your token from environment variables\n",
    "login(token=your_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5ca44",
   "metadata": {},
   "source": [
    "### Podcast URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ce0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.youtube.com/watch?v=9kB14zbtLOk\" #Hitler's first big mistake in World War II | Norman Ohler and Lex Fridman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2895e95",
   "metadata": {},
   "source": [
    "### Downloading the Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb362ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=9kB14zbtLOk\n",
      "[youtube] 9kB14zbtLOk: Downloading webpage\n",
      "[youtube] 9kB14zbtLOk: Downloading tv client config\n",
      "[youtube] 9kB14zbtLOk: Downloading tv player API JSON\n",
      "[youtube] 9kB14zbtLOk: Downloading web safari player API JSON\n",
      "[youtube] 9kB14zbtLOk: Downloading m3u8 information\n",
      "[info] 9kB14zbtLOk: Downloading 1 format(s): 251\n",
      "[download] podcast.webm has already been downloaded\n",
      "[download] 100% of    8.55MiB\n"
     ]
    }
   ],
   "source": [
    "opts = {\"format\": \"bestaudio/best\", \"outtmpl\": \"podcast.%(ext)s\"}\n",
    "with yt_dlp.YoutubeDL(opts) as ydl:\n",
    "    ydl.download([url])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b895b7",
   "metadata": {},
   "source": [
    "### Extracting Audio Segment from Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aa3901e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='podcast.mp3'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Convert to MP3\n",
    "audio = AudioSegment.from_file(\"podcast.webm\")\n",
    "audio.export(\"podcast.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b59fab1",
   "metadata": {},
   "source": [
    "### Transcribing text from Podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd10312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "/home/mayur/.local/lib/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Transcribe with open-source model\n",
    "asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-small\", return_timestamps=True)\n",
    "transcript = asr(\"podcast.mp3\")[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Deletion\"\"\"\n",
    "\n",
    "del asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a08f4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I see most of the officers of the Wehrmacht as not necessarily Nazis in the way that they would shout Heil Hitler all the time. They were highly intelligent, highly trained, super professionals that ran a very effective war machine. And at one point more and more of these generals realized that the orders that Hitler were were given were not really helping, you know, and they have their men dying because of it. So that creates a lot of tension. And that that led to the mistake that Hitler did in Dunkirk, basically. What Churchill called the sickle cut, which was the idea to storm through the Anden mountains and kind of cut off the British and French troops who were still in the north of Belgium trying to figure out what was going on. the Germans are behind them so that they kind of cut as a sickle into enemy territory, the sickle cut. That was so successful that basically the campaign was won already. So then the Germans invaded, like occupied all the cities on the canal back to England to kind of cut off the British completely so they couldn't even flee. But there was just, Dunkirk was open, the last port that was open and the German army was like, you know, they were already on the outskirts of Dunkirk. They could have just taken it and closed that, you know, that hole for the British military to get out. But Hitler then did his famous, and this is all the dynamic of the Western campaign, you know, a lot of things happen every day. And then they're saying like, we're going to have Dunkirk tomorrow and then it's over. And then Hitler stops the tanks. It's his famous Haltebefehl, the order to stop. And you know, they were all on meth. You know, they didn't want to stop. But Hitler was not on meth. Hitler was, he was, he basically, it was a little bit similar than Berlin Munich thing. Hitler didn't really understand that campaign. It was too fast for him. He, because they didn't say like, oh, they're all on meth, They're not going to sleep. They're going to behave erratically. They didn't discuss this. They discussed this in the old-fashioned terms. And Hitler was seeing like, they do not protect their flanks. What if the British come from the north? This is terrible. Like, militarily, it was... They were already fighting World War II, while Hitler was still fighting World War I, and especially the Allies, they were still fighting World War I. But the Tank Generals on math or the Tank Generals without math, the 10 generals per se, they were fighting a new type of war. And Hitler then got a visit from Göring, the head of the Air Force, the Luftwaffe. And Göring was a morphinist. That is very well documented. Like he was on morphine. He was high as a kite most of the time. And that comes with losing touch with reality, I would say. Or at least it changes your grip on reality. Maybe you're still a good decision maker, But it could lead to, you know, if you're intoxicated, let's say you're writing and you're intoxicated, you think it's great, but the next day you read it and it's shit, you know? So Göring was using morphine in the morning, then met Hitler at the Felsen Nest, which was Hitler's headquarters to command the Western campaign, the Felsen Nest. And Göring said to him, if the army generals are now gonna take Dunkirk, then basically the army has won this campaign. And that will give army high command, which is already against you because they were, you know? For them, Hitler was always like the, their Kleine Gefreite, the like the small kind of regular army guy, because that's what Hitler had been in the, in the first world war. And now suddenly he was the big decision maker. So they never, they thought they make much better decisions than him. So Göring says, their power will be so overwhelming they will for now uncalled the shots how this war will continue and what will be done next. You should let me with the Luftwaffe do the job from the air. The national socialist Luftwaffe is going to end the Western campaign. So he thought that he could destroy, it doesn't make sense, you know, even destroy the British military from Muth planes. Maybe you can do it, it, but certainly he couldn't do it. So the tank generals received the Haltzbefehl, the stopping order. They didn't believe it when they received it because the victory, this would have been complete victory over Great Britain. This would have been the end of Great Britain. The whole British military was encircled, but they did get out through Dunkirk. That's why the movie Dunkirk by Christopher Nolan is not good because he doesn't describe what happened on the German side. It's just this heroic British thing. Yeah, we just got out and we reformed and then we beat, you know, but this was just because Hitler was afraid of the power of his army high command and convinced by Göring's morphine high vision that he would stop it with the Air Force, which he couldn't, which he couldn't. I mean, he bombed and then the British, you know, they weren't ships and few ships were sunk. But basically they got out. You need to do this on the ground. at least back then you would have needed to do it on the ground. So that was a big mistake by Hitler. That's why von Mahnstein, one of the three tank generals from February 17th was Rommel von Mahnstein and Guderian. And von Mahnstein, he later said, he spoke of a Verlohrer-Nasig, a lost victory. He said the Western campaign was a lost victory because we really could have achieved the victory. We could have dominated, you know, Britain. They could have invaded Britain. There was no more military. Well, okay, on land. Where there was still the Royal Air Force. And the Navy. And the Navy, yeah. So like, so invading Britain, I think any invasion of actual Britain is a gigantic mistake on the Nazi part. But if Britain doesn't have a standing army anymore, it's much easier than we still have one. I think it's still extremely difficult to invade, but it's much easier to sort of neutralize, make sure that that Britain is not a player in the war. For sure. Maybe Hitler wouldn't have invaded at all, anyhow. Also because of his sort of not respect, but non-hatred of the British Empire. Because they also white supremacists. So why would we fight them? You know, that doesn't make sense. While the French, they were already like half black basically in Hitler's eyes. If we're to talk about counterfactual history of the possible trajectories of the war that would lead to Nazi victory, one of the big mistakes is the invasion of Britain. So you already mentioned the mistake with Dunkirk, but beyond that, if they even captured mainland Europe, Europe, they could have just neutralized the British threat and not invaded Britain. And then go after the oil, which is much needed maybe in the Middle East. So focus on that campaign before invading the Soviet Union. And then maybe wait for the Soviet Union to invade them through Poland, which will be likely coming, or wait until 1943, something like this, to invade East. without the Western front having to be been there. And the other really big mistake is declaring war against the United States. Having complete disrespect for the United States and declaring war against the United States. Which didn't have to be done at all. So it's collecting enemies when those didn't have to be done. So there is, to me actually, There's a lot of paths there as dark as it is to imagine for Nazi Germany to be successful in the invasion of the Soviet Union even. Well, I think that's why the Wehrmacht officers were pissed at Hitler because they knew that they could actually win if it was done in a certain way. But Hitler's ideology and his stupidity and later also his degeneration of his cognitive abilities did not allow the Wehrmacht to fight in the most effective way. So Hitler was a very bad leader after Dunkirk.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f4663",
   "metadata": {},
   "source": [
    "### Clearning GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36b3e32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "# 2c. Clear the GPU memory cache and run garbage collection\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88630772",
   "metadata": {},
   "source": [
    "### Summarize the transcribed text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a089ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 4: Summarize with open-source model\n",
    "# summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device='cpu')\n",
    "# summary = summarizer(transcript, max_length=150, min_length=40, do_sample=False)\n",
    "\n",
    "# print(summary[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6934f8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1761 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript size: 1761 tokens. Creating 2 chunks.\n",
      "\n",
      "--- Performing Final Reduction Summary ---\n",
      "\n",
      "--- Final Podcast Summary ---\n",
      " I see most of the officers of the Wehrmacht as not necessarily Nazis in the way that they would shout Heil Hitler all the time . They were highly intelligent, highly trained, super professionals that ran a very effective war machine . But at one point more and more of these generals realized that the orders that Hitler were given were not really helping, you know, and they have their men dying because of it . The movie Dunkirk by Christopher Nolan is not good because he doesn't describe what happened on the German side .\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import math\n",
    "\n",
    "# --- Configuration for DistilBART ---\n",
    "MODEL_NAME = \"sshleifer/distilbart-cnn-12-6\" # <-- Updated Model Name\n",
    "MAX_INPUT_LENGTH = 1024                     # Max tokens DistilBART can handle\n",
    "OVERLAP = 200                               # Overlap tokens between chunks\n",
    "DEVICE = \"cpu\" #\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load Tokenizer and Model\n",
    "# Note: DistilBART uses the standard BART tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME) \n",
    "\n",
    "# Load the DistilBART model in half-precision (FP16)\n",
    "model = BartForConditionalGeneration.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16  # Highly recommended for 8GB VRAM\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "def chunk_and_summarize(text: str, model, tokenizer, max_len: int = MAX_INPUT_LENGTH, overlap: int = OVERLAP) -> str:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks, summarizes each chunk, and returns\n",
    "    the combined summaries as a single string (the 'Meta-Document').\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Tokenize the input text\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True).to(DEVICE)\n",
    "    \n",
    "    # Calculate the step size and number of chunks\n",
    "    input_size = input_ids.size(1)\n",
    "    step = max_len - overlap\n",
    "    \n",
    "    # Determine the number of chunks needed\n",
    "    num_chunks = math.ceil((input_size - overlap) / step)\n",
    "\n",
    "    chunk_summaries = []\n",
    "\n",
    "    print(f\"Transcript size: {input_size} tokens. Creating {num_chunks} chunks.\")\n",
    "\n",
    "    # 2. Iterate through chunks and summarize (MAP phase)\n",
    "    for i in range(num_chunks):\n",
    "        start = i * step\n",
    "        end = min(start + max_len, input_size)\n",
    "        \n",
    "        chunk_input_ids = input_ids[:, start:end]\n",
    "        \n",
    "        # Perform generation (summarization) for the chunk\n",
    "        summary_ids = model.generate(\n",
    "            chunk_input_ids,\n",
    "            num_beams=4,\n",
    "            max_length=200, # Max length for individual chunk summaries\n",
    "            min_length=50,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        summary_text = tokenizer.decode(summary_ids.squeeze(), skip_special_tokens=True)\n",
    "        chunk_summaries.append(summary_text)\n",
    "\n",
    "        if end >= input_size:\n",
    "            break\n",
    "\n",
    "    # 3. Combine chunk summaries\n",
    "    combined_summaries = \" \".join(chunk_summaries)\n",
    "    \n",
    "    return combined_summaries\n",
    "\n",
    "def final_summarize(text: str, model, tokenizer) -> str:\n",
    "    \"\"\"\n",
    "    Performs the final summarization on the combined chunk summaries (REDUCE phase).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Performing Final Reduction Summary ---\")\n",
    "    \n",
    "    # Encode the combined summaries (Truncate if the combined text is still too long)\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(DEVICE)\n",
    "    \n",
    "    # Generate the final summary\n",
    "    final_summary_ids = model.generate(\n",
    "        input_ids,\n",
    "        num_beams=4,\n",
    "        max_length=500, # Max length for the final podcast summary\n",
    "        min_length=100,\n",
    "        length_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    final_summary = tokenizer.decode(final_summary_ids.squeeze(), skip_special_tokens=True)\n",
    "    return final_summary\n",
    "\n",
    "\n",
    "# --- Example Execution ---\n",
    "# NOTE: Replace this with the actual long transcript you get from Whisper!\n",
    "podcast_transcript = transcript\n",
    "\n",
    "# 1. Map: Chunk and Summarize\n",
    "meta_document = chunk_and_summarize(podcast_transcript, model, tokenizer)\n",
    "\n",
    "# 2. Reduce: Final Summary\n",
    "final_summary_text = final_summarize(meta_document, model, tokenizer)\n",
    "\n",
    "print(\"\\n--- Final Podcast Summary ---\")\n",
    "print(final_summary_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b149d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
