{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WVRcRlcBiCO"
      },
      "source": [
        "Reference: [HuggingFace Advance Tutorial On RAG](https://huggingface.co/learn/cookbook/en/advanced_rag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVnfIyHFwNb5"
      },
      "source": [
        "# Advance RAG Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nInVLaXwKJi"
      },
      "source": [
        "![](https://cdn-lfs-us-1.huggingface.co/repos/13/3d/133d8ca2460bf82ba2bdbe928d91a6c780364a6d0cf9005087db081cca492c02/ed22547b1538ea4fd18ea26777e14d9f7e51b3388b34d3cadf165cc37a7f63e0?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27RAG_workflow.png%3B+filename%3D%22RAG_workflow.png%22%3B&response-content-type=image%2Fpng&Expires=1709467466&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwOTQ2NzQ2Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzEzLzNkLzEzM2Q4Y2EyNDYwYmY4MmJhMmJkYmU5MjhkOTFhNmM3ODAzNjRhNmQwY2Y5MDA1MDg3ZGIwODFjY2E0OTJjMDIvZWQyMjU0N2IxNTM4ZWE0ZmQxOGVhMjY3NzdlMTRkOWY3ZTUxYjMzODhiMzRkM2NhZGYxNjVjYzM3YTdmNjNlMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=kW2J%7EyjYZ5Yb0uiWxYIR%7EbNNWUnmhF6vCWwsEtKSzLAVK6PRHW%7El0SmQoz0U%7EFlYd-FsEQcw8lhQ-lT06q1u4QCtF8EnanqZCngeQOYbH-bpFZ59OGMJiZwu8hbgiP6vr5x81DLi3FaDFXLSrra52Jqpctiylivism49Kf1kjTd6H7vl8ZywuMLl4O-rchqPT5oJ-r5fY9nmXsN8PWJfVBFKGzJovQjBTgFjwL8ydaZhjCOu2WgIWuFELys8XnDda2HV1kjZY2b1miqBDudCg3zb3Xvd3P-BXAScxzwUbdB5Ju2vZhHf31uNwEapFxvnj4YW7nGC1%7EeCKdEi%7EvprgQ__&Key-Pair-Id=KCD77M1F0VK2B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qart5SmTGhND"
      },
      "source": [
        "### Installing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fK425mODBiCP"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap plotly datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIvN_hlcGtAh"
      },
      "source": [
        "### Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "2VQCdgGtBiCQ"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)  # this will be helpful when visualizing retriever outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKjDwGuMHJBA"
      },
      "source": [
        "### Download the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "qfDIQgkDBiCQ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTLGwaRVHPhf"
      },
      "source": [
        "### Langchain Document for Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8fbfacf1b3ba454cba690b0fd1087012",
            "4fbcc6df7a9d4fb091c9f673d32c476f",
            "f021740d3e9f438c867023a8504523c6",
            "458f8bfc1e784b6aadb05180b0eef8c3",
            "9c5c295246fe40ddab724c7daaa4bcd0",
            "5951f1d595d74934925cc69c03b3afd4",
            "fba9612e9d6c447780616271e0060366",
            "ed142b9f14e54ce59a6bd4de39c9187d",
            "a741439b9ee54bea9c9088594e74e0bc",
            "018bec5629f84580a819be71322a156f",
            "4bf05beed2754fe0887326f4b3368299"
          ]
        },
        "id": "TJ75uC__BiCQ",
        "outputId": "83c32294-6377-4c90-b736-a24171668045"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fbfacf1b3ba454cba690b0fd1087012",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2647 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for i, doc in enumerate(tqdm(ds)) if i <= 500\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atOkHVAByQMY",
        "outputId": "b57f0352-ce6c-42bc-fec0-e0b93360d720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content=' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget üèÅ üéâ!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n', metadata={'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'})"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "RAW_KNOWLEDGE_BASE[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoThxvy9Hlar"
      },
      "source": [
        "### Chunking/Text Splitter\n",
        "\n",
        "\"000000000000000000011111111111111112222222222222233333333333333344444444444444444444\"\n",
        "\n",
        "\"0000000000000000000111111111111111122222222222222\" - 50 characters\n",
        "\n",
        "\"2222222222222233333333333333344444444444444444444\" - 50 characters with overlapping \"22222222222222\" from previous chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "eKtgDi23BiCQ"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n",
        "# This list is taken from LangChain's MarkdownTextSplitter class.\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # the maximum number of characters in a chunk: we selected this value arbitrarily\n",
        "    chunk_overlap=100,  # the number of characters to overlap between chunks\n",
        "    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n",
        "    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n",
        "    separators=MARKDOWN_SEPARATORS,\n",
        ")\n",
        "\n",
        "docs_processed = []\n",
        "for doc in RAW_KNOWLEDGE_BASE:\n",
        "    docs_processed += text_splitter.split_documents([doc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVVpiEJyJyIA",
        "outputId": "5580c4a4-9171-4c3e-8831-8a1a55d38a84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "501"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(RAW_KNOWLEDGE_BASE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "-gw9GVkgJoIN"
      },
      "outputs": [],
      "source": [
        "len_doc = [len(d.page_content) for d in docs_processed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDMC51xUKJH2",
        "outputId": "bcdb3da2-107d-4f4a-e5e0-007339a55191"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "965"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs_processed[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501,
          "referenced_widgets": [
            "0aa461a279cb4427b1e50e873c2e7c92",
            "b7742df54511423d85f3fa278b0568b7",
            "354a251aae9743e187f8e81e216c93ba",
            "711a19d1e71e422ca30f159b43a95346",
            "269f46e2ae65490bb0979b3ba2feac59",
            "1c5e32c5684a438d9beebf53d6dca44d",
            "539ab16f96904190a96c24e4887e4720",
            "c113cb1af7334bd9a9d8d9b70fd54144",
            "ac7efd811a614e62b792f358592df303",
            "f8643c56e10344e1a9f604bb53c94f5c",
            "a2d67e62b6124e81bbc8b89d36940e50"
          ]
        },
        "id": "VIhjfS5zBiCQ",
        "outputId": "ee8a1107-6692-4232-96bb-dc4467482a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model's maximum sequence length: 512\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0aa461a279cb4427b1e50e873c2e7c92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/6250 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter.\n",
        "print(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\")\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "\n",
        "# Plot the distrubution of document lengths, counted as the number of tokens\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484,
          "referenced_widgets": [
            "05aad73dab3e47ddb2ccfe58eaba1ab2",
            "3dcd42cbd0c349f5acc0ff6721747935",
            "7e4323870dd74937bc70177055a6e6ab",
            "333bd4d5514a4a13ba2625bf6462746f",
            "5c45d478e80c42c9af0d9dd69e2239e1",
            "9640cf54b16d47d48149585c581eedec",
            "9f6d70f8b35643f285f76e9f308dab23",
            "5738b57325e844cca7c503e4740a686f",
            "a36888daf90846c9ba578e21feb8148a",
            "8a615eb5f34240a3a778c1019065c4f7",
            "f27c8431cb2a4c7aba2eedc7804d59da"
          ]
        },
        "id": "vtUK2kgABiCR",
        "outputId": "d1314ad9-cc05-4cc0-8d60-70f27ab91e01"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05aad73dab3e47ddb2ccfe58eaba1ab2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4041 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=MARKDOWN_SEPARATORS,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in knowledge_base:\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in docs_processed:\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")\n",
        "\n",
        "# Let's visualize the chunk sizes we would have in tokens from a common model\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]\n",
        "fig = pd.Series(lengths).hist()\n",
        "plt.title(\"Distribution of document lengths in the knowledge base (in count of tokens)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obj2735y1mDS"
      },
      "outputs": [],
      "source": [
        "tokenizer.encode(docs_processed[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "OCBpYKtzBiCR"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # set True for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "vzi7FxSFBiCR"
      },
      "outputs": [],
      "source": [
        "# embed a user query in the same space\n",
        "user_query = \"How to create a pipeline object?\"\n",
        "query_vector = embedding_model.embed_query(user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIzfp75u0wf6"
      },
      "outputs": [],
      "source": [
        "query_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcmBDyGzBiCR",
        "outputId": "9404982d-9fa6-476b-de3a-bd4535c45965"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pacmap/pacmap.py:828: UserWarning:\n",
            "\n",
            "Warning: random state is set to 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "embedding_projector = pacmap.PaCMAP(n_components=2, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, random_state=1)\n",
        "\n",
        "embeddings_2d = [\n",
        "    list(KNOWLEDGE_VECTOR_DATABASE.index.reconstruct_n(idx, 1)[0]) for idx in range(len(docs_processed))\n",
        "] + [query_vector]\n",
        "\n",
        "# fit the data (The index of transformed data corresponds to the index of the original data)\n",
        "documents_projected = embedding_projector.fit_transform(np.array(embeddings_2d), init=\"pca\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "id": "oyzP14ECBiCR",
        "outputId": "5cb06487-428f-49eb-e5ef-d8f4e4809e49"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ec98e91e-37f4-4bac-bfab-07fdc609f553\" class=\"plotly-graph-div\" style=\"height:700px; width:1000px;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ec98e91e-37f4-4bac-bfab-07fdc609f553\")) {                    Plotly.newPlot(                        \"ec98e91e-37f4-4bac-bfab-07fdc609f553\",                        [{\"customdata\":[[\"Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](htt...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fhf-endpoints-documentation\\u002fmain\\u002fassets\\u002f1_cre...\"],[\"Access and read Logs\\n\\nHugging Face Endpoints provides access to the logs of your Endpoints through t...\"],[\"Hugging Face Inference Endpoints documentation\\n\\n## Setup\\n\\n```bash\\npip install hf-doc-builder==0.4.0 ...\"],[\"Pricing\\n\\n\\u003cdiv class=\\\"flex md:justify-start mb-2 text-gray-400 items-center\\\"\\u003e\\n  \\u003ca href=\\\"https:\\u002f\\u002fui.e...\"],[\"## CPU Instances\\n\\nThe table below shows currently available CPU instances and their hourly pricing. ...\"],[\"## GPU Instances\\n\\nThe table below shows currently available GPU instances and their hourly pricing. ...\"],[\"```\\ninstance hourly rate * ((hours * # min replica) + (scale-up hrs * # additional replicas))\\n```\\n\\n#...\"],[\"Supported Transformers & Diffusers Tasks\\n\\nInference Endpoints offers out-of-the-box support for Mach...\"],[\"```\\n\\n### Text Classification\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the s...\"],[\"```\\n\\n### Text Generation\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"This sound track was beautiful! It paints the sener...\"],[\"```\\n\\n**Binary**\\n```bash\\ncurl --request POST \\\\\\n  --url https:\\u002f\\u002f{ENDPOINT}\\u002f \\\\\\n  --header 'Content-Type...\"],[\"```\\n\\n**Binary**\\n\\n```bash\\ncurl --request POST \\\\\\n  --url https:\\u002f\\u002f{ENDPOINT}\\u002f \\\\\\n  --header 'Content-Typ...\"],[\"```\\n\\n\\n### Additional parameters\\n\\nYou can add additional parameters, which are supported by the `pipe...\"],[\"Access and view Metrics\\n\\nHugging Face Endpoints provides access to the metrics and analytics of your...\"]],\"hovertemplate\":\"source=hf-endpoints-documentation\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hf-endpoints-documentation, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hf-endpoints-documentation, circle\",\"showlegend\":true,\"x\":[0.4424069,0.54181397,0.9715669,0.90865827,-0.49623573,-0.12908152,-0.8010803,-0.63330203,-3.1909542,-6.527479,-6.9283295,3.462539,3.310946,-6.3644285,0.5783904],\"xaxis\":\"x\",\"y\":[-1.7916015,-1.7808515,-2.0515025,-2.1873248,-1.3339281,-2.5446255,-2.2343605,-2.158833,-1.0641443,-3.4494512,-3.5030432,-1.596528,-1.6182945,-3.7435741,-1.8380047],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Choosing a metric for your task\\n\\n**So you've trained your model and want to see how well it‚Äôs doing ...\"],[\"```\\n\\u003e\\u003e\\u003e precision_metric = evaluate.load(\\\"precision\\\")\\n\\u003e\\u003e\\u003e results = precision_metric.compute(referen...\"],[\"```\\n\\n### Task-specific metrics\\n\\nPopular ML tasks like Machine Translation and Named Entity Recogniti...\"],[\"\\u003cTip warning={true}\\u003e\\nüí°\\nGLUE is actually a collection of different subsets on different tasks, so fir...\"],[\"```\\n\\u003e\\u003e\\u003e from evaluate import load\\n\\u003e\\u003e\\u003e squad_metric = load(\\\"squad\\\")\\n\\u003e\\u003e\\u003e predictions = [{'prediction_t...\"],[\"--\\ntitle: poseval\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: a...\"],[\"`references`: a list of lists of reference labels, i.e. the ground truth\\u002ftarget values.\\n\\nIt can also...\"],[\"```\\n\\n## Output values\\n\\nThis metric returns a a classification report as a dictionary with a summary ...\"],[\"`f1`: the average [F1 score](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002ff1), on a scale between 0.0 and 1.0.\\n\\n\\n#...\"],[\"```\\n\\n## Limitations and bias\\n\\nIn contrast to [seqeval](https:\\u002f\\u002fgithub.com\\u002fchakki-works\\u002fseqeval), the...\"],[\"--\\ntitle: MAPE\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app....\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mape': array([0.5, 1. ])}\\n```\\n\\n#### Values from Popu...\"],[\"```\\n\\n## Limitations and Bias\\nOne limitation of MAPE is that it cannot be used if the ground truth is...\"],[\"--\\ntitle: ROUGE\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app...\"],[\"```\\n\\nOne can also pass a custom tokenizer which is especially useful for non-latin languages.\\n```pyt...\"],[\"```\\n```\\n\\n### Inputs\\n- **predictions** (`list`): list of predictions to score. Each prediction\\n      ...\"],[\"```\\n\\nThe ROUGE values are in the range of 0 to 1.\\n\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nA...\"],[\"```\\n\\n## Limitations and Bias\\nSee [Schluter (2017)](https:\\u002f\\u002faclanthology.org\\u002fE17-2007\\u002f) for an in-dep...\"],[\"--\\ntitle: Word Length\\nemoji: ü§ó\\ncolorFrom: green\\ncolorTo: purple\\nsdk: gradio\\nsdk_version: 3.0.2\\napp_f...\"],[\"```\\n\\nThis metric outputs a dictionary containing the number of words in the input string (`word leng...\"],[\"Working with Keras and Tensorflow\\n\\n\\n\\nEvaluate can be easily intergrated into your Keras and Tensorfl...\"],[\"x_train = np.expand_dims(x_train, -1)\\nx_test = np.expand_dims(x_test, -1)\\n\\n\\nmodel = keras.Sequential...\"],[\"```\\n\\n## Callbacks\\n\\nSuppose we want to keep track of model metrics while a model is training. We can ...\"],[\"```\\n\\n## Using an Evaluate Metric for... Evaluation!\\n\\nWe can also use the same metric after model tra...\"],[\"--\\ntitle: CharCut\\nemoji: üî§\\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ap...\"],[\"```\\n\\n## Citation\\n```bibtex\\n@inproceedings{lardilleux-lepage-2017-charcut,\\n    title = \\\"{CHARCUT}: Hu...\"],[\"--\\ntitle: IndicGLUE\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n    \\n## Output values\\n\\nThe output of the metric depends on the IndicGLUE subset chosen, consisti...\"],[\"```\\n\\nMinimal values for the Wiki-NER subset (which outputs `accuracy` and `f1`):\\n\\n```python\\n\\u003e\\u003e\\u003e indi...\"],[\"```\\n    \\n## Further References \\n- [IndicNLP website](https:\\u002f\\u002findicnlp.ai4bharat.org\\u002fhome\\u002f)...\"],[\"--\\ntitle: Google BLEU\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_fil...\"],[\"The minimum value of precision and recall is then returned as the score.\\n\\n\\n## Intended Uses\\nThis met...\"],[\"```\\n\\n### Inputs\\n- **predictions** (list of str): list of translations to score.\\n- **references** (li...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\n\\n### Examples\\nExample with one reference per sample:\\n```python...\"],[\"```\\n\\nExample with multiple references for the first sample, and with `min_len` adjusted to `2`, inst...\"],[\"```\\n\\nExample with multiple references for the first sample, with `min_len` adjusted to `2`, instead ...\"],[\"```\\n\\n## Limitations and Bias\\n\\nThe GoogleBLEU metric does not come with a predefined tokenization fun...\"],[\"--\\ntitle: \\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: app.py\\np...\"],[\"```\\n\\nFrugalScore calculates how good are the predictions given some references, based on a set of sc...\"],[\"```\\n\\nPartial values: \\n\\n```python\\n\\u003e\\u003e\\u003e frugalscore = evaluate.load(\\\"frugalscore\\\")\\n\\u003e\\u003e\\u003e results = frugal...\"],[\"```\\n\\n## Limitations and bias\\n\\nFrugalScore is based on [BertScore](https:\\u002f\\u002fhuggingface.co\\u002fmetrics\\u002fber...\"],[\"| FrugalScore                                        | Student     | Teacher        | Method     |\\n|...\"],[\"| [moussaKam\\u002ffrugalscore_medium_roberta_bert-score](https:\\u002f\\u002fhuggingface.co\\u002fmoussaKam\\u002ffrugalscore_med...\"],[\"Depending on the size of the model picked, the loading time will vary: the `tiny` models will load v...\"],[\"```\\n\\n## Further References\\n- [Original FrugalScore code](https:\\u002f\\u002fgithub.com\\u002fmoussaKam\\u002fFrugalScore)\\n-...\"],[\"--\\ntitle: Mean IoU\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file: ...\"],[\"```\\n\\n### Inputs\\n**Mandatory inputs**\\n- `predictions` (`List[ndarray]`): List of predicted segmentati...\"],[\"The values of all of the scores reported range from from `0.0` (minimum) and `1.0` (maximum).\\n\\nOutpu...\"],[\"```\\n\\n#### Values from Popular Papers\\n\\nThe [leaderboard for the CityScapes dataset](https:\\u002f\\u002fpaperswit...\"],[\"### Examples\\n\\n```python\\n\\u003e\\u003e\\u003e import numpy as np\\n\\u003e\\u003e\\u003e mean_iou = evaluate.load(\\\"mean_iou\\\")\\n\\u003e\\u003e\\u003e # suppos...\"],[\"## Limitations and Bias\\nMean IOU is an average metric, so it will not show you where model predictio...\"],[\"```\\n\\n\\n## Further References\\n- [Wikipedia article - Jaccard Index](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fJacc...\"],[\"--\\ntitle: SuperGLUE\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"2. **Calculating the metric**: the metric takes two inputs : one list with the predictions of the mo...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the SuperGLUE subset chosen, consisting of...\"],[\"```\\n\\nMinimal values for the MultiRC subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfro...\"],[\"```\\n\\n## Limitations and bias\\nThis metric works only with datasets that have the same format as the [...\"],[\"ü§ó Transformers\\n\\nTo run the ü§ó Transformers examples make sure you have installed the following librar...\"],[\"```\\n\\n## Trainer\\n\\nThe metrics in `evaluate` can be easily integrated with the [`~transformers.Trainer...\"],[\"trainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=small_train_dataset,\\n ...\"],[\"```\\n\\n## Seq2SeqTrainer\\n\\nWe can use the [`~transformers.Seq2SeqTrainer`] for sequence-to-sequence tas...\"],[\"model_inputs[\\\"labels\\\"] = labels[\\\"input_ids\\\"]\\n    return model_inputs\\n\\ntokenized_billsum = billsum.ma...\"],[\"trainer = Seq2SeqTrainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=tokenized_bills...\"],[\"```\\n\\nYou can use any `evaluate` metric with the `Trainer` and `Seq2SeqTrainer` as long as they are c...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nYou can adapt the `--build_dir` to set any temporary folder that you prefer. This command will ...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\nFo...\"],[\"```\\n## XXXConfig\\n\\n[[autodoc]] XXXConfig\\n```\\n\\nThis will include every public method of the configurat...\"],[\"```\\n## XXXTokenizer\\n\\n[[autodoc]] XXXTokenizer\\n    - all\\n    - __call__\\n```\\n\\n### Writing source docum...\"],[\"```\\n\\nIf the description is too long to fit in one line, another indentation is necessary before writ...\"],[\"```\\n```\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\nWe follow the [doctest](https:\\u002f\\u002fdocs.pyth...\"],[\"```\\n\\n#### Adding an image\\n\\nDue to the rapidly growing repository, it is important to make sure that ...\"],[\"--\\ntitle: Spearman Correlation Coefficient Metric \\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradi...\"],[\"## How to Use\\nAt minimum, this metric only requires a `list` of predictions and a `list` of referenc...\"],[\"```\\n\\n### Inputs\\n- **`predictions`** (`list` of `float`): Predicted labels, as returned by a model.\\n-...\"],[\"```\\n\\nThe same example, but that also returns the pvalue:\\n```python\\n\\u003e\\u003e\\u003e spearmanr_metric = evaluate.l...\"],[\"```\\n\\n## Limitations and Bias\\n\\n\\n## Citation\\n```bibtex\\n@book{kokoska2000crc,\\n  title={CRC standard pro...\"],[\"--\\ntitle: TREC Eval\\nemoji: ü§ó \\ncolorFrom: blue\\ncolorTo: red\\nsdk: gradio\\nsdk_version: 3.19.1\\napp_file:...\"],[\"```\\n\\n### Inputs\\n- **predictions** *(dict): a single retrieval run.*\\n    - **query** *(int): Query ID...\"],[\"### Output Values\\n- **runid** *(str): Run name.*  \\n- **num_ret** *(int): Number of retrieved documen...\"],[\"```\\n\\nA more realistic use case with an examples from [`trectools`](https:\\u002f\\u002fgithub.com\\u002fjoaopalotti\\u002ftr...\"],[\"```\\n\\n```python\\nresult\\n\\n{'runid': 'InexpC2',\\n 'num_ret': 100000,\\n 'num_rel': 6074,\\n 'num_rel_ret': 31...\"],[\"```\\n\\n## Limitations and Bias\\nThe `trec_eval` metric requires the inputs to be in the TREC run and qr...\"],[\"A quick tour\\n\\nü§ó Evaluate provides access to a wide range of evaluation tools. It covers a range of m...\"],[\"```\\n\\nIf you want to make sure you are loading the right type of evaluation (especially if there are ...\"],[\"```\\n\\n## Module attributes\\n\\nAll evalution modules come with a range of useful attributes that help to...\"],[\"```\\n\\nYou can see that it describes how the metric works in theory. If you use this metric for your w...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nNote that features always describe the type of a single input element. In general we wil...\"],[\"```\\nEvaluation modules return the results in a dictionary. However, in some instances you build up t...\"],[\"```\\n\\n### Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metri...\"],[\"```\\n\\nThe `combine` function accepts both the list of names of the metrics as well as an instantiated...\"],[\"```\\n\\nThe content of the JSON file look like the following:\\n\\n```json\\n{\\n    \\\"experiment\\\": \\\"run 42\\\",\\n  ...\"],[\"```\\n\\n## Evaluator\\n\\nThe [`evaluate.evaluator`] provides automated evaluation and only requires a mode...\"],[\"```\\n\\nCalculating the value of the metric alone is often not enough to know if a model performs signi...\"],[\"```\\n\\nThe evaluator expects a `\\\"text\\\"` and `\\\"label\\\"` column for the data input. If your dataset diffe...\"],[\"```\\n\\nWhich lets you visually compare the 4 models and choose the optimal one for you, based on one o...\"],[\"```\\n\\nEvaluation can be run by loading the `EvaluationSuite` and calling the `run()` method with a mo...\"]],\"hovertemplate\":\"source=evaluate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"evaluate, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"evaluate, circle\",\"showlegend\":true,\"x\":[-3.8730426,-4.315601,-4.0758705,-5.532783,-4.1339874,9.640984,-4.6230636,-4.321944,-4.432207,-7.1262445,9.643605,-4.302894,-4.5246124,-7.0816355,9.642087,-5.1184387,-4.7662587,-4.648739,-7.398247,9.643626,-5.3811884,-3.3775272,-2.8570967,-3.1842287,-3.477256,9.643628,-7.510586,9.641591,-4.3994613,-4.390916,-4.4328446,2.9254339,9.642877,-4.5685835,-5.114363,-4.7014675,-5.1206274,-4.800485,-6.7705026,9.643943,-4.501551,-4.47149,-7.25127,-7.062822,-7.415968,-6.1848326,2.981103,9.6459875,-3.664388,-4.4390383,-7.2133074,-4.002561,-6.9505043,2.99024,9.642724,-4.2767477,-4.3637056,-4.5110087,-7.266733,0.72435874,-3.2908337,-2.6357908,-3.9650633,-3.6971645,-2.5131757,-3.1572738,2.2876709,2.731812,1.992454,-0.31134254,-2.9598324,-4.156169,-5.163463,1.8064569,9.639705,-4.5230064,-4.362848,-4.4734926,-7.150856,9.642809,-4.80958,-4.6261673,-4.4252653,-4.5004888,-4.3518214,-3.8751676,-3.9737222,-4.17109,-4.4355702,-3.7076676,-3.8476684,-3.9196985,-4.0648313,2.3419628,-3.632882,-3.755137,-4.1367497,-3.7374997,-3.8861787],\"xaxis\":\"x\",\"y\":[-8.40818,-9.368866,-8.558999,-3.687962,-9.063048,15.931111,-9.612831,-9.5518875,-9.554732,-1.722886,15.929054,-9.543618,-9.680965,-2.14868,15.930477,-8.18154,-9.002827,-9.32797,-1.9425093,15.9303,-8.170741,-8.08616,-6.4977145,-7.5147047,-7.9098163,15.928737,-2.212001,15.930678,-9.343684,-9.44421,-9.136089,-1.1365778,15.931174,-9.517754,-8.623631,-9.62552,-8.837072,-9.476768,-6.3914256,15.92892,-9.50967,-9.519964,-2.0642352,-2.5676434,-2.802003,-2.005556,-1.0315957,15.926475,-9.096984,-9.275817,-1.5605794,-9.489374,-1.8718035,-1.224736,15.930482,-9.238737,-9.419125,-9.408263,-2.0849195,-1.6649047,-7.7973127,-6.2495656,-6.486193,-6.9353228,-6.1567016,-7.5297446,-3.5985074,-3.6767771,-3.6879203,1.5038446,-5.196921,-6.819488,-6.4142175,-3.5720482,15.925565,-9.551645,-9.368399,-9.361224,-2.6014564,15.930473,-8.566651,-8.576204,-8.350232,-8.841142,-8.749859,-8.322857,-8.525482,-8.796994,-9.451833,-8.471633,-8.598319,-8.799708,-8.962063,-4.980054,-8.096158,-8.21655,-7.932836,-8.064152,-7.9562263],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"‰∏ªË¶ÅÁâπÁÇπ\\n\\nËÆ©Êàë‰ª¨Êù•‰ªãÁªç‰∏Ä‰∏ã Gradio ÊúÄÂèóÊ¨¢ËøéÁöÑ‰∏Ä‰∫õÂäüËÉΩÔºÅËøôÈáåÊòØ Gradio ÁöÑ‰∏ªË¶ÅÁâπÁÇπÔºö\\n\\n1. [Ê∑ªÂä†Á§∫‰æãËæìÂÖ•](#example-inputs)\\n2. [‰º†ÈÄíËá™ÂÆö‰πâÈîôËØØÊ∂àÊÅØ](#erro...\"],[\"ÁªßÁª≠‰∫ÜËß£Á§∫‰æãÔºåËØ∑ÂèÇÈòÖ[Êõ¥Â§öÁ§∫‰æã](https:\\u002f\\u002fgradio.app\\u002fmore-on-examples)ÊåáÂçó„ÄÇ\\n\\n## ÈîôËØØ\\n\\nÊÇ®Â∏åÊúõÂêëÁî®Êà∑‰º†ÈÄíËá™ÂÆö‰πâÈîôËØØÊ∂àÊÅØ„ÄÇ‰∏∫Ê≠§Ôºåwith `gr.Error(\\\"...\"],[\"Âè¶‰∏Ä‰∏™ÊúâÁî®ÁöÑÂÖ≥ÈîÆÂ≠óÂèÇÊï∞ÊòØ `label=`ÔºåÂÆÉÂ≠òÂú®‰∫éÊØè‰∏™ `Component` ‰∏≠„ÄÇËøô‰øÆÊîπ‰∫ÜÊØè‰∏™ `Component` È°∂ÈÉ®ÁöÑÊ†áÁ≠æÊñáÊú¨„ÄÇËøòÂèØ‰ª•‰∏∫ËØ∏Â¶Ç `Textbox` Êàñ `Radio` ‰πãÁ±ªÁöÑ...\"],[\"```\\n\\n## ÊóóÊ†á\\n\\nÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºå\\\"Interface\\\" Â∞ÜÊúâ‰∏Ä‰∏™ \\\"Flag\\\" ÊåâÈíÆ„ÄÇÂΩìÁî®Êà∑ÊµãËØïÊÇ®ÁöÑ `Interface` Êó∂ÔºåÂ¶ÇÊûúÁúãÂà∞ÊúâË∂£ÁöÑËæìÂá∫Ôºå‰æãÂ¶ÇÈîôËØØÊàñÊÑèÂ§ñÁöÑÊ®°ÂûãË°å‰∏∫Ôºå‰ªñ‰ª¨ÂèØ‰ª•Â∞ÜËæìÂÖ•Ê†áËÆ∞‰∏∫...\"],[\"```\\n\\n_flagged\\u002flogs.csv_\\n\\n```csv\\nim,Output\\nim\\u002f0.png,Output\\u002f0.png\\nim\\u002f1.png,Output\\u002f1.png\\n```\\n\\nÂ¶ÇÊûúÊÇ®Â∏åÊúõÁî®Êà∑Êèê‰æõ...\"],[\"```\\n\\nÁõ∏ÂèçÔºåËøôÈáåÊàë‰ª¨‰øùÁïôÂõæÂÉèÁöÑÂéüÂßãÂ§ßÂ∞èÔºå‰ΩÜÂú®Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ numpy Êï∞ÁªÑ‰πãÂâçÂèçËΩ¨È¢úËâ≤Ôºö\\n\\n```py\\nimg = gr.Image(invert_colors=True, type=\\\"numpy\\\"...\"],[\"```\\n\\n## ÈòüÂàó (Queuing)\\n\\nÂ¶ÇÊûúÊÇ®ÁöÑÂ∫îÁî®Á®ãÂ∫èÈ¢ÑËÆ°‰ºöÊúâÂ§ßÈáèÊµÅÈáèÔºåËØ∑ with `queue()` ÊñπÊ≥ïÊù•ÊéßÂà∂Â§ÑÁêÜÈÄüÁéá„ÄÇËøôÂ∞ÜÊéíÈòüÂ§ÑÁêÜË∞ÉÁî®ÔºåÂõ†Ê≠§‰∏ÄÊ¨°Âè™Â§ÑÁêÜ‰∏ÄÂÆöÊï∞ÈáèÁöÑËØ∑Ê±Ç„ÄÇÈòüÂàó‰ΩøÁî® Webso...\"],[\"```\\n\\n## Ëø≠‰ª£ËæìÂá∫ (Iterative Outputs)\\n\\nÂú®Êüê‰∫õÊÉÖÂÜµ‰∏ãÔºåÊÇ®ÂèØËÉΩÈúÄË¶Å‰º†Ëæì‰∏ÄÁ≥ªÂàóËæìÂá∫ËÄå‰∏çÊòØ‰∏ÄÊ¨°ÊòæÁ§∫Âçï‰∏™ËæìÂá∫„ÄÇ‰æãÂ¶ÇÔºåÊÇ®ÂèØËÉΩÊúâ‰∏Ä‰∏™ÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÂ∏åÊúõÊòæÁ§∫ÁîüÊàêÁöÑÊØè‰∏™Ê≠•È™§ÁöÑÂõæÂÉèÔºåÁõ¥Âà∞ÊúÄÁªà...\"],[\"```\\n\\nÊÇ®‰ª•‰∏éÂ∏∏ËßÑÂáΩÊï∞Áõ∏ÂêåÁöÑÊñπÂºèÂ∞ÜÁîüÊàêÂô®Êèê‰æõÁªô Gradio„ÄÇ‰æãÂ¶ÇÔºåËøôÊòØ‰∏Ä‰∏™ÔºàËôöÊãüÁöÑÔºâÂõæÂÉèÁîüÊàêÊ®°ÂûãÔºåÂÆÉÂú®ËæìÂá∫ÂõæÂÉè‰πãÂâçÁîüÊàêÊï∞‰∏™Ê≠•È™§ÁöÑÂô™Èü≥Ôºö\\n\\n$code_fake_diffusion\\n$demo_fa...\"],[\"Gradio ÊîØÊåÅ‰º†ÈÄí*ÊâπÂ§ÑÁêÜ*ÂáΩÊï∞„ÄÇÊâπÂ§ÑÁêÜÂáΩÊï∞Âè™ÊòØÊé•ÂèóËæìÂÖ•ÂàóË°®Âπ∂ËøîÂõûÈ¢ÑÊµãÂàóË°®ÁöÑÂáΩÊï∞„ÄÇ\\n\\n‰æãÂ¶ÇÔºåËøôÊòØ‰∏Ä‰∏™ÊâπÂ§ÑÁêÜÂáΩÊï∞ÔºåÂÆÉÊé•Âèó‰∏§‰∏™ËæìÂÖ•ÂàóË°®Ôºà‰∏Ä‰∏™ÂçïËØçÂàóË°®Âíå‰∏Ä‰∏™Êï¥Êï∞ÂàóË°®ÔºâÔºåÂπ∂ËøîÂõû‰øÆÂâ™ËøáÁöÑÂçïËØçÂàóË°®‰Ωú‰∏∫ËæìÂá∫Ôºö\\n...\"],[\"```\\n\\n‰ΩøÁî®ÊâπÂ§ÑÁêÜÂáΩÊï∞ÁöÑ‰ºòÁÇπÊòØÔºåÂ¶ÇÊûúÂêØÁî®‰∫ÜÈòüÂàóÔºåGradio ÊúçÂä°Âô®ÂèØ‰ª•Ëá™Âä®*ÊâπÂ§ÑÁêÜ*‰º†ÂÖ•ÁöÑËØ∑Ê±ÇÂπ∂Âπ∂Ë°åÂ§ÑÁêÜÂÆÉ‰ª¨Ôºå‰ªéËÄåÂèØËÉΩÂä†Âø´ÊºîÁ§∫ÈÄüÂ∫¶„ÄÇ‰ª•‰∏ãÊòØ Gradio ‰ª£Á†ÅÁöÑÁ§∫‰æãÔºàËØ∑Ê≥®ÊÑè `batch=True...\"],[\"```\\n\\nÂú®‰∏äÈù¢ÁöÑÁ§∫‰æã‰∏≠ÔºåÂèØ‰ª•Âπ∂Ë°åÂ§ÑÁêÜ 16 ‰∏™ËØ∑Ê±ÇÔºàÊÄªÊé®ÁêÜÊó∂Èó¥‰∏∫ 5 ÁßíÔºâÔºåËÄå‰∏çÊòØÂàÜÂà´Â§ÑÁêÜÊØè‰∏™ËØ∑Ê±ÇÔºàÊÄªÊé®ÁêÜÊó∂Èó¥‰∏∫ 80 ÁßíÔºâ„ÄÇËÆ∏Â§ö Hugging Face ÁöÑ `transformers` Âíå `...\"],[\"## Gradio Á¨îËÆ∞Êú¨ (Colab Notebooks)\\n\\nGradio ÂèØ‰ª•Âú®‰ªª‰ΩïËøêË°å Python ÁöÑÂú∞ÊñπËøêË°åÔºåÂåÖÊã¨Êú¨Âú∞ Jupyter Á¨îËÆ∞Êú¨ÂíåÂçè‰ΩúÁ¨îËÆ∞Êú¨ÔºåÂ¶Ç[Google Colab](...\"],[\"Gradio Demo: blocks_random_slider\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n\\nimport gradio as gr\\n\\n\\ndef...\"],[\"State in Blocks\\n\\nWe covered [State in Interfaces](https:\\u002f\\u002fgradio.app\\u002finterface-state), this guide ta...\"],[\"Â¶Ç‰Ωï‰ΩøÁî®Âú∞ÂõæÁªÑ‰ª∂ÁªòÂà∂ÂõæË°®\\n\\nRelated spaces:\\nTags: PLOTS, MAPS\\n\\n## ÁÆÄ‰ªã\\n\\nÊú¨ÊåáÂçó‰ªãÁªçÂ¶Ç‰Ωï‰ΩøÁî® Gradio ÁöÑ `Plot` ÁªÑ‰ª∂Âú®Âú∞Âõæ‰∏äÁªòÂà∂Âú∞ÁêÜÊï∞ÊçÆ„ÄÇGradi...\"],[\"dataset = load_dataset(\\\"gradio\\u002fNYC-Airbnb-Open-Data\\\", split=\\\"train\\\")\\ndf = dataset.to_pandas()\\n\\ndef f...\"],[\"```\\n\\nÂú®‰∏äÈù¢ÁöÑ‰ª£Á†Å‰∏≠ÔºåÊàë‰ª¨ÂÖàÂ∞Ü CSV Êï∞ÊçÆÂä†ËΩΩÂà∞‰∏Ä‰∏™ pandas dataframe ‰∏≠„ÄÇËÆ©Êàë‰ª¨È¶ñÂÖàÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞ÔºåËøôÂ∞Ü‰Ωú‰∏∫ gradio Â∫îÁî®Á®ãÂ∫èÁöÑÈ¢ÑÊµãÂáΩÊï∞„ÄÇËØ•ÂáΩÊï∞Â∞ÜÊé•ÂèóÊúÄ‰Ωé‰ª∑Ê†º„ÄÅÊúÄÈ´ò‰ª∑Ê†ºËåÉÂõ¥...\"],[\"```\\n\\n‰∏äÈù¢ÁöÑ‰ª£Á†Å‰∏≠ÔºåÊàë‰ª¨ÈÄöËøá‰º†ÂÖ•ÁªèÁ∫¨Â∫¶ÂàóË°®Êù•ÂàõÂª∫‰∏Ä‰∏™Êï£ÁÇπÂõæ„ÄÇÊàë‰ª¨Ëøò‰º†ÂÖ•‰∫ÜÂêçÁß∞Âíå‰ª∑Ê†ºÁöÑËá™ÂÆö‰πâÊï∞ÊçÆÔºå‰ª•‰æøÂú®Èº†Ê†áÊÇ¨ÂÅúÂú®ÊØè‰∏™Ê†áËÆ∞‰∏äÊó∂ÊòæÁ§∫È¢ùÂ§ñÁöÑ‰ø°ÊÅØ„ÄÇÊé•‰∏ãÊù•ÔºåÊàë‰ª¨‰ΩøÁî® `update_layout` Êù•ÊåáÂÆö...\"],[\"```\\n\\nÊàë‰ª¨‰ΩøÁî® `gr.Column` Âíå `gr.Row` Â∏ÉÂ±ÄËøô‰∫õÁªÑ‰ª∂ÔºåÂπ∂‰∏∫ÊºîÁ§∫Âä†ËΩΩÊó∂ÂíåÁÇπÂáª \\\" Êõ¥Êñ∞Á≠õÈÄâ \\\" ÊåâÈíÆÊó∂Ê∑ªÂä†‰∫Ü‰∫ã‰ª∂Ëß¶ÂèëÂô®Ôºå‰ª•Ëß¶ÂèëÂú∞ÂõæÊõ¥Êñ∞Êñ∞ÁöÑÁ≠õÈÄâÊù°‰ª∂„ÄÇ\\n\\n‰ª•‰∏ãÊòØÂÆåÊï¥ÊºîÁ§∫‰ª£Á†ÅÔºö\\n\\n...\"],[\"Gradio Demo: examples_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"Gradio Demo: number_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr....\"],[\"Gradio Demo: map_airbnb\\n### Display an interactive map of AirBnB locations with Plotly. Data is host...\"],[\"```\\nimport gradio as gr\\nimport plotly.graph_objects as go\\nfrom datasets import load_dataset\\n\\ndataset...\"],[\"return fig\\n\\nwith gr.Blocks() as demo:\\n    with gr.Column():\\n        with gr.Row():\\n            min_p...\"],[\"Gradio Demo: question-answering\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\nimport gra...\"],[\"`@gradio\\u002fbutton`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Button } from \\\"@gradio\\u002fbutton\\\";\\n\\u003c\\u002fscript\\u003e\\n\\n\\u003cbutton type...\"],[\"Gradio Demo: sales_projections\\n\\n\\n```\\n!pip install -q gradio pandas numpy matplotlib\\n```\\n\\n\\n```\\nimport...\"],[\"Gradio and W&B Integration\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fakhaliq\\u002fJoJoGAN\\nTags: WAND...\"],[\"Get started [here](https:\\u002f\\u002fgradio.app\\u002fgetting_started)\\n\\n### Hugging Face Spaces\\n\\nHugging Face Spaces...\"],[\"```\\n\\n3. Finetune StyleGAN and W&B experiment tracking\\n\\n   This next step will open a W&B dashboard t...\"],[\"for idx in tqdm(range(num_iter)):\\n       mean_w = generator.get_latent(torch.randn([latents.size(0),...\"],[\"```\\n\\nalpha = 1.0\\nalpha = 1-alpha\\n\\npreserve_color = True\\nnum_iter = 100\\nlog_interval = 50\\n\\nsamples = ...\"],[\"if preserve_color:\\nid_swap = [9,11,15,16,17]\\nelse:\\nid_swap = list(range(7, generator.n_latent))\\n\\nfor...\"],[\"g_optim.zero_grad()\\n    loss.backward()\\n    g_optim.step()\\n\\nout_table = wandb.Table(data=samples, co...\"],[\"```\\n\\n4. Save, Download, and Load Model\\n\\n    Here's how to save and download your model.\\n\\n```python\\n\\n...\"],[\"plt.rcParams['figure.dpi'] = 150\\n\\n\\n\\ntransform = transforms.Compose(\\n    [\\n        transforms.Resize(...\"],[\"```\\n\\n5. Build a Gradio Demo\\n\\n```python\\n\\nimport gradio as gr\\n\\ntitle = \\\"JoJoGAN\\\"\\ndescription = \\\"Gradio...\"],[\"```\\n\\n7. (Optional) Embed W&B plots in your Gradio App\\n\\n   It's also possible to embed W&B plots with...\"],[\"```\\n\\n## Conclusion\\n\\nWe hope you enjoyed this brief demo of embedding a Gradio demo to a W&B report! ...\"],[\"Gradio Demo: duplicatebutton_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n...\"],[\"Gradio Demo: upload_button_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    \\n    with gr.Row():\\n        with gr.Col...\"],[\"@gradio\\u002fimageeditor\\n\\n## 0.2.0\\n\\n### Features\\n\\n- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f680...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f6a9151d5c9432c724098...\"],[\"[`5177132`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fcommit\\u002f5177132d718c77f6d47869b4334afae6380394cb)]:...\"],[\"- @gradio\\u002fimage@0.5.0\\n  - @gradio\\u002fupload@0.5.3\\n  - @gradio\\u002fclient@0.9.0\\n  - @gradio\\u002fwasm@0.4.0\\n  - @...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`b639e04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"A brand new component, completely separate from `Image` that provides simple editing capabilities.\\n\\n...\"],[\"```py\\n\\ndef fn(im):\\n    im[\\\"composite\\\"] # the full canvas\\n    im[\\\"background\\\"] # the background image...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6502](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"Gradio Demo: chatinterface_system_prompt\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr...\"],[\"Gradio Demo: streaming_wav2vec\\n\\n\\n```\\n!pip install -q gradio torch transformers \\n```\\n\\n\\n```\\nfrom trans...\"],[\"component-styles\\n\\n## Textbox\\n\\n| name        | type                                 | description    ...\"],[\"## Checkbox\\n\\n| name        | type                                 | description                    |...\"],[\"## Dropdown\\n\\n| name        | type                                 | description                    |...\"],[\"## Audio\\n\\n| name      | type                                 | description         |\\n| --------- | -...\"],[\"## Label\\n\\n| name        | type   | description                    |\\n| ----------- | ------ | -------...\"],[\"## HTML\\n\\nNothing\\n\\n## Gallery\\n\\n| name        | type                                      | descriptio...\"],[\"## Plot\\n\\nNothing (yet)\\n\\n## Markdown\\n\\nNothing\\n\\n## Button\\n\\n| name         | type                      ...\"],[\"Gradio Demo: blocks_webcam\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport numpy as np\\n\\nimport gradio...\"],[\"Gradio Demo: on_listener_live\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"‰∏ªÈ¢ò Theming\\n\\nTags: THEMES\\n\\n## ‰ªãÁªç\\n\\nGradio ÂÖ∑ÊúâÂÜÖÁΩÆÁöÑ‰∏ªÈ¢òÂºïÊìéÔºåÂèØËÆ©ÊÇ®Ëá™ÂÆö‰πâÂ∫îÁî®ÁöÑÂ§ñËßÇÂíåÊÑüËßâ„ÄÇÊÇ®ÂèØ‰ª•ÈÄâÊã©ÂêÑÁßç‰∏ªÈ¢òÔºåÊàñËÄÖÂàõÂª∫Ëá™Â∑±ÁöÑ‰∏ªÈ¢ò„ÄÇË¶ÅËøôÊ†∑ÂÅöÔºåËØ∑Â∞Ü `theme=...\"],[\"```\\n\\n$demo_theme_builder\\n\\nÊÇ®ÂèØ‰ª•‰ΩøÁî®‰∏äÈù¢ÁöÑ Spaces ‰∏äËøêË°åÁöÑ Theme BuilderÔºå‰ΩÜÈÄöËøá `gr.themes.builder()` Âú®Êú¨Âú∞ÂêØÂä®Êó∂ËøêË°åÈÄüÂ∫¶Êõ¥Âø´„ÄÇ...\"],[\"3 ‰∏™È¢úËâ≤ÊûÑÈÄ†ÂáΩÊï∞ÂèÇÊï∞ÊòØÔºö\\n\\n- `primary_hue`ÔºöËøôÊòØ‰∏ªÈ¢ò‰∏≠ÁöÑ‰∏ªËâ≤„ÄÇÂú®ÈªòËÆ§‰∏ªÈ¢ò‰∏≠ÔºåÊ≠§ÂÄºËÆæÁΩÆ‰∏∫ `gradio.themes.colors.orange`„ÄÇ\\n- `secondary_hue...\"],[\"```\\n\\nÊàñËÄÖÁõ¥Êé•‰ΩøÁî® `Color` ÂØπË±°ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-1.hf.space?__theme=light...\"],[\"```\\n\\nÊàñËÄÖÁõ¥Êé•‰ΩøÁî® `Size` ÂØπË±°ÔºåÂ¶Ç‰∏ãÊâÄÁ§∫Ôºö\\n\\n```python\\nwith gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-2.hf.space?__theme=light...\"],[\"```\\n\\n\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-extended-step-3.hf.space?__theme=light...\"],[\"```\\n\\nÂú®‰∏äÈù¢ÁöÑÁ§∫‰æã‰∏≠ÔºåÊàë‰ª¨Â∞Ü `loader_color` Âíå `slider_color` ÂèòÈáèËÆæÁΩÆ‰∏∫`#FF0000`ÔºåÂ∞ΩÁÆ°Êï¥‰Ωì `primary_color` ‰ΩøÁî®ËìùËâ≤Ë∞ÉËâ≤Êùø„ÄÇÊÇ®ÂèØ‰ª•‰ª•ËøôÁßçÊñπ...\"],[\"### CSS ÂèòÈáèÁªÑÁªá\\n\\nËôΩÁÑ∂ÊúâÊï∞Áôæ‰∏™ CSS ÂèòÈáèÔºå‰ΩÜÂπ∂‰∏çÈúÄË¶Å‰∏∫ÊØè‰∏™ÂèòÈáèÈÉΩÊåáÂÆöÂçïÁã¨ÁöÑÂÄº„ÄÇÂÆÉ‰ª¨ÈÄöËøáÂºïÁî®‰∏ÄÁªÑÊ†∏ÂøÉÂèòÈáèÂíåÂΩºÊ≠§ÂºïÁî®Êù•Ëé∑ÂèñÂÄº„ÄÇËøôÊ†∑ÂÅöÂèØ‰ª•‰ªÖ‰øÆÊîπÂ∞ëÈáèÂèòÈáè‰ª•ÊîπÂèòÊï¥‰∏™‰∏ªÈ¢òÁöÑÂ§ñËßÇÂíåÊÑüËßâÔºåÂêåÊó∂‰πüÂèØ‰ª•Êõ¥...\"],[\"```\\n\\nÂú®‰∏äÈù¢ÁöÑÁ§∫‰æã‰∏≠ÔºåÊàë‰ª¨Â∞Ü `button_primary_background_fill` Âíå `button_primary_background_fill_hover` ÂèòÈáèÂàÜÂà´ËÆæÁΩÆ‰∏∫`*...\"],[\"```\\n\\nÁé∞Âú®ÔºåÂ¶ÇÊûúÊàë‰ª¨Êõ¥Êîπ `button_primary_background_fill` ÂèòÈáèÔºå`button_primary_background_fill_hover` Âíå `button_...\"],[\"```\\n\\n`button_primary_border_dark` Â∞Ü‰ªé `button_primary_background_fill_dark` Ëé∑ÂèñÂÖ∂ÂÄºÔºåÂõ†‰∏∫ÊöóÊ®°ÂºèÊÄªÊòØ‰ΩøÁî®ÂèòÈáèÁöÑÊöóÁâàÊú¨„ÄÇ\\n\\n##...\"],[\"\\u003cdiv class=\\\"wrapper\\\"\\u003e\\n\\u003ciframe\\n\\tsrc=\\\"https:\\u002f\\u002fgradio-theme-new-step-2.hf.space?__theme=light\\\"\\n\\tframebo...\"],[\"Âú®ÂàõÂª∫‰∏ªÈ¢òÂêéÔºåÊÇ®ÂèØ‰ª•Â∞ÜÂÖ∂‰∏ä‰º†Âà∞ HuggingFace HubÔºåËÆ©ÂÖ∂‰ªñ‰∫∫Êü•Áúã„ÄÅ‰ΩøÁî®ÂíåÊûÑÂª∫‰∏ªÈ¢òÔºÅ\\n\\n### ‰∏ä‰º†‰∏ªÈ¢ò\\n\\nÊúâ‰∏§Áßç‰∏ä‰º†‰∏ªÈ¢òÁöÑÊñπÂºèÔºåÈÄöËøá‰∏ªÈ¢òÁ±ªÂÆû‰æãÊàñÂëΩ‰ª§Ë°å„ÄÇÊàë‰ª¨Â∞Ü‰ΩøÁî®‰πãÂâçÂàõÂª∫ÁöÑ‚Äúseafoam...\"],[\"```\\n\\n- ÈÄöËøáÂëΩ‰ª§Ë°å\\n\\nÈ¶ñÂÖàÂ∞Ü‰∏ªÈ¢ò‰øùÂ≠òÂà∞Á£ÅÁõò\\n\\n```python\\nseafoam.dump(filename=\\\"seafoam.json\\\")\\n```\\n\\nÁÑ∂Âêé‰ΩøÁî®‚Äúupload_theme‚ÄùÂëΩ‰ª§Ôºö...\"],[\"```\\n\\nË¶Å‰∏ä‰º†‰∏ªÈ¢òÔºåÊÇ®ÂøÖÈ°ªÊã•Êúâ‰∏Ä‰∏™ HuggingFace Ë¥¶Êà∑ÔºåÂπ∂ÈÄöËøá `hf_token` ÂèÇÊï∞‰º†ÈÄíÊÇ®ÁöÑ[ËÆøÈóÆ‰ª§Áâå](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingfac...\"],[\"### ÂèëÁé∞‰∏ªÈ¢ò\\n\\n[‰∏ªÈ¢òÂ∫ì](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fgradio\\u002ftheme-gallery)ÊòæÁ§∫‰∫ÜÊâÄÊúâÂÖ¨ÂºÄÁöÑ gradio ‰∏ªÈ¢ò„ÄÇÂú®ÂèëÂ∏É‰∏ªÈ¢ò‰πãÂêéÔºå\\nÂÆÉÂ∞ÜÂú®Âá†ÂàÜ...\"],[\"```\\n\\nÊÇ®‰πüÂèØ‰ª•Áõ¥Êé•Â∞Ü‰∏ªÈ¢òÂ≠óÁ¨¶‰∏≤‰º†ÈÄíÁªô `Blocks` Êàñ `Interface`Ôºà`gr.Blocks(theme=\\\"gradio\\u002fseafoam\\\")`Ôºâ\\n\\nÊÇ®ÂèØ‰ª•ÈÄöËøá‰ΩøÁî®ËØ≠‰πâÁâàÊú¨Ë°®ËææÂºèÂ∞ÜÊÇ®ÁöÑÂ∫î...\"],[\"his demo shows how you can build an interactive dashboard with gradio. Click on a python library on ...\"],[\"gradio\\n\\n## 4.11.0\\n\\n### Features...\"],[\"- [#6842](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6842) [`846d52d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6809](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6809) [`1401d99`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6833](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6833) [`1b9d423`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes\\n\\n- [#6829](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6829) [`50496f9`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.10.0\\n\\n### Features\\n\\n- [#6798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6798) [`245d58e`](https...\"],[\"### Fixes\\n\\n- [#6799](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6799) [`c352811`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 4.9.1\\n\\n### Features\\n\\n- [#6781](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6781) [`a807ede`](https:...\"],[\"### Fixes\\n\\n- [#6525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6525) [`5d51fbc`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6745](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6745) [`3240d04`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6671](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6671) [`299f5e2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6666](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6666) [`30c9fbb`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6704](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6704) [`24e0481`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6416](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6416) [`5177132`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"### Fixes...\"],[\"- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6676](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6676) [`fe40308`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6639](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6639) [`9a6ff70`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6694](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6694) [`dfc61ec`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6759](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6759) [`28a7aa9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.8.0\\n\\n### Features...\"],[\"- [#6624](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6624) [`1751f14`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6565](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6565) [`9bf1ad4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6607](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6607) [`13ace03`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6572](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6572) [`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- [#6551](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6551) [`8fc562a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"## 4.7.1\\n\\n### Features\\n\\n- [#6537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6537) [`6d3fecfa4`](http...\"],[\"- [#6532](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6532) [`96290d304`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6523](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6523) [`63f466882`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6538](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6538) [`147926196`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6528) [`f53b01cbf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6500](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6500) [`830b6c0e6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.5.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\n Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Fixes\\n\\n- [#6497](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#6428](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6428) [`ac4ca59c9`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6455](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6455) [`179f5bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6423) [`62d35c3d1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6419](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6419) [`1959471a8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6441](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6441) [`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6457](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6457) [`d00fcf89d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.3.0\\n\\n### Features...\"],[\"- [#6395](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6395) [`8ef48f852`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6099](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6099) [`d84209703`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6412) [`649f3ceb6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6414](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6414) [`da1e31832`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.2.0\\n\\n### Features...\"],[\"- [#6333](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6333) [`42f76aeeb`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6356](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6356) [`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6368](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6368) [`8a3f45c26`](https:\\u002f\\u002fgithub.co...\"],[\"## 4.1.2\\n\\n### Features\\n\\n- [#6318](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6318) [`d3b53a457`](http...\"],[\"- [#6310](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6310) [`dfdaf1092`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6317](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6317) [`19af2806a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6311](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6311) [`176c4d140`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6309](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6309) [`c56128781`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.1.1\\n\\n### Fixes\\n\\n- [#6288](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6288) [`92278729e`](https:\\u002f...\"],[\"- [#6261](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6261) [`8bbeca0e7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6232](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6232) [`ac4f2bcde`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6266](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6266) [`e32bac894`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6236](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6236) [`6bce259c5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6249](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6249) [`2cffcf3c3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6211) [`a4a931dd3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 4.0.2\\n\\n### Fixes\\n\\n- [#6191](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6191) [`b555bc09f`](https:\\u002f...\"],[\"## 4.0.0\\n\\n### Highlights\\n\\n4.0 is a big release, so here are the main highlights:\\n\\n**1. Custom Compon...\"],[\"\\u003cimg style=\\\"width:50%\\\" src=\\\"https:\\u002f\\u002fi.imgur.com\\u002fewUIuUc.png\\\"\\u003e\\n\\n**4. Custom Share Servers**: \\n\\nGradio...\"],[\"Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the ...\"],[\"**Other changes related to the `gradio` library**:\\n\\n* Removes the deprecated `status_tracker` parame...\"],[\"### Migrating to Gradio 4.0\\n\\nHere are some concrete tips to help migrate to Gradio 4.0:\\n\\n#### **Usin...\"],[\"```\\n\\nIn order for the HTML component to be able to serve `image.png`, you will need to add `image.pn...\"],[\"```\\n\\n\\n#### **Using `concurrency_limit` instead of `concurrency_count`**\\n\\nPreviously, in Gradio 3.x, ...\"],[\"To summarize migration:\\n\\n* For events that execute quickly or don't use much CPU or GPU resources, y...\"],[\"```\\n\\nNow, you should write:\\n\\n```py\\ngr.ImageEditor(sources=(), brush=gr.Brush(colors=[\\\"#000000\\\"]))\\n``...\"],[\"```\\n\\nUnlike the `Image` component, which passes the input image as a single value into the predictio...\"],[\"- [#6184](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6184) [`86edc0199`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6153](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6153) [`1162ed621`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6152](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6152) [`982bff2fd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6135](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6135) [`bce37ac74`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6098](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6098) [`c3bc515bf`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6129](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6129) [`0d261c6ec`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6014](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6014) [`cad537aac`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6018](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6018) [`184834d02`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6114) [`39227b6fa`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6089](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6089) [`cd8146ba0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6027](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6027) [`de18102b8`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6044](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6044) [`9053c95a1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6146](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6146) [`40a171ea6`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5826](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5826) [`ce036c5d4`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6076) [`f3f98f923`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.0-beta.13\\n\\n### Features\\n\\n- [#5964](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5964) [`5fbda0b...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`d2314e53b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5894](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5894) [`fee3d527e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`85ba6de13`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.48.0\\n\\n### Features...\"],[\"- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5915](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5915) [`e24163e15`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5819](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5819) [`5f1cbc436`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5864](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5864) [`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5904](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5904) [`891d42e9b`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5890](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5890) [`c4ba832b3`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5930](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5930) [`361823896`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.47.1\\n\\n### Fixes\\n\\n- [#5816](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5816) [`796145e2c`](https:...\"],[\"For more information check the [`FileExplorer` documentation](https:\\u002f\\u002fgradio.app\\u002fdocs\\u002ffileexplorer)....\"],[\"- [#5798](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5798) [`a0d3cc45c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5794](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5794) [`f096c3ae1`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.46.1\\n\\n### Features\\n\\n- [#5124](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5124) [`6e56a0d9b`](htt...\"],[\"## 3.46.0\\n\\n### Features\\n\\n- [#5699](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5699) [`8f0fed857`](htt...\"],[\"- [#5735](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5735) [`abb5e9df4`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5731](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5731) [`c9af4f794`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.2\\n\\n### Features\\n\\n- [#5722](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5722) [`dba651904`](htt...\"],[\"- [#5714](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5714) [`a0fc5a296`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5705](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5705) [`78e7cf516`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5726) [`96c4b97c7`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.45.1\\n\\n### Fixes\\n\\n- [#5701](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5701) [`ee8eec1e5`](https:...\"],[\"- [#5675](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5675) [`b619e6f6e`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5681](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5681) [`40de3d217`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5652](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5652) [`2e25d4305`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5660](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5660) [`d76555a12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5240) [`da05e59a5`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5590](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5590) [`d1ad1f671`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5625](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5625) [`9ccc4794a`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5633](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5633) [`341402337`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5593](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5593) [`88d43bd12`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.4\\n\\n### Features\\n\\n- [#5514](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5514) [`52f783175`](htt...\"],[\"### Fixes\\n\\n- [#5587](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5587) [`e0d61b8ba`](https:\\u002f\\u002fgithub.co...\"],[\"- [#5562](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5562) [`50d9747d0`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5553](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5553) [`d1bf23cd2`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 3.44.2\\n\\n### Fixes\\n\\n- [#5537](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5537) [`301c7878`](https:\\u002f...\"],[\"- [#5516](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5516) [`c5fe8eba`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5525](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5525) [`21f1db40`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.44.0\\n\\n### Features...\"],[\"- [#5505](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5505) [`9ee20f49`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5488](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5488) [`8909e42a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5474](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5474) [`041560f9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5459](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5459) [`bd2fda77`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5496](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5496) [`82ec4d26`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.43.2\\n\\n### Fixes\\n\\n- [#5456](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5456) [`6e381c4f`](https:\\u002f...\"],[\"- [#5165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5165) [`c77f05ab`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5417](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5417) [`d14d63e3`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002fgithub.com...\"],[\"Thanks [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82)!\\n\\n#### Added the ability to attach event lis...\"],[\"```\\n\\n Thanks [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94)!\\n\\n### Features...\"],[\"- [#5334](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5334) [`c5bf9138`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5370](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5370) [`61803c65`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5369](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5369) [`b8968898`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5394](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5394) [`4d94ea0a`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.41.2\\n\\n### Features\\n\\n- [#5284](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5284) [`5f25eb68`](http...\"],[\"## 3.41.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"```\\n\\n Thanks [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton)!\\n\\n#### Add `render` function to `\\u003c...\"],[\"```\\n\\n Thanks [@hannahblair](https:\\u002f\\u002fgithub.com\\u002fhannahblair)!\\n\\n### Features...\"],[\"- [#5268](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5268) [`f49028cf`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5283](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5283) [`a7460557`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5280](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5280) [`a2f42e28`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4943](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4943) [`947d615d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5188](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5188) [`b22e1888`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5305](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5305) [`15075241`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5264](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5264) [`46a2b600`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5256](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5256) [`933db53e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5179](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5179) [`6fb92b48`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5122](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5122) [`3b805346`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5231](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5231) [`87f1c2b4`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5235](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5235) [`1ecf88ac`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.40.0\\n\\n### Highlights\\n\\n#### Client.predict will now return the final output for streaming endpoi...\"],[\"```python\\nimport gradio as gr\\nfrom pydub import AudioSegment\\n\\ndef stream_audio(audio_file):\\n    audi...\"],[\"```\\n\\nFrom the backend, streamed outputs are served from the `\\u002fstream\\u002f` endpoint instead of the `\\u002ffil...\"],[\"- [#5081](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5081) [`d7f83823`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5125](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5125) [`80be7a1c`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5046) [`5244c587`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5047](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5047) [`883ac364`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5104](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5104) [`34f6b22e`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5035](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5035) [`8b4eb8ca`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5080](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5080) [`37caa2e0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5062](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5062) [`7d897165`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5114](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5114) [`56d2609d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5039](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5039) [`620e4645`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#5140](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5140) [`cd1353fa`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"## 3.39.0\\n\\n### Highlights\\n\\n#### Create Discord Bots from Gradio Apps ü§ñ ([#4960](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fgradio-builds.s3.amazonaws.com\\u002fdemo-files\\u002fdiscordbots\\u002fguide\\u002fllama_chat.gif\\\"\\u003e\\n...\"],[\"But once again, you can deploy ANY `gr.ChatInterface` app exposed on the internet! So don't hesitate...\"],[\"- [#4995](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4995) [`3f8c210b`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4985](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4985) [`b74f8453`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"### Fixes\\n\\n- [#4997](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4997) [`41c83070`](https:\\u002f\\u002fgithub.com...\"],[\"- Provide a parameter `animate` (`False` by default) in `gr.make_waveform()` which animates the over...\"],[\"- Add default sketch color argument `brush_color`. Also, masks drawn on images are now slightly tran...\"],[\"### Bug Fixes:\\n\\n- Fixes `cancels` for generators so that if a generator is canceled before it is com...\"],[\"## 3.37\\n\\n### New Features:\\n\\nIntroducing a new `gr.ChatInterface` abstraction, which allows Gradio us...\"],[\"```\\n\\nWhich produces:\\n\\n\\u003cimg width=\\\"1291\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fassets...\"],[\"- Chatbot messages now show hyperlinks to download files uploaded to `gr.Chatbot()` by [@dawoodkhan8...\"],[\"```\\n\\n```py\\nwith gr.Blocks() as demo:\\n    gr.Markdown(\\\"ÿ≥ŸÑÿßŸÖ\\\", rtl=True)\\ndemo.launch()...\"],[\"```\\n\\n- The `get_api_info` method of `Blocks` now supports layout output components [@freddyaboulton]...\"],[\"* Add missing `display: flex` property to `Row` so that flex styling is applied to children by [@han...\"],[\"### Other Changes:\\n\\n- Warning on mobile that if a user leaves the tab, websocket connection may brea...\"],[\"## 3.36.1\\n\\n### New Features:\\n\\n- Hotfix to support pydantic v1 and v2 by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"No changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.36.0\\n\\n### New Featur...\"],[\"### Bug Fixes:...\"],[\"- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg...\"],[\"- Fix `make_waveform` to work with paths that contain spaces [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4...\"],[\"- Ensure that Gradio does not silently fail when running on a port that is occupied by [@abidlabs](h...\"],[\"- Removed uncessessary `type` deprecation warning by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyabou...\"],[\"- Fix bug where `show_label` was hiding the entire component for `gr.Label` by [@freddyaboulton](htt...\"],[\"### Other Changes:...\"],[\"- Add `.git-blame-ignore-revs` by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in [PR 4586](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"- Better errors when you define two Blocks and reference components in one Blocks from the events in...\"],[\"### Breaking Changes:\\n\\n[PR 4683](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4683) removes the explict...\"],[\"### Other Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3....\"],[\"- Min and max value for gr.Number by [@artegoser](https:\\u002f\\u002fgithub.com\\u002fartegoser) and [@dawoodkhan82](...\"],[\"- Add `latex_delimiters` parameter to `Chatbot` to control the delimiters used for LaTeX and to disa...\"],[\"Example:\\n\\n```python\\ndef start_process(name):\\n    gr.Info(\\\"Starting process\\\")\\n    if name is None:\\n  ...\"],[\"```\\n\\n### Bug Fixes:...\"],[\"- Add support for PAUSED state in the JS client by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 4...\"],[\"- Fix new line issue with `gr.Chatbot()` by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR ...\"],[\"- Fix dispatched errors from within components [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 478...\"],[\"### Other Changes:\\n\\n- Change styling of status and toast error components by [@hannahblair](https:\\u002f\\u002f...\"],[\"### Breaking Changes:\\n\\n- The behavior of the `Clear` button has been changed for `Slider`, `Checkbox...\"],[\"- Remove target=\\\"\\\\_blank\\\" override on anchor tags with internal targets by [@hannahblair](https:\\u002f\\u002fgi...\"],[\"- Fix video rendering in Safari by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 4433](https:\\u002f\\u002fg...\"],[\"### Other Changes:\\n\\n- When running on Spaces, handler functions will be transformed by the [PySpaces...\"],[\"No changes to highlight.\\n\\n## 3.33.1\\n\\n### New Features:\\n\\nNo changes to highlight.\\n\\n### Bug Fixes:\\n\\n- ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.33.0\\n\\n### New Features:\\n\\n- Introduced `gradio ...\"],[\"- Fix bug where Label change event was triggering itself by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffre...\"],[\"- Do not send HF token to other domains via `\\u002fproxy` route by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlab...\"],[\"### Other Changes:\\n\\n- Remove flicker of loading bar by adding opacity transition, by [@aliabid94](ht...\"],[\"### Bug Fixes:\\n\\n- Fixed Gallery\\u002fAnnotatedImage components not respecting GRADIO_DEFAULT_DIR variable...\"],[\"### Other Changes:\\n\\n- Refactor web component `initial_height` attribute by [@whitphx](https:\\u002f\\u002fgithub...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.31.0\\n\\n### New Features:\\n\\n- The reloader comman...\"],[\"- Fix \\\"TypeError: issubclass() arg 1 must be a class\\\" When use Optional[Types] by [@lingfengchencn](...\"],[\"- Fixes a bug with typing.get_type_hints() on Python 3.9 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs)...\"],[\"### Other Changes:\\n\\n- Change `gr.Chatbot()` markdown parsing to frontend using `marked` library and ...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n## 3.30.0\\n\\n### New Features:\\n\\n- Adds a `root_path` ...\"],[\"### Bug Fixes:\\n\\n- Records username when flagging by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR ...\"],[\"- Allow users to upload audio files in Audio component on iOS by by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- Fix `gr.Slider` `release` event not triggering on mobile by [@space-nuko](https:\\u002f\\u002fgithub.com\\u002fspace...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"### New Features:\\n\\n- Add support for `visual-question-answering`, `document-question-answering`, and...\"],[\"### Bug Fixes:\\n\\n- Fixes issue with `matplotlib` not rendering correctly if the backend was not set t...\"],[\"### Full Changelog:\\n\\n- Safer version of `gr.HuggingFaceDatasetSaver` using HTTP methods instead of g...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\n- CI:...\"],[\"- Fix duplicate play commands in full-screen mode of 'video'. by [@tomchang25](https:\\u002f\\u002fgithub.com\\u002fto...\"],[\"- Fix issue in `gr.Gallery()` where setting height causes aspect ratio of images to collapse by [@da...\"],[\"- Fix bug where port was not reused if the demo was closed and then re-launched by [@freddyaboulton]...\"],[\"### Documentation Changes:\\n\\n- Make use of `gr` consistent across the docs by [@duerrsimon](https:\\u002f\\u002fg...\"],[\"### Full Changelog:\\n\\n- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https:\\u002f\\u002fgithub.com...\"],[\"![AnnotatedImage screenshot](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f232142720-86e0020f-be...\"],[\"```\\n\\nSee the [image_segmentation demo](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002ftree\\u002fmain\\u002fdemo\\u002fimage_seg...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fix code markdown support in `gr.Chatbot()` component by [@dawoodkhan82](http...\"],[\"```python\\nwith gr.Blocks() as demo:\\n    img = gr.Image()\\n    textbox = gr.Textbox()\\n\\n    def select_...\"],[\"```\\n\\n![Recording 2023-04-08 at 17 44 39](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f230748572...\"],[\"- Increase timeout for sending analytics data by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in...\"],[\"- Fix bug where the upload button was not properly handling the `file_count='multiple'` case by [@fr...\"],[\"### Documentation Changes:\\n\\n- Fix invalid argument docstrings, by [@akx](https:\\u002f\\u002fgithub.com\\u002fakx) in ...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\n  ![Theme Builder](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f7870876\\u002f228204929-d71cbba5-69c2-45...\"],[\"- Fixed bug where text for altair plots was not legible in dark mode by [@freddyaboulton](https:\\u002f\\u002fgi...\"],[\"- Fixed bug where chatbot does not autoscroll inside of a tab, row or column by [@dawoodkhan82](http...\"],[\"- Fixes certain `_js` return values being double wrapped in an array, by [@space-nuko](https:\\u002f\\u002fgithu...\"],[\"- Fix items in `gr.Dropdown` besides the selected item receiving a checkmark, by [@space-nuko](https...\"],[\"### Documentation Changes:\\n\\n- Makes some fixes to the Theme Guide related to naming of variables, by...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Removed heavily-mocked tests related to comet_ml, wandb, ...\"],[\"### Full Changelog:\\n\\n- Mobile responsive iframes in themes guide by [@aliabd](https:\\u002f\\u002fgithub.com\\u002fali...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.23.0\\n\\n### New Features:\\n\\n###### Theme Sha...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.22.1\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"###### `elem_classes`\\n\\nAdd keyword argument `elem_classes` to Components to control class names of c...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Pinned `pyright==1.1.298` for stability by [@abidlabs](ht...\"],[\"###### Uploading\\n\\nThere are two ways to upload a theme, via the theme class instance or the command ...\"],[\"```\\n\\n2. Via the command line\\n\\nFirst save the theme to disk\\n\\n```python\\nmy_theme.dump(filename=\\\"my_the...\"],[\"```\\n\\nby [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3428](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\n\\u003cimg width=\\\"1054\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f224116682-...\"],[\"```python\\nimport gradio as gr\\n\\nwith gr.Blocks() as demo:\\n    gallery = gr.Gallery([\\\"images\\u002f1.jpg\\\", \\\"...\"],[\"```\\n\\nBy [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 3399](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"### Bug Fixes:\\n\\n- Use `huggingface_hub` to send telemetry on `interface` and `blocks`; eventually to...\"],[\"### Documentation Changes:\\n\\n- Added a section on security and access when sharing Gradio apps by [@a...\"],[\"### Testing and Infrastructure Changes:\\n\\n- Fixes tests that were failing locally but passing on CI b...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Prevent in-place updates of ...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.20.1\\n\\n### New Features:\\n\\n- Add `height` k...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Ensure uploaded images are always shown in the sketch tool by [@pngwn](https:...\"],[\"```\\n\\nby [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3211](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Updated image upload component to accept all image formats, including lossless formats like .webp ...\"],[\"- Allow developers to access the username of a logged-in user from the `gr.Request()` object using t...\"],[\"- Ensure `mirror_webcam` is always respected by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 3245](http...\"],[\"- Remove embed's `initial_height` when loading is complete so the embed finds its natural height onc...\"],[\"- Fix bug where `height` set in `Gallery.style` was not respected by the front-end by [@freddyaboult...\"],[\"- Fix error when using backen_fn and custom js at the same time by [@jialeicui](https:\\u002f\\u002fgithub.com\\u002fj...\"],[\"### Documentation Changes:\\n\\n- Added the `types` field to the dependency field in the config by [@fre...\"],[\"### Full Changelog:\\n\\n- Fixed comment typo in components.py by [@eltociear](https:\\u002f\\u002fgithub.com\\u002feltoci...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.19.0\\n\\n### New Features:\\n\\n###### Improved ...\"],[\"###### New `gr.BarPlot` component! üìä\\n\\nCreate interactive bar plots from a high-level interface with ...\"],[\"```\\n\\nBy [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 3157](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"```\\n\\nBy [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 3165](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002f...\"],[\"- Fixes `gr.utils.delete_none` to only remove props whose values are `None` from the config by [@abi...\"],[\"- Fix bug where auth cookies where not sent when connecting to an app via http by [@freddyaboulton](...\"],[\"### Documentation Changes:\\n\\n- Sort components in docs by alphabetic order by [@aliabd](https:\\u002f\\u002fgithu...\"],[\"- Fix demos page css and add close demos button by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3151]...\"],[\"- Fixed gradio share links so that they are persistent and do not reset if network\\n  connection is d...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.18.0\\n\\n### New Features:\\n\\n###### Revamped ...\"],[\"```\\n\\nBy [@maxaudron](https:\\u002f\\u002fgithub.com\\u002fmaxaudron) in [PR 3075](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio...\"],[\"- Fixes URL resolution on Windows by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 3108](https:\\u002f\\u002fg...\"],[\"- A share link will automatically be created when running on Sagemaker notebooks so that the front-e...\"],[\"### Documentation Changes:\\n\\n- Added a guide on the 4 kinds of Gradio Interfaces by [@yvrjsharma](htt...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.17.1\\n\\n### New Features:\\n\\n###### iOS image...\"],[\"### Bug Fixes:\\n\\n- Fix bug where examples were not rendered correctly for demos created with Blocks a...\"],[\"* Fix a broken link in the Quick Start guide, by [@cakiki](https:\\u002f\\u002fgithub.com\\u002fcakiki) in [PR 3109](h...\"],[\"```\\n\\n\\u003cimg width=\\\"1087\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260197...\"],[\"```\\n\\n![chatbot_load](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f213260220-3eaa25b7-a38b-48c6...\"],[\"- Fixes bug where interpretation event was not configured correctly by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fixes bug where temporary uploaded files were not being added to temp sets by [@abidlabs](https:\\u002f\\u002f...\"],[\"- Added better support for symlinks in the way absolute paths are resolved by [@abidlabs](https:\\u002f\\u002fgi...\"],[\"- Adding missing embedded components on docs by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 3027](ht...\"],[\"- Preserve selected image of Gallery through updated by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddya...\"],[\"### Documentation Changes:\\n\\n- SEO improvements to guides by[@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.16.2\\n\\n### New Features:\\n\\nNo changes to hi...\"],[\"- Fixed file upload fails for files with zero size by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan8...\"],[\"- Fix bug where outputs for examples where not being returned by the backend by [@freddyaboulton](ht...\"],[\"### Documentation Changes:\\n\\nNo changes to highlight.\\n\\n### Testing and Infrastructure Changes:\\n\\nNo ch...\"],[\"```\\n\\nProgress indicator bar by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 2750](https:\\u002f\\u002fgithu...\"],[\"```\\n\\n\\u003cimg width=\\\"610\\\" alt=\\\"Screenshot 2023-01-03 at 4 14 36 PM\\\" src=\\\"https:\\u002f\\u002fuser-images.githubuserc...\"],[\"- Fixed bug where an error opening an audio file led to a crash by [@FelixDombek](https:\\u002f\\u002fgithub.com...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using Google Sheets to create a real-time dashboard w...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- The `default_enabled` parame...\"],[\"With this component you can easily create time series visualizations with customizable\\nappearance fo...\"],[\"```\\n\\n![image](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f208711646-81ae3745-149b-46a3-babd-0...\"],[\"### Documentation Changes:\\n\\n- Added a Guide on using BigQuery with Gradio's `DataFrame` and `Scatter...\"],[\"No changes to highlight.\\n\\n## 3.14.0\\n\\n### New Features:\\n\\n###### Add Waveform Visual Support to Audio\\n...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed issue where too many temporary files were created, all with randomly ge...\"],[\"No changes to highlight.\\n\\n### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.13.1\\n\\n### New F...\"],[\"```\\n\\nThese links are a more secure and scalable way to create shareable demos!\\n\\n### Bug Fixes:\\n\\n- Al...\"],[\"### Breaking Changes:\\n\\nNo changes to highlight.\\n\\n### Full Changelog:\\n\\n- Fixed typo in parameter `vis...\"],[\"The `gr.ScatterPlot` component accepts a pandas dataframe and some optional configuration parameters...\"],[\"```\\n\\n\\u003cimg width=\\\"404\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f206737726-...\"],[\"```\\n\\n\\u003cimg width=\\\"1366\\\" alt=\\\"image\\\" src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204660697...\"],[\"```\\n\\n![label_bg_color_update](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f204400372-80e53857-...\"],[\"### Bug Fixes:\\n\\n- Fixed issue where image thumbnails were not showing when an example directory was ...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"Here's a simple example that references a local image `lion.jpg` that is in the same\\nfolder as the P...\"],[\"```\\n\\n![Alt text](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f1778297\\u002f204357455-5c1a4002-eee7-479d-9a1e...\"],[\"```\\n\\n###### Update Accordion properties from the backend\\n\\nYou can now update the Accordion `label` a...\"],[\"```\\n\\n![update_accordion](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f203164176-b102eae3-babe-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"## 3.11.0\\n\\n### New Features:\\n\\n###### Upload Button\\n\\nThere is now a new component called the `UploadB...\"],[\"```\\n\\n###### Revamped API documentation page\\n\\nNew API Docs page with in-browser playground and update...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixed bug that limited files from being sent over websockets to 16MB. The new...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Bug Fixes:\\n\\n- Updated the minimum FastApi used in tests to version 0.87 by [@freddyaboulton](htt...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"When you load an upstream app with `gr.Blocks.load`, you can now specify which fn\\nto call with the `...\"],[\"```\\n\\nThe `api_name` parameter will take precedence over the `fn_index` parameter.\\n\\n### Bug Fixes:\\n\\n-...\"],[\"### Testing and Infrastructure Changes:\\n\\nNo changes to highlight.\\n\\n### Breaking Changes:\\n\\nNo changes...\"],[\"This can be used to:\\n\\n- Create live visualizations that show the most up to date data\\n- Refresh the ...\"],[\"```\\n\\n![live_demo](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f198357377-633ce460-4e31-47bd-82...\"],[\"### Bug Fixes:\\n\\n- Fix whitespace issue when using plotly. [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodk...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.7\\n\\n### New Features:\\n\\n###### Batched Func...\"],[\"```\\n\\nThe advantage of using batched functions is that if you enable queuing, the Gradio\\nserver can a...\"],[\"```\\n\\n### Bug Fixes:\\n\\n- Fixes issue where plotly animations, interactivity, titles, legends, were not...\"],[\"### Documentation Changes:\\n\\n- Added an example interactive dashboard to the \\\"Tabular & Plots\\\" sectio...\"],[\"- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlab...\"],[\"- Clearer error message when events are defined outside of a Blocks scope, and a warning if you\\n  tr...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.6\\n\\n### New Features:\\n\\n###### Cancelling R...\"],[\"cancel_on_change.change(None, None, None, cancels=[click_event, pred_event])\\n\\n\\ndemo.queue(concurrenc...\"],[\"```\\n\\nFor interfaces, a stop button will be added automatically if the function uses a `yield` statem...\"],[\"```\\n\\n![stop_interface_rl](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f41651716\\u002f195952883-e7ca4235-aae3...\"],[\"### Documentation Changes:\\n\\n- Adds a demo to show how a sound alert can be played upon completion of...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.5\\n\\n### Bug Fixes:\\n\\n- Ensure that Gradio d...\"],[\"### New Features:\\n\\n- When an `Image` component is set to `source=\\\"upload\\\"`, it is now possible to dr...\"],[\"- Speeds up Gallery component by using temporary files instead of base64 representation in the front...\"],[\"- Updated share link message to reference new Spaces Hardware [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlab...\"],[\"- Stops a gradio launch from hogging a port even after it's been killed [@aliabid94](https:\\u002f\\u002fgithub....\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4.1\\n\\n### New Features:\\n\\n###### 1. See Pas...\"],[\"1. Fix typo in guide image path by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 2357]...\"],[\"7. Fix bug where new typeable slider doesn't respect the minimum and maximum values [@dawoodkhan82](...\"],[\"### Documentation Changes:\\n\\n1. New Guide: Connecting to a Database üóÑÔ∏è\\n\\n   A new guide by [@freddyabo...\"],[\"- Create a guide on how to connect an app to a database hosted on the cloud by [@freddyaboulton](htt...\"],[\"- Catch the permission exception on the audio component by [@Ian-GL](https:\\u002f\\u002fgithub.com\\u002fIan-GL) in [...\"],[\"- Lets users provide a `gr.update()` dictionary even if post-processing is disabled [@abidlabs](http...\"],[\"### Contributors Shoutout:\\n\\nNo changes to highlight.\\n\\n## 3.4\\n\\n### New Features:\\n\\n###### 1. Gallery C...\"],[\"```\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192399521-7360b1a9-7ce0-443e-8e94-8...\"],[\"```\\n\\n![color-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410500-3c8c3e64-a5fd-4df2-...\"],[\"```\\n\\n![webcam-sketch](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f192410820-0ffaf324-776e-4e1f...\"],[\"As well as other fixes\\n\\n### Bug Fixes:\\n\\n1. Fix bug where max concurrency count is not respected in q...\"],[\"### Documentation Changes:\\n\\n1. Adding a Playground Tab to the Website by [@aliabd](https:\\u002f\\u002fgithub.co...\"],[\"- Website fixes and refactoring by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2280](https:\\u002f\\u002fgithub....\"],[\"- Respect Upstream Queue when loading interfaces\\u002fblocks from Spaces by [@freddyaboulton](https:\\u002f\\u002fgit...\"],[\"- Fix Web Tracker Script by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 2308](https:\\u002f\\u002fgithub.com\\u002fgra...\"],[\"### Contributors Shoutout:\\n\\n- [@SkyTNT](https:\\u002f\\u002fgithub.com\\u002fSkyTNT) made their first contribution in ...\"],[\"```\\n\\n![example](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f189086273-f5e7087d-71fa-4158-90a9-...\"],[\"```\\n\\n![187936493-5c90c01d-a6dd-400f-aa42-833a096156a1](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f902...\"],[\"- safari fixes by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 2138](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangt...\"],[\"- Preserve Labels In Interpretation Components by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulto...\"],[\"### Contributors Shoutout:\\n\\n- [@lamhoangtung](https:\\u002f\\u002fgithub.com\\u002flamhoangtung) made their first cont...\"],[\"```\\n\\n- Configure a maximum queue size\\n\\n```python\\ndemo = gr.Interface(...)\\ndemo.queue(max_size=100)\\nd...\"],[\"```\\n\\n- Automatic conversion of videos so they are playable in the browser (thanks to PR #2003). Grad...\"],[\"```\\n\\n###### 5. New Guide üñäÔ∏è\\n\\n- [Gradio and W&B Integration](https:\\u002f\\u002fgradio.app\\u002fGradio_and_Wandb_Inte...\"],[\"- Reset components to original state by setting value to None by [@freddyaboulton](https:\\u002f\\u002fgithub.co...\"],[\"- Encourage people to keep trying when queue full by [@apolinario](https:\\u002f\\u002fgithub.com\\u002fapolinario) in...\"],[\"- Allow frontend method execution on Block.load event by [@codedealer](https:\\u002f\\u002fgithub.com\\u002fcodedealer...\"],[\"- feat(samples table\\u002fgallery): Crop thumbs to square by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted)...\"],[\"### Contributors Shoutout:\\n\\n- [@chrisemezue](https:\\u002f\\u002fgithub.com\\u002fchrisemezue) made their first contri...\"],[\"```\\n\\nBut you can also embed demos that are running anywhere, you just need to link the demo to `src`...\"],[\"```\\n\\nIf you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ex...\"],[\"###### 5. `gr.Examples()` for Blocks üß±\\n\\nWe've added the `gr.Examples` component helper to allow you ...\"],[\"### Full Changelog:...\"],[\"- File component: list multiple files and allow for download #1446 by [@dawoodkhan82](https:\\u002f\\u002fgithub...\"],[\"- Add python-3.7 tests by [@freddyaboulton](https:\\u002f\\u002fgithub.com\\u002ffreddyaboulton) in [PR 1818](https:\\u002f\\u002f...\"],[\"### Contributors Shoutout:\\n\\n- [@nhankiet](https:\\u002f\\u002fgithub.com\\u002fnhankiet) made their first contribution...\"],[\"```\\n\\n![hello-blocks](https:\\u002f\\u002fuser-images.githubusercontent.com\\u002f9021060\\u002f168684108-78cbd24b-e6bd-4a04-...\"],[\"###### 4. New Components: Model3D, Dataset, and More..\\n\\nWe've introduced a lot of new components in ...\"],[\"- Gradio dash fe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 807](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- add test infra + add browser tests to CI by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 852](https:\\u002f...\"],[\"- backend_default_value_refactoring by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 871](...\"],[\"- 3d Image Component by [@dawoodkhan82](https:\\u002f\\u002fgithub.com\\u002fdawoodkhan82) in [PR 775](https:\\u002f\\u002fgithub....\"],[\"- Redesign 1 by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 918](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002f...\"],[\"- allow audio components to take a string value by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 930](ht...\"],[\"- add frontend for page load events by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 967](https:\\u002f\\u002fgithub...\"],[\"- State and variables by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 977](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- added interactive parameter to components by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 992](...\"],[\"- release 2.9.4 by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1006](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- fixed failing test on main by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1023](https:\\u002f\\u002fgithub...\"],[\"- GAN Gradio Guide: Adjustments to iframe heights by [@NimaBoscarino](https:\\u002f\\u002fgithub.com\\u002fNimaBoscari...\"],[\"- Update text by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted) in [PR 1050](https:\\u002f\\u002fgithub.com\\u002fgradio...\"],[\"- inputless-interfaces by [@FarukOzderim](https:\\u002f\\u002fgithub.com\\u002fFarukOzderim) in [PR 1038](https:\\u002f\\u002fgith...\"],[\"- Dark text by [@ronvoluted](https:\\u002f\\u002fgithub.com\\u002fronvoluted) in [PR 1049](https:\\u002f\\u002fgithub.com\\u002fgradio-a...\"],[\"- Website Reload: README in demos docker by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1100](https:...\"],[\"- Interface types: handle input-only, output-only, and unified interfaces by [@abidlabs](https:\\u002f\\u002fgit...\"],[\"- Stacked form inputs css by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1134](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"- add select event for tabitems by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1154](https:\\u002f\\u002fgithub.co...\"],[\"- image gallery component + img css by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1140](https...\"],[\"- enable flex grow for gr-box by [@radames](https:\\u002f\\u002fgithub.com\\u002fradames) in [PR 1165](https:\\u002f\\u002fgithub....\"],[\"- 962 dataframe by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1186](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- add copy functionality to json by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1205](https:\\u002f\\u002fgithub.c...\"],[\"- Hotfixes for course demos by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1222](https:\\u002f\\u002fgithub....\"],[\"- ensure defaults height match for media inputs by [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn) in [PR 1236](h...\"],[\"- Labels spacing by [@gary149](https:\\u002f\\u002fgithub.com\\u002fgary149) in [PR 1254](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- Fixes to components by [@abidlabs](https:\\u002f\\u002fgithub.com\\u002fabidlabs) in [PR 1260](https:\\u002f\\u002fgithub.com\\u002fgr...\"],[\"- Add embedded demos to website by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1270](https:\\u002f\\u002fg...\"],[\"- Create Streamables by [@aliabid94](https:\\u002f\\u002fgithub.com\\u002faliabid94) in [PR 1279](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"- Mobile responsive guides by [@aliabd](https:\\u002f\\u002fgithub.com\\u002faliabd) in [PR 1293](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"### Contributors Shoutout:\\n\\n- [@JefferyChiang](https:\\u002f\\u002fgithub.com\\u002fJefferyChiang) made their first co...\"],[\"@gradio\\u002fstatustracker\\n\\n## 0.4.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6...\"],[\"## 0.4.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.2\\n\\n### Patch Changes\\n\\n- Updated dependencies...\"],[\"## 0.3.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5342](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5342) [`afac0006`](https...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5215) [`fbdad78a`](https...\"],[\"create-svelte\\n\\nEverything you need to build a Svelte project, powered by [`create-svelte`](https:\\u002f\\u002fg...\"],[\"imple image classification in Pytorch with Gradio's Image input and Label output....\"],[\"`@gradio\\u002fatoms`\\n\\n```html\\n\\u003cscript lang=\\\"ts\\\"\\u003e\\n\\timport { Block, BlockTitle, BlockLabel, IconButton, Emp...\"],[\"```\\n\\nShareButton:\\n```javascript\\n\\texport let formatter: (arg0: any) =\\u003e Promise\\u003cstring\\u003e;\\n\\texport let v...\"],[\"his text generation demo works like autocomplete. There's only one textbox and it's used for both th...\"],[\"Gradio Demo: sound_alert\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo r...\"],[\"Gradio Demo: theme_extended_step_2\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimpor...\"],[\"ÂÆûÊó∂ËØ≠Èü≥ËØÜÂà´\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused, https:\\u002f\\u002fhugging...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-streaming-asr-paused.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"350\\\" title=\\\"Gra...\"],[\"‰∏ãÈù¢ÊòØÊûÑÂª∫ÂÆûÊó∂ËØ≠Èü≥ËØÜÂà´ÔºàASRÔºâÂ∫îÁî®Á®ãÂ∫èÁöÑÊ≠•È™§Ôºö\\n\\n1. [ËÆæÁΩÆ Transformers ASR Ê®°Âûã](#1-set-up-the-transformers-asr-model)\\n2. [‰ΩøÁî® T...\"],[\"```\\n\\nÂ∞±ÊòØËøôÊ†∑ÔºÅÈªòËÆ§ÊÉÖÂÜµ‰∏ãÔºåËá™Âä®ËØ≠Èü≥ËØÜÂà´Ê®°ÂûãÁÆ°ÈÅì‰ºöÂä†ËΩΩ Facebook ÁöÑ `facebook\\u002fwav2vec2-base-960h` Ê®°Âûã„ÄÇ\\n\\n## 2. ‰ΩøÁî® Transformers ÂàõÂª∫...\"],[\"```\\n\\nÈÇ£‰πàËøôÈáåÂèëÁîü‰∫Ü‰ªÄ‰πàÔºü`transcribe` ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™ÂèÇÊï∞ `audio`ÔºåÂÆÉÊòØÁî®Êà∑ÂΩïÂà∂ÁöÑÈü≥È¢ëÊñá‰ª∂ÁöÑÊñá‰ª∂Ë∑ØÂæÑ„ÄÇ`pipeline` ÂØπË±°ÊúüÊúõ‰∏Ä‰∏™Êñá‰ª∂Ë∑ØÂæÑÔºåÂπ∂Â∞ÜÂÖ∂ËΩ¨Êç¢‰∏∫ÊñáÊú¨ÔºåÁÑ∂ÂêéËøîÂõûÂà∞ÂâçÁ´Ø...\"],[\"Â•ΩÊ∂àÊÅØÊòØÔºåÊàë‰ª¨ÂèØ‰ª•ÂæàÂÆπÊòìÂú∞Ë∞ÉÊï¥ÂàöÂàöÂàõÂª∫ÁöÑÊºîÁ§∫Ôºå‰ΩøÂÖ∂Êàê‰∏∫ÊµÅÂºèÁöÑÔºå‰ΩøÁî®Áõ∏ÂêåÁöÑ `Wav2Vec2` Ê®°Âûã„ÄÇ\\n\\nÊúÄÂ§ßÁöÑÂèòÂåñÊòØÊàë‰ª¨Áé∞Âú®ÂøÖÈ°ªÂºïÂÖ•‰∏Ä‰∏™ `state` ÂèÇÊï∞ÔºåÂÆÉ‰øùÂ≠òÂà∞ÁõÆÂâç‰∏∫Ê≠¢*ËΩ¨ÂΩïÁöÑÈü≥È¢ë*„ÄÇËøôÊ†∑Ôºå...\"],[\"```\\n\\nËØ∑Ê≥®ÊÑèÔºåÊàë‰ª¨ËøòËøõË°å‰∫ÜÂè¶‰∏Ä‰∏™Êõ¥ÊîπÔºåÂç≥Êàë‰ª¨ËÆæÁΩÆ‰∫Ü `live=True`„ÄÇËøô‰ΩøÂæó Gradio Êé•Âè£‰øùÊåÅÊåÅÁª≠ËøêË°åÔºåÂõ†Ê≠§ÂÆÉÂèØ‰ª•Ëá™Âä®ËΩ¨ÂΩïÈü≥È¢ëÔºåËÄåÊó†ÈúÄÁî®Êà∑ÂèçÂ§çÁÇπÂáªÊèê‰∫§ÊåâÈíÆ„ÄÇ\\n\\nËÆ©Êàë‰ª¨ÁúãÁúãÂÆÉÁöÑÊïàÊûúÔºàÂú®‰∏ã...\"],[\"```python\\nfrom transformers import pipeline\\nimport gradio as gr\\nimport time\\n\\np = pipeline(\\\"automatic...\"],[\"```\\n\\nÂ∞ùËØï‰∏ãÈù¢ÁöÑÊºîÁ§∫ÔºåÊü•ÁúãÂ∑ÆÂºÇÔºàÊàñ[Âú®Êñ∞Ê†áÁ≠æÈ°µ‰∏≠ÊâìÂºÄ](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fstreaming-asr-paused)ÔºâÔºÅ\\n\\n\\u003cifram...\"],[\"‰∏ãÈù¢ÊòØ‰∏Ä‰∏™ÂÆåÊï¥ÁöÑÁ§∫‰æãÔºàÂú® Linux ‰∏äÔºâÔºö\\n\\nÈ¶ñÂÖàÈÄöËøáÁªàÁ´ØÂÆâË£Ö DeepSpeech Â∫ìÂπ∂‰∏ãËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºö\\n\\n```bash\\nwget https:\\u002f\\u002fgithub.com\\u002fmozilla\\u002fDeep...\"],[\"```\\n\\nÁÑ∂ÂêéÔºåÂàõÂª∫‰∏é‰πãÂâçÁõ∏‰ººÁöÑ `transcribe()` ÂáΩÊï∞Ôºö\\n\\n```python\\nfrom deepspeech import Model\\nimport numpy as np\\n\\nmode...\"],[\"```\\n\\nËøêË°åÊâÄÊúâËøô‰∫õÂ∫îËØ•ÂÖÅËÆ∏ÊÇ®‰ΩøÁî®‰∏Ä‰∏™ÊºÇ‰∫ÆÁöÑ GUI ÈÉ®ÁΩ≤ÂÆûÊó∂ ASR Ê®°Âûã„ÄÇÂ∞ùËØï‰∏Ä‰∏ãÔºåÁúãÂÆÉÂú®ÊÇ®ÈÇ£ÈáåËøêË°åÂæóÊúâÂ§öÂ•Ω„ÄÇ\\n\\n---\\n\\n‰Ω†Â∑≤ÁªèÂÆåÊàê‰∫ÜÔºÅËøôÂ∞±ÊòØÊûÑÂª∫Áî®‰∫é ASR Ê®°ÂûãÁöÑÂü∫‰∫é Web ÁöÑ GUI ...\"],[\"Gradio Demo: sine_curve\\n\\n\\n```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\nimport gradio as g...\"],[\"his text generation demo takes in input text and returns generated text. It uses the Transformers li...\"],[\"Gradio Demo: color_generator\\n\\n\\n```\\n!pip install -q gradio opencv-python numpy\\n```...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport cv2\\nimport numpy as np\\nimport random\\n\\n\\n# Convert decimal color ...\"],[\"@gradio\\u002fclient\\n\\n## 0.9.3\\n\\n### Features\\n\\n- [#6814](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6814) [`...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#6398](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6398) [`67ddd40`](https:...\"],[\"## 0.8.1\\n\\n### Fixes\\n\\n- [#6383](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6383) [`324867f63`](https:\\u002f...\"],[\"## 0.7.1\\n\\n### Features\\n\\n- [#6137](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6137) [`2ba14b284`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.7.0-beta.1\\n\\n### Features\\n\\n- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40...\"],[\"## 0.7.0-beta.0\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.5.2\\n\\n### Fixes\\n\\n- [#5840](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5840) [`4e62b8493`](https:\\u002f...\"],[\"Thanks to a new capability that allows components to communicate directly with the server _without_ ...\"],[\"### Fixes\\n\\n- [#5776](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5776) [`c0fef4454`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5682](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5682) [`c57f1b75e`](http...\"],[\"## 0.3.1\\n\\n### Fixes\\n\\n- [#5412](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5412) [`26fef8c7`](https:\\u002f\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5133](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5133) [`61129052`](https...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- [#4717](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4717) [`ab5d1ea0`](...\"],[\"- [#4315](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4315) [`b525b122`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#4202](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f4202) [`a26e9afd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"- [#3605](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f3605) [`ae4277a9`](https:\\u002f\\u002fgithub.com\\u002fgradio-app...\"],[\"@gradio\\u002flabel\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`3cdeabc68`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5215](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"Case Study: A Component to Display PDFs\\n\\nLet's work through an example of building a custom gradio c...\"],[\"```\\n\\n\\nTip: You should change the name of the component.\\nSome of the screenshots assume the component...\"],[\"The complete `package.json` should look like this:\\n\\n```json\\n{\\n  \\\"name\\\": \\\"gradio_pdf\\\",\\n  \\\"version\\\": \\\"...\"],[\"```\\n\\n\\nTip: Running `npm install` will install the latest version of the package available. You can i...\"],[\"```\\n\\n## Step 3: Frontend - Launching the Dev Server\\n\\nRun the `dev` command to launch the development...\"],[\"export let elem_id = \\\"\\\";\\n\\texport let elem_classes: string[] = [];\\n\\texport let visible = true;\\n\\texpor...\"],[\"```\\n\\n\\nTip: The `gradio`` object passed in here contains some metadata about the application as well ...\"],[\"```\\n\\nYou should see the following when you navigate to your app after saving your current changes:\\n\\n...\"],[\"```\\n\\nNow import `PdfUploadText.svelte` in your `\\u003cscript\\u003e` and pass it to the `Upload` component!\\n\\n``...\"],[\"```\\n\\nAlso create the following variables:\\n\\n```ts\\n    let pdfDoc;\\n    let numPages = 1;\\n    let curre...\"],[\"```\\n\\n\\nTip: The `$:` syntax in svelte is how you declare statements to be reactive. Whenever any of t...\"],[\"```\\n\\n\\nTip: The `gradio.dispatch` method is actually what is triggering the `change` or `upload` even...\"],[\"```\\n\\nCongratulations! You have a working pdf uploader!\\n\\n![upload-gif](https:\\u002f\\u002fgradio-builds.s3.amazo...\"],[\"```\\n\\nCongratulations! The frontend is almost complete üéâ\\n\\n![multipage-pdf-gif](https:\\u002f\\u002fgradio-builds....\"],[\"\\u003cdiv\\n\\tclass:table={type === \\\"table\\\"}\\n\\tclass:gallery={type === \\\"gallery\\\"}\\n\\tclass:selected\\n\\tstyle=\\\"jus...\"],[\"```\\n\\n\\nTip: Exercise for the reader - reduce the code duplication between `Index.svelte` and `Example...\"],[\"class PDF(Component):\\n\\n    EVENTS = [\\\"change\\\", \\\"upload\\\"]\\n\\n    data_model = FileData\\n\\n    def __init_...\"],[\"```\\n\\n## Step 10: Add a demo and publish!\\n\\nTo test our backend code, let's add a more complex demo th...\"],[\"```\\n\\nSee our demo in action below!\\n\\n\\u003cvideo autoplay muted loop\\u003e\\n  \\u003csource src=\\\"https:\\u002f\\u002fgradio-builds...\"],[\"```\\n\\n\\nI hope you enjoyed this tutorial!\\nThe complete source code for our component is [here](https:\\u002f...\"],[\"Gradio Demo: stream_audio_out\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the d...\"],[\"```\\nimport gradio as gr\\nfrom pydub import AudioSegment\\nfrom time import sleep\\n\\nwith gr.Blocks() as d...\"],[\"gradio-ui\\n\\nThis folder contains all of the Gradio UI and component source code.\\n\\n- [set up](#setup)\\n...\"],[\"```\\n\\nIf you have formatting failures then you can run the following command to fix them:\\n\\n```bash\\npn...\"],[\"Gradio Demo: gallery_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr \\n\\nwith gr...\"],[\"Gradio Demo: blocks_scroll\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndemo = gr.B...\"],[\"website\\n\\n## 0.20.3\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.3.3\\n\\n## 0.20.2\\n...\"],[\"## 0.19.0\\n\\n### Features\\n\\n- [#5885](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5885) [`9919b8a`](https...\"],[\"## 0.17.0\\n\\n### Features\\n\\n- [#6533](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6533) [`e2810fcfc`](htt...\"],[\"## 0.15.0\\n\\n### Features\\n\\n- [#6436](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6436) [`58e3ca826`](htt...\"],[\"## 0.14.0\\n\\n### Features\\n\\n- [#6387](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6387) [`9d6d72f44`](htt...\"],[\"### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002fcode@0.2.3\\n\\n## 0.12.0\\n\\n### Features\\n\\n- [#6...\"],[\"## 0.11.1\\n\\n### Features\\n\\n- [#6189](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6189) [`345ddd888`](htt...\"],[\"- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.11.0-beta.0\\n\\n### Features...\"],[\"- [#6082](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6082) [`037e5af33`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6097](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6097) [`439efa39d`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.9.0\\n\\n### Features\\n\\n- [#5386](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5386) [`0312c990f`](http...\"],[\"## 0.7.0\\n\\n### Features\\n\\n- [#5643](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5643) [`f661c0733`](http...\"],[\"### Fixes\\n\\n- [#5608](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5608) [`eebf9d71f`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5423](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5423) [`bb31cd7d`](https...\"],[\"### Fixes\\n\\n- [#5304](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5304) [`05892302`](https:\\u002f\\u002fgithub.com...\"],[\"## 0.2.1\\n\\n### Fixes\\n\\n- [#5324](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5324) [`31996c99`](https:\\u002f\\u002f...\"],[\"Thanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n### Features\\n\\n- [#5298](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"## 0.1.0\\n\\n### Features\\n\\n- [#5076](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5076) [`2745075a`](https...\"],[\"## 0.0.2\\n\\n### Features\\n\\n- [#5009](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5009) [`3e70fc81`](https...\"],[\"Gradio Demo: live_dashboard\\n### This demo shows how you can build a live interactive dashboard with ...\"],[\"```\\n!pip install -q gradio plotly\\n```\\n\\n\\n```\\nimport math\\n\\nimport pandas as pd\\n\\nimport gradio as gr\\nim...\"],[\"Image Classification in TensorFlow and Keras\\n\\nRelated spaces: https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs...\"],[\"```\\n\\nThis line automatically downloads the MobileNet model and weights using the Keras library.\\n\\n## ...\"],[\"```\\n\\nLet's break this down. The function takes one parameter:\\n\\n- `inp`: the input image as a `numpy`...\"],[\"Gradio Demo: queue_full_e2e_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\nimport ...\"],[\"Gradio Demo: blocks_simple_squares\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo...\"],[\"Gradio Demo: blocks_static\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndemo = gr.Bl...\"],[\"Gradio Demo: main_note\\n\\n\\n```\\n!pip install -q gradio scipy numpy matplotlib\\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\nfrom math import log2, pow\\nimport os\\n\\nimport numpy as np\\nfrom scipy.fftpack import fft\\n\\nimport g...\"],[\"@gradio\\u002fatoms\\n\\n## 0.4.1\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.3.0\\n\\n### Highlights\\n\\n#### New `ImageEditor` component ([#6169](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgr...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.2.2\\n\\n### Fixes\\n\\n- [#6254](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#6136](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6136) [`667802a6c...\"],[\"## 0.2.0-beta.4\\n\\n### Features\\n\\n- [#5938](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5938) [`13ed8a485...\"],[\"## 0.1.4\\n\\n### Patch Changes\\n\\n- Updated dependencies []:\\n  - @gradio\\u002futils@0.1.2\\n\\n## 0.1.3\\n\\n### Patch...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"demo for predicting the depth of an image and generating a 3D model of it....\"],[\"Gradio Demo: video_component\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the de...\"],[\"TensorFlow Âíå Keras ‰∏≠ÁöÑÂõæÂÉèÂàÜÁ±ª\\n\\nÁõ∏ÂÖ≥Á©∫Èó¥Ôºöhttps:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fabidlabs\\u002fkeras-image-classifier\\nÊ†áÁ≠æÔºöVIS...\"],[\"ËÆ©Êàë‰ª¨ÂºÄÂßãÂêßÔºÅ\\n\\n### ÂÖàÂÜ≥Êù°‰ª∂\\n\\nÁ°Æ‰øùÊÇ®Â∑≤Áªè[ÂÆâË£Ö](\\u002fgetting_started)‰∫Ü `gradio` Python ÂåÖ„ÄÇÊàë‰ª¨Â∞Ü‰ΩøÁî®‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉÁöÑ Keras ÂõæÂÉèÂàÜÁ±ªÊ®°ÂûãÔºåÂõ†Ê≠§ÊÇ®ËøòÂ∫îËØ•ÂÆâË£Ö‰∫Ü...\"],[\"```\\n\\nÊ≠§Ë°å‰ª£Á†ÅÂ∞Ü‰ΩøÁî® Keras Â∫ìËá™Âä®‰∏ãËΩΩ MobileNet Ê®°ÂûãÂíåÊùÉÈáç„ÄÇ\\n\\n## Á¨¨‰∫åÊ≠• ‚Äî‚Äî ÂÆö‰πâ `predict` ÂáΩÊï∞\\n\\nÊé•‰∏ãÊù•ÔºåÊàë‰ª¨ÈúÄË¶ÅÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞ÔºåËØ•ÂáΩÊï∞Êé•Êî∂*Áî®Êà∑ËæìÂÖ•*‰Ωú‰∏∫ÂèÇÊï∞...\"],[\"```\\n\\nËÆ©Êàë‰ª¨Êù•ËØ¶ÁªÜ‰∫ÜËß£‰∏Ä‰∏ã„ÄÇËØ•ÂáΩÊï∞Êé•Âèó‰∏Ä‰∏™ÂèÇÊï∞Ôºö\\n\\n- `inp`ÔºöËæìÂÖ•ÂõæÂÉèÁöÑ `numpy` Êï∞ÁªÑ\\n\\nÁÑ∂ÂêéÔºåÂáΩÊï∞Ê∑ªÂä†‰∏Ä‰∏™ÊâπÊ¨°Áª¥Â∫¶ÔºåÈÄöËøáÊ®°ÂûãËøõË°åÂ§ÑÁêÜÔºåÂπ∂ËøîÂõûÔºö\\n\\n- `confidences`ÔºöÈ¢Ñ...\"],[\"```\\n\\nËøôÂ∞ÜÁîüÊàê‰ª•‰∏ãÁïåÈù¢ÔºåÊÇ®ÂèØ‰ª•Âú®ÊµèËßàÂô®‰∏≠Á´ãÂç≥Â∞ùËØïÔºàÂ∞ùËØï‰∏ä‰º†ÊÇ®Ëá™Â∑±ÁöÑÁ§∫‰æãÔºÅÔºâÔºö\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fabidlabs-keras-image-classifier.hf.sp...\"],[\"his demo identifies if two speakers are the same person using Gradio's Audio and HTML components....\"],[\"Gradio Demo: image_classifier_interface_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading f...\"],[\"```\\n\\n\\n```\\nimport gradio as gr\\nimport pathlib\\n\\ncurrent_dir = pathlib.Path(__file__).parent\\n\\nimages = ...\"],[\"@gradio\\u002fimage\\n\\n## 0.5.3\\n\\n### Fixes\\n\\n- [#6766](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6766) [`7326...\"],[\"## 0.5.0\\n\\n### Features\\n\\n- [#6726](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6726) [`21cfb0a`](https:...\"],[\"### Fixes\\n\\n- [#6709](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6709) [`6a9151d`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nThanks [@pngwn](https:\\u002f\\u002fgithub.com\\u002fpngwn)!\\n\\n## 0.3.6\\n\\n### Fixes\\n\\n- [#6441](https:\\u002f\\u002fgithub.com\\u002fg...\"],[\"## 0.3.4\\n\\n### Features\\n\\n- [#6363](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6363) [`4d3aad33a`](http...\"],[\"## 0.3.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`bca6c2c80`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2ba14b284`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.9\\n\\n### Features...\"],[\"- [#6143](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6143) [`e4f7b4b40`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"### Fixes\\n\\n- [#6146](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6146) [`40a171ea6`](https:\\u002f\\u002fgithub.co...\"],[\"### Fixes\\n\\n- [#6046](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6046) [`dbb7de5e0`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#5627](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5627) [`b67115e8e`](http...\"],[\"## 0.3.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.3\\n\\n### Fixes\\n\\n- [#5528](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5528) [`dc86e4a7`](https:\\u002f\\u002f...\"],[\"## 0.2.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`abf1c57d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"## 0.1.1\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"Gradio Demo: latex\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.Blocks() as ...\"],[\"Gradio Demo: video_identity\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the dem...\"],[\"@gradio\\u002ffallback\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`f816136a0`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.2.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.0-beta.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`14fc612d8`](https:\\u002f\\u002fgithub.com\\u002fgradio-...\"],[\"## 0.2.0-beta.0\\n\\n### Features\\n\\n- [#5507](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5507) [`1385dc688...\"],[\"We now have better support for markdown in `gr.Markdown` and `gr.Dataframe`. Including syntax highli...\"],[\"üöÄ Creating Discord Bots from Gradio Apps üöÄ\\n\\nTags: NLP, TEXT, CHAT\\n\\nWe're excited to announce that Gr...\"],[\"```\\n\\n### Step 2: Deploying our App\\n\\nIn order to create a discord bot for our app, it must be accessi...\"],[\"```\\n\\n### Step 5: Add the bot to your server\\n\\nVisit the space of your discord bot. You should see \\\"Ad...\"],[\"```\\n\\n## ü¶æ Using State of The Art LLMs ü¶æ\\n\\nWe have created an organization on Hugging Face called [gra...\"],[\"```\\n\\n## ü¶ú Additional LLMs ü¶ú\\n\\nIn addition to Meta's 70 billion Llama 2 model, we have prepared templa...\"],[\"Gradio Demo: blocks_essay\\n\\n\\n```\\n!pip install -q gradio \\n```...\"],[\"```\\nimport gradio as gr\\n\\ncountries_cities_dict = {\\n    \\\"USA\\\": [\\\"New York\\\", \\\"Los Angeles\\\", \\\"Chicago\\\"]...\"],[\"def reset_bounds(minimum, maximum):\\n        return gr.Number(minimum=minimum, maximum=maximum)\\n\\n    ...\"],[\"Gradio Demo: generate_tone\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport gr...\"],[\"`@gradio\\u002fform`\\n\\n```html\\n\\u003cscript\\u003e\\n\\timport { Form } from \\\"@gradio\\u002fform\\\";\\n\\u003c\\u002fscript\\u003e\\n```\\n\\nForm\\n```javasc...\"],[\"Gradio Demo: hello_world_3\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\ndef greet(na...\"],[\"Gradio Components: The Key Concepts\\n\\nIn this section, we discuss a few important concepts when it co...\"],[\"```\\n\\nThe interactive version of the component is much more complex -- you can upload images or snap ...\"],[\"```python\\nimport numpy as np\\nimport gradio as gr\\n\\ndef sepia(input_img):\\n    sepia_filter = np.array(...\"],[\"```\\n\\nThis will create a Gradio app which has an `Image` component as the input and the output. \\nIn t...\"],[\"* As a component author, **YOU** control the format of the data displayed in the frontend as well as...\"],[\"### What you need to remember\\n\\n* If you expect your component to be used as input, it is important t...\"],[\"Real Time Speech Recognition\\n\\nTags: ASR, SPEECH, STREAMING\\n\\n## Introduction\\n\\nAutomatic speech recogn...\"],[\"Here's how to build a real time speech recognition (ASR) app:\\n\\n1. [Set up the Transformers ASR Model...\"],[\"```\\n\\nThat's it!\\n\\n## 2. Create a Full-Context ASR Demo with Transformers\\n\\nWe will start by creating a...\"],[\"Gradio Demo: unified_demo_text_generation\\n\\n\\n```\\n!pip install -q gradio torch transformers\\n```\\n\\n\\n```\\n...\"],[\"Gradio Demo: calculator_blocks_cached\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\n...\"],[\"Gradio Demo: image-simple\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo ...\"],[\"`@gradio\\u002fimageeditor`...\"],[\"Gradio Demo: chatbot_multimodal\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the...\"],[\"```\\nimport gradio as gr\\nimport os\\nimport time\\n\\n# Chatbot demo with multimodal input (text, markdown,...\"],[\"txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=False).then(\\n        bot, chatb...\"],[\"Gradio Demo: dataframe_block-ui-test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwi...\"],[\"Gradio Demo: on_listener_test\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwith gr.B...\"],[\"The Backend üêç\\n\\nThis guide will cover everything you need to know to implement your custom component'...\"],[\"```\\n\\n## The methods you need to implement\\n\\nWhen you inherit from any of these classes, the following...\"],[\"```\\n\\n### `api_info`\\n\\nA JSON-schema representation of the value that the `preprocess` expects. \\nThis ...\"],[\"```\\n\\n### `read_from_flag`\\nConvert from the format stored in the `csv` or `json` file used for flaggi...\"],[\"```\\n\\nBy adding these four lines of code, your component automatically implements the methods needed ...\"],[\"```\\n\\nEven if your component does not expect a \\\"complex\\\" JSON data structure it can be beneficial to ...\"],[\"Gradio Demo: blocks_flag\\n\\n\\n```\\n!pip install -q gradio numpy\\n```\\n\\n\\n```\\nimport numpy as np\\nimport grad...\"],[\"Gradio Demo: concurrency_without_queue\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\ni...\"],[\"Gradio Demo: dashboard\\n### This demo shows how you can build an interactive dashboard with gradio. C...\"],[\"```\\nimport gradio as gr\\nimport pandas as pd\\nimport plotly.express as px\\nfrom helpers import *\\n\\n\\nLIBR...\"],[\"def create_issue_plot(libraries, issue_choices):\\n    if \\\"Issue\\\" not in issue_choices:\\n        return...\"],[\"if __name__ == \\\"__main__\\\":\\n    demo.launch()...\"],[\"@gradio\\u002fslider\\n\\n## 0.2.6\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](http...\"],[\"## 0.2.0-beta.8\\n\\n### Features\\n\\n- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd...\"],[\"## 0.2.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"### Fixes\\n\\n- [#5984](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5984) [`66549d8d2`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`e70805d54`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5697](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5697) [`f4e4f82b5`](http...\"],[\"## 0.1.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`afac0006`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgrad...\"],[\"##### Various performance improvements\\n\\nThese improvements will be particularly beneficial to large ...\"],[\"@gradio\\u002fmodel3d\\n\\n## 0.4.11\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`828fb9e`](https:\\u002f\\u002fgithub.co...\"],[\"## 0.4.8\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`6a9151d`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.7\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`206af31`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradi...\"],[\"## 0.4.4\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`2f805a7dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`854b482f5`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.4.0\\n\\n### Features\\n\\n- [#6240](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6240) [`dd901c1b0`](http...\"],[\"- [#5498](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5498) [`287fe6782`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.8\\n\\n### Features...\"],[\"- [#6149](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6149) [`90318b1dd`](https:\\u002f\\u002fgithub.com\\u002fgradio-ap...\"],[\"## 0.3.0-beta.7\\n\\n### Features\\n\\n- [#6016](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f6016) [`83e947676...\"],[\"## 0.3.0-beta.6\\n\\n### Features\\n\\n- [#5960](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5960) [`319c30f3f...\"],[\"## 0.2.3\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`8f0fed857`](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgra...\"],[\"## 0.2.0\\n\\n### Features\\n\\n- [#5373](https:\\u002f\\u002fgithub.com\\u002fgradio-app\\u002fgradio\\u002fpull\\u002f5373) [`79d8f9d8`](https...\"],[\"##### Improved markdown support\\n\\nWe now have better support for markdown in `gr.Markdown` and `gr.Da...\"],[\"# @gradio\\u002fmodel3D\\n\\n## 0.0.2\\n\\n### Patch Changes\\n\\n- Updated dependencies [[`667875b2`](https:\\u002f\\u002fgithub....\"],[\"mport { Meta } from \\\"@storybook\\u002fblocks\\\";\\n\\n\\u003cMeta title=\\\"Introduction\\\" \\u002f\\u003e\\n\\n\\u003cstyle\\u003e\\n\\t{`\\n    img {\\n     ...\"],[\"\\u003cdiv class=\\\"divider\\\" \\u002f\\u003e\\n\\n\\u003cdiv class=\\\"subheading\\\"\\u003eResources\\u003c\\u002fdiv\\u003e\\n\\u003cul\\u003e\\n\\n  \\u003cli\\u003e\\u003ca href=\\\"https:\\u002f\\u002fgradio...\"],[\"simple demo showcasing the upload button used with its `upload` event trigger....\"],[\"`@gradio\\u002fhighlightedtext`\\n\\n```html\\n\\u003cscript\\u003e\\n    import { BaseStaticHighlightedText, BaseInteractiveH...\"],[\"Gradio Demo: image_component_events\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\nwit...\"],[\"Gradio Demo: reverse_audio\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\n# Downloading files from the demo...\"],[\"Gradio Demo: blocks_page_load\\n\\n\\n```\\n!pip install -q gradio \\n```\\n\\n\\n```\\nimport gradio as gr\\n\\n\\ndef prin...\"],[\"Gradio & LLM Agents ü§ù\\n\\nLarge Language Models (LLMs) are very impressive but they can be made even mo...\"],[\"## gradio_tools - An end-to-end example\\n\\nTo get started with `gradio_tools`, all you need to do is i...\"],[\"agent = initialize_agent(tools, llm, memory=memory, agent=\\\"conversational-react-description\\\", verbos...\"],[\"```\\n\\nYou'll note that we are using some pre-built tools that come with `gradio_tools`. Please see th...\"],[\"```\\n\\nThe requirements are:\\n\\n1. The name for your tool\\n2. The description for your tool. This is cruc...\"],[\"And that's it!\\n\\nOnce you have created your tool, open a pull request to the `gradio_tools` repo! We ...\"],[\"```\\n\\nSome notes on this implementation:\\n\\n1. All instances of `GradioTool` have an attribute called `...\"]],\"hovertemplate\":\"source=gradio\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"gradio, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"gradio, circle\",\"showlegend\":true,\"x\":[11.285637,11.184846,12.218112,11.71761,12.04979,10.572504,11.3417425,11.816092,11.25457,11.239903,11.242276,11.368923,11.83025,12.823364,13.565293,11.137639,11.130221,11.190616,11.217422,11.005311,12.622884,12.709434,12.381922,12.002629,11.541098,12.346214,14.704404,12.412649,12.449102,12.942163,12.866348,-2.076223,-1.7742231,-2.164114,-2.1383834,-2.01828,-1.8253218,12.562261,12.573146,13.1957655,12.755314,12.950945,13.613377,16.200579,18.815798,17.382267,16.756674,15.002191,18.77501,12.741026,12.871961,15.8145275,13.459662,12.399815,4.221043,4.1838408,4.129345,3.7262895,4.161925,4.1238475,4.034951,12.633128,12.895838,10.514509,10.388111,10.521377,10.689321,10.3894,10.769398,10.599315,10.433635,10.429557,10.411829,10.472293,10.513582,10.407706,10.496311,4.0146284,3.5468433,10.706514,10.541722,11.010049,12.39998,17.927889,16.80922,16.163893,16.73008,16.906378,17.068394,15.975456,17.451715,15.928072,16.156134,15.794574,16.809223,16.789358,14.560817,15.997238,14.710638,15.520715,15.938797,16.625309,15.385455,16.019402,14.723817,16.735075,16.639639,16.44727,15.666799,14.660586,18.031872,16.729988,16.471048,14.598107,16.025671,14.6473465,16.904762,16.229527,16.125559,16.63569,15.146547,16.074873,15.539231,16.757599,14.741564,15.898208,16.272633,15.155949,16.601137,15.5150795,14.728167,16.406796,15.423815,16.154352,16.74658,16.912935,15.491241,16.142796,15.874656,17.395205,16.788805,15.520343,16.486317,15.910128,15.767028,15.828736,16.595022,17.9397,17.127151,16.401737,16.972692,16.570618,14.986791,13.640255,13.603555,13.429595,12.769351,13.19051,16.979803,16.772053,16.925175,15.683299,16.390305,15.735074,16.723347,16.666044,16.903694,16.732252,17.26522,15.311306,15.914068,16.654894,16.933655,15.973839,16.673162,16.624556,16.822943,16.06619,16.110476,16.165249,16.576025,16.969774,16.266953,16.423368,17.215363,16.131329,16.846514,16.831886,18.255375,16.506556,15.948273,16.324272,17.031046,16.696604,16.843477,16.672373,16.875717,16.918512,14.723254,15.587184,15.502032,15.785821,16.111807,16.403933,16.47988,16.901936,16.874344,16.820961,16.017138,16.838894,16.207933,16.341335,15.81192,15.386041,16.805336,16.674986,16.826126,16.800657,16.337471,17.253653,17.057022,16.447472,16.268263,16.708221,16.336058,14.723072,17.045567,16.030125,15.748397,17.213179,15.528709,16.382278,15.42828,15.915804,15.920808,15.488161,14.725118,15.969302,15.736749,16.411283,15.327859,15.202241,16.949852,16.980803,15.558717,16.131865,13.493765,3.1945481,15.0303,15.44947,16.456495,15.792612,15.809626,16.188215,17.987093,15.74291,14.959752,3.117669,17.572117,15.893938,16.567327,16.558573,15.95867,14.918591,15.624622,17.04162,15.466338,15.4431715,15.502993,15.530824,15.028529,12.331584,2.9777956,15.955866,15.742062,15.97581,15.567955,15.994533,16.657017,15.418717,15.20323,16.998945,16.477043,15.670193,13.662181,13.210034,13.577454,14.770276,14.22352,16.320362,14.451517,13.087235,15.963933,13.929107,13.898585,14.145782,12.701489,16.062834,15.906109,17.146397,16.042349,16.02166,14.804648,16.01381,15.882652,16.209738,15.560581,15.941758,14.797282,16.479338,15.771765,16.020885,15.555837,15.66647,15.036739,2.484834,14.802485,15.983046,15.506884,16.291988,16.174606,15.939799,16.334373,15.974114,16.15109,16.039907,16.053637,16.213322,15.928004,16.99463,16.283987,15.999387,15.964474,16.488808,15.014663,15.84808,16.042747,16.386059,15.529163,16.18737,15.986408,16.019337,15.18104,15.546132,16.306837,16.065674,15.975627,15.950232,15.99303,16.394867,16.278212,12.540737,14.748432,14.247019,12.916748,14.787501,16.16515,16.48366,15.821416,16.031204,15.35332,15.953259,15.489172,16.72643,15.70911,15.945851,16.574228,15.935415,16.050848,15.933034,15.443414,16.040773,3.1569529,3.0244594,14.1482935,13.604495,13.205343,15.442165,16.162159,16.689735,16.352869,16.035213,15.350885,14.064011,14.537256,15.839404,14.652227,15.881173,15.881483,15.83519,14.707956,15.997729,15.764552,16.274538,12.520045,15.248989,15.401915,16.110163,16.157593,16.072248,16.254805,16.543434,15.981214,15.775343,16.022087,16.198553,16.234474,15.917165,16.005308,15.744345,13.70101,15.116274,16.135284,15.945723,15.940008,15.889818,15.902277,16.088253,15.500034,16.127329,15.886014,15.990424,14.607735,14.768682,16.156977,15.573662,15.333674,12.413791,15.531917,15.5738325,15.648174,16.579136,16.009802,14.091628,15.009505,12.429943,12.587961,13.715547,14.979257,15.989646,15.594363,12.985469,12.873672,14.094724,16.538765,16.09421,14.66279,14.482703,16.708319,16.001646,15.97306,16.215832,15.948807,16.138325,13.096632,16.02268,16.002832,12.404659,16.051497,15.333487,14.737896,13.355053,15.945289,15.7515745,16.077461,15.546121,15.208555,2.3551764,12.773304,15.863822,15.58233,16.01341,16.305004,16.05512,15.508351,16.01799,15.952142,15.562911,16.750624,15.987569,16.504784,15.690882,16.325836,15.663935,13.157735,13.176657,14.776094,16.534172,16.306988,16.555094,16.534225,16.547314,15.213893,14.605408,14.820172,15.896758,16.727882,15.509439,15.96433,12.903513,12.9200325,15.825503,16.62056,16.283287,16.165102,15.067969,15.54646,12.295451,12.900476,13.073321,14.735989,15.505894,15.091949,16.909166,16.572924,16.813177,16.003939,14.818819,16.271166,15.413886,15.839494,15.316773,15.865697,16.885294,15.32361,14.9734335,16.534155,15.682263,15.520788,16.387032,15.607917,15.854917,15.058484,14.577456,15.142455,15.942388,15.903239,17.01537,15.853467,15.219826,15.543217,15.805358,15.656481,15.611674,15.408833,15.654429,15.638567,18.046507,17.03279,18.617174,18.80366,18.906164,17.722748,17.453974,2.7355483,-1.2334329,14.911788,3.3041916,2.995721,12.596114,12.837898,11.133316,11.495692,2.63784,12.001692,11.099786,11.448806,11.269301,11.6592045,11.243999,3.396968,11.19586,11.047793,12.594155,-5.953258,12.82378,12.702579,16.364805,16.842457,17.289286,17.297579,17.618294,18.388601,17.358324,16.429571,16.198086,16.79912,17.31297,16.944555,18.014591,18.237429,16.871227,16.597666,16.197144,19.195168,19.222034,19.204798,17.993723,17.32403,18.645,19.168327,17.98766,17.348246,14.610416,14.8812895,15.128945,15.227521,14.772272,14.619322,14.59446,14.615314,14.638403,14.70208,14.664289,14.539383,14.592315,14.755901,3.836006,14.515687,14.261598,11.946163,14.797308,13.537294,12.445643,12.292753,15.257984,2.8051353,12.634394,12.834916,18.206738,17.02451,18.069511,16.39834,17.946024,18.290117,18.616373,16.420713,14.73591,16.901339,15.719978,16.759933,17.394686,18.001204,16.984993,18.193192,16.243929,17.915655,16.368744,17.746283,17.056911,12.492115,12.319283,-2.3814912,-2.3858562,-2.0477197,12.872763,12.687553,12.492629,12.71266,12.078201,18.464169,17.066439,15.572889,18.003922,18.101479,18.266653,18.149738,17.921059,-2.5253057,12.923634,11.222662,11.30016,-2.0045958,11.556767,11.174735,12.426466,12.554106,12.709404,18.52038,17.434156,17.986912,16.820757,17.39019,18.992666,18.61644,16.770285,16.345402,14.738594,16.960333,16.106075,16.81581,18.589493,18.407211,17.82753,18.761982,18.582157,18.059227,17.4176,17.700468,12.63568,12.420812,19.077124,19.287687,18.52029,17.809265,18.624958,18.924389,18.038412,17.869154,13.704408,13.584094,13.637357,13.307304,13.436625,12.643084,13.31388,1.6144933,12.48891,14.621338,12.87879,14.167649,13.849467,12.783706,13.447213,14.094251,14.246146,11.529274,-3.1984286,11.520114,12.570465,12.859032,12.627774,14.851264,13.417291,13.918816,3.123449,12.782082,12.840911,14.406057,12.959611,13.457271,13.131286,13.723774,14.312022,12.7007675,12.844835,12.439587,12.246114,11.907795,2.6243305,19.257624,18.131042,17.989126,17.229933,17.401814,19.320452,17.561485,18.002071,17.348173,18.326452,19.160843,19.239008,18.594616,19.211412,17.590298,17.346802,14.716341,15.68063,17.263672,18.65225,19.28009,17.059925,17.91128,18.201378,14.752065,13.9717865,3.4085002,14.569911,12.9197,12.392291,12.727285,12.925767,13.435221,3.054677,13.080018,13.408555,13.354184,13.221016],\"xaxis\":\"x\",\"y\":[1.239628,1.2139592,1.0194017,1.0772661,1.2055376,1.2918115,1.0667106,1.0275729,1.1757268,1.187888,1.1576198,1.251657,1.4576933,1.3276083,1.4658151,1.4913526,1.7402445,1.4950184,1.4242212,1.3511698,1.2541134,1.2432209,2.1319447,1.9940794,1.7687056,1.9276241,1.0681102,2.2513115,2.236839,2.099645,1.7300162,-6.381549,-6.614109,-6.8013034,-6.2855086,-5.0293193,-6.7313223,1.8841964,2.3375387,1.9767146,1.2493912,1.1470739,1.2620388,2.7663443,2.9645104,1.6294211,1.2254564,1.8417598,3.0221436,1.3314707,1.662742,2.793847,2.2079687,0.30413416,-0.5381998,-0.4940624,-0.66203237,-0.8568563,-0.5189612,-0.65501326,-0.8282548,1.436792,1.3391856,1.2875762,1.2799997,1.3594974,1.3328172,1.2960541,1.3315084,1.2756243,1.3135833,1.3002845,1.3144312,1.3038818,1.3096228,1.2921308,1.2792333,-0.588008,-1.8202188,1.183811,1.2606127,1.2443563,2.3196592,3.0085127,1.7989938,1.8305513,1.4034159,2.2077272,2.1837454,2.2991297,2.758005,1.5402956,2.6130824,1.5327706,1.3987756,1.5314085,2.2064447,1.8617805,7.1596794,1.25751,2.0856032,1.2898579,2.0759156,1.4348451,7.314239,2.0314066,1.4043875,1.7506351,2.1037254,0.90369415,2.7341926,1.6164821,2.0224104,2.2388473,1.5598546,1.4447885,3.6688094,2.3889244,1.7055457,1.3044455,2.7729385,1.7394135,1.581152,1.8074007,7.3531256,1.6937395,1.7160337,1.4129204,1.8539059,2.5394492,7.309321,1.4060397,1.0793676,2.545662,3.32197,2.5309248,1.2806475,1.5021317,2.4147496,2.813519,1.6272067,2.3848705,2.186072,2.184529,2.1674309,1.8803227,1.5401933,2.792203,3.4733384,2.3441756,3.6261551,3.411076,1.4727767,1.3106205,1.6890291,1.7634416,1.6220189,1.2903836,1.2625146,1.3095995,0.9686079,1.2143073,1.0974481,2.3626175,1.4212298,1.1825228,1.0632495,0.9798151,1.1028447,1.2801578,1.0601873,1.26958,1.1061832,2.201241,1.2424397,1.6214173,1.96921,1.3176839,1.3731683,0.956014,1.4977007,1.4972157,2.1143906,0.7292542,1.764427,2.6369538,1.5157053,2.4335167,2.7478194,1.4494535,0.94633186,1.118912,1.186677,0.9217516,0.69706386,2.7145543,1.4586235,1.4941036,7.3276896,1.1004504,1.2916267,1.6466153,2.2573664,1.7134128,1.7064186,2.730248,2.6172419,1.5681149,1.753951,1.5933152,1.5003482,3.336525,3.9930897,2.6306918,1.4861069,2.4091916,2.6405964,1.8215327,1.6131262,3.2150922,2.2091804,1.9249166,3.0508854,2.2227895,1.3983393,1.8993124,1.1535686,2.791417,2.1084738,2.6654565,1.1465436,2.1978047,1.8517498,2.3571162,2.298859,2.1400013,7.2807565,2.030943,2.537428,1.7788161,1.6748886,1.6115181,2.894378,2.6951175,2.8861735,2.3314693,1.6186262,-1.2819424,2.5940366,2.2417126,1.6039758,2.2304056,2.9941475,2.7373247,3.5157676,2.7833083,0.43751583,-1.1942496,3.146426,2.5881784,1.98671,1.460636,2.7842174,2.756473,1.2123433,2.4242294,1.04847,2.48071,1.0485586,1.8760325,2.1983318,0.16570076,-2.5134068,1.4538567,2.3135142,1.8565131,2.2920578,1.6336629,1.4870888,1.5683879,2.0186405,2.0696108,2.1567411,2.0454967,2.8243184,3.0804193,2.8521445,2.0777602,2.3348887,2.979691,2.3147776,1.8317692,3.3913772,2.6860442,2.6047862,2.7648792,1.5057037,3.0178833,3.598393,3.332091,5.4706,5.1306777,7.005705,3.1265626,2.996908,3.101834,2.334236,3.1947737,7.063619,1.8377221,2.629421,4.1400485,4.3508873,2.404582,2.2844095,-2.272491,6.9435906,3.1486714,3.004438,2.6840615,4.203608,4.228724,2.6136234,2.6898854,4.6655087,5.2654815,5.371303,2.870911,2.2659783,3.6843712,3.143229,4.560115,5.331286,1.891805,2.8924794,4.660836,5.2655463,3.3330293,1.8988438,2.9804194,5.229781,5.382132,3.4028757,4.204427,4.5076323,5.3526573,3.4272912,3.3387914,2.7029963,3.8184829,4.339944,1.3953048,2.3058836,2.4366014,1.4782428,7.005372,3.2689052,1.9217181,4.8966928,5.490712,2.1924486,3.4369402,3.6668603,1.9679182,2.5849416,4.1907597,3.9836898,3.0402913,5.4757066,5.7179704,1.9574059,5.4082117,-3.2320673,-2.4847636,2.5910366,1.6133988,1.471845,1.899225,3.5323532,3.2304318,4.558804,5.119592,4.079775,1.8750715,1.8376108,2.920013,1.8448393,3.3861783,3.2355359,3.2937672,1.879108,4.1935835,4.4537435,4.6739693,2.2459035,3.5280209,1.370046,3.3658328,3.0110614,4.612069,2.6465144,2.3866758,5.338863,1.5412803,2.8999703,2.9479814,3.939794,5.5302434,3.5283077,4.8855615,2.5233777,2.6529636,3.3078272,3.3652089,3.6052346,3.125408,3.0340042,4.2417674,5.953598,3.083811,2.7006738,5.4599476,1.2728009,6.901553,3.3563352,4.330823,4.3744483,2.483954,2.4799342,4.639353,4.9146805,3.234264,5.5165253,3.339066,4.2927446,2.3553793,2.1595237,1.7941519,2.264736,4.9560323,4.2380376,1.5062107,1.5336621,1.6305816,3.2132864,5.1817007,1.0097069,1.9728538,2.3945377,5.190192,5.202841,4.6365366,5.2952294,5.0794053,1.4994876,2.3518693,5.1876497,2.1013582,5.2313204,4.566934,2.1243215,1.5876125,2.8338695,4.7647357,3.0182352,1.9100482,4.372673,-2.6330724,1.4345804,2.9233024,4.3652735,5.3390975,4.12931,3.1976933,2.7545478,2.496779,5.66221,1.7222927,2.5019321,4.8022523,2.2990568,1.6899776,3.044529,5.046423,1.6266196,1.657919,6.913458,2.8384838,3.3982763,2.802023,2.764559,2.7033777,2.005527,2.017807,7.0316815,3.0140977,2.351869,1.3265042,2.2179263,1.7501369,1.5388203,2.4745564,2.3471906,3.001449,2.920261,2.692707,1.692751,1.4526993,1.6093919,0.44093382,7.2119284,1.2509179,1.7187856,3.2283792,3.455623,3.6399038,1.4143821,2.3199475,2.3496852,2.5787446,2.6921475,1.5031643,2.1977537,2.72721,2.7010782,2.374262,2.8355196,3.0594049,2.9435318,1.9075125,2.2009354,2.2344947,1.6633945,1.992021,2.9747574,3.1736588,2.8581958,2.607934,2.877399,2.1441257,2.7529948,2.8554723,2.9272168,2.599447,2.5806787,2.4972954,1.6255091,2.5321777,3.567648,2.9044888,2.8323686,2.8542025,3.5130806,3.2300785,-2.5943534,-7.099203,0.5700584,-2.1351671,-2.0903196,0.46421024,1.3344618,1.1017058,1.1442038,1.028144,0.40627065,0.9239224,0.34547412,1.0267528,-0.21845561,1.0526936,-1.2891382,0.7087044,1.0596294,2.1504269,-4.3100433,1.3467199,1.5292802,1.6997198,2.1455748,2.25113,1.6713867,2.2428937,2.6266081,1.8033642,1.5819889,1.7565528,2.07638,2.174701,1.8533509,2.5735002,2.708695,1.4218277,1.5732563,1.5922978,2.9883754,3.0049853,2.9909933,2.5080764,1.7387279,2.6976721,2.9744177,3.5529733,3.211092,0.42106694,0.57052726,0.7514971,0.9834521,0.68954295,0.41195548,0.6194662,0.7088445,0.5352465,0.4044891,0.44287804,0.49996027,0.6168791,0.5212129,-1.0636816,0.56349444,0.5753774,2.1056376,0.6594425,1.7623031,0.3764798,0.05063171,0.97594327,-2.1859708,1.2198908,1.2474772,2.762394,3.0359821,2.765611,3.4040642,2.5841641,2.8120685,2.806464,1.2178633,7.2998304,1.6595675,1.835565,2.6827412,3.193043,2.9147968,2.8264992,2.6607525,2.9095128,3.5601642,1.3095071,2.5551507,2.6341212,2.1914356,2.195283,0.4865528,-7.8148217,-7.711105,1.3529272,1.4023519,1.2489567,0.45129913,-0.07928347,3.0782955,3.736712,1.325906,2.855604,2.383095,2.5194144,3.6654558,3.5095248,1.7952216,0.78834546,1.2946573,1.1983373,-0.42114863,1.2350545,1.1572093,0.47616032,1.570093,1.3765315,3.0673435,3.05036,2.9296162,2.2815583,2.7616184,3.0348713,2.7530062,1.7696567,1.1017401,7.2796006,1.6057003,2.3260477,2.556019,2.8223102,2.715857,3.3209288,3.0305083,3.0549164,3.5379093,3.6453352,3.081186,1.6436112,0.8492791,2.9348176,3.0277822,2.7886894,1.8334733,2.7549977,2.887209,3.5956388,3.504776,2.7801387,2.9543598,2.9303985,2.995986,2.8550575,1.2281725,1.6550384,-3.0864727,0.17950419,0.49493796,1.4355013,1.0492492,0.8293754,1.2681606,1.0069522,0.70294255,0.6499671,-0.23229612,-2.5227482,-0.28253245,1.6072668,1.3895738,1.4202222,1.8780166,2.2497478,2.4555798,-2.335784,1.4906781,1.2938049,0.731064,0.8546682,0.7257033,0.7277947,0.67526126,0.6318436,1.3523874,1.4513689,2.3717587,2.303347,2.2177908,-2.5250778,3.0057297,2.6936855,2.7497916,1.4676714,2.6453636,3.0150769,2.7692666,3.4848955,3.4563835,2.9822497,2.9941134,3.0217662,2.9269254,3.0029619,3.2043517,1.4484911,7.3206444,1.8503922,1.6952394,2.6732278,3.0269353,3.5091305,3.5279653,2.8560252,0.79135704,1.9340665,-2.125391,0.52525723,1.2542996,0.22307046,1.3540311,1.9005047,1.9952135,-2.083462,1.5023085,1.6048292,1.6839741,1.5444669],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### What kinds of TPU are available?\\n\\nNew users are often very confused by the range of TPUs, and th...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe second way to access a TPU is via a **TPU VM.** When using a TPU VM, you connect directl...\"],[\"### I keep hearing about this XLA thing. What‚Äôs XLA, and how does it relate to TPUs?\\n\\nXLA is an opti...\"],[\"\\u003cTip\\u003e\\n\\n**ü§óSpecific HuggingFace Tipü§ó:** We‚Äôve put a lot of effort into rewriting our TensorFlow model...\"],[\"```\\n\\nThis might seem very restrictive at first, but most neural net code doesn‚Äôt need to do this. Yo...\"],[\"```\\n\\nThis code is totally fine in NumPy or PyTorch, but it breaks in XLA! Why? Because the shape of ...\"],[\"```\\n\\nHere, we avoid data-dependent shapes by computing the loss for every position, but zeroing out ...\"],[\"\\u003cTip\\u003e\\n\\n**ü§óSpecific HuggingFace Tipü§ó:** Our tokenizers and data collators have methods that can help ...\"],[\"### Summary\\n\\nThere was a lot in here, so let‚Äôs summarize with a quick checklist you can follow when ...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nüëÄ The resulting model can be found here: https:\\u002f\\u002fhuggingface.co\\u002fnielsr\\u002flayoutlmv3-finetuned-fun...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## VisionTextDualEncoderConfig\\n\\n[[autodoc]] VisionTextDualEncoderConfig\\n\\n## VisionTextDualEncoderPro...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"With this approach, the model can detect objects based on textual descriptions without prior trainin...\"],[\"```\\n\\n## Zero-shot object detection pipeline\\n\\nThe simplest way to try out inference with OWL-ViT is t...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```py\\n\\u003e\\u003e\\u003e predictions = detector(\\n...     image,\\n...     candidate_labels=[\\\"human face\\\", \\\"rocket\\\", \\\"...\"],[\"```\\n\\nLet's visualize the predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import ImageDraw\\n\\n\\u003e\\u003e\\u003e draw = ImageDraw.Dra...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e import requests\\n\\n\\u003e\\u003e\\u003e url = \\\"https:...\"],[\"```\\n\\nPass the inputs through the model, post-process, and visualize the results. Since the image pro...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nPreviously for post-processing you passed the single image's size as a tensor, but you can also...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fcircleci.com\\u002fgh\\u002fhuggingface\\u002ftransformers\\\"\\u003e\\n        \\u003cimg alt=...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fR...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fhf.co\\u002fcourse\\\"\\u003e\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhug...\"],[\"–ú–æ–¥–µ–ª–∏ transformers —Ç–∞–∫–∂–µ –º–æ–≥—É—Ç –≤—ã–ø–æ–ª–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–¥–∞—á, —Ç–∞–∫–∏–µ –∫–∞–∫ –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ç–∞–±–ª–∏—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, —Ä–∞...\"],[\"ü§ó Transformers –æ–ø–∏—Ä–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–∏ —Å–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è - [Jax](https:\\u002f\\u002fjax.r...\"],[\"–í –æ–±–ª–∞—Å—Ç–∏ NLP ( –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ ):\\n- [–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ª–æ–≤ —Å –ø–æ–º–æ—â—å...\"],[\"- [–û–±–æ–±—â–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é BART](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+me...\"],[\"- [–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –ø–æ–º–æ—â—å—é...\"],[\"DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+u...\"],[\"s+also+used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portugu...\"],[\"8Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon...\"],[\"C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud...\"],[\"egenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest...\"],[\"f+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000...\"],[\"00%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%28...\"],[\"tres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belongi...\"],[\"+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+t...\"],[\"%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amount...\"],[\"r+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+depa...\"],[\"+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+ha...\"],[\"+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+...\"],[\"diverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+...\"],[\"l+trees+divided+into+16%2C000+species)...\"],[\"- [–ü–µ—Ä–µ–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berli...\"],[\"–í –æ–±–ª–∞—Å—Ç–∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∑—Ä–µ–Ω–∏—è:\\n- [–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å –ø–æ–º–æ—â—å—é ViT](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"–í –æ–±–ª–∞—Å—Ç–∏ –∑–≤—É–∫–∞:\\n- [–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏ —Å –ø–æ–º–æ—â—å—é Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002ffac...\"],[\"–í –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö:\\n- [–û—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–∞–±–ª–∏—Ü–µ —Å –ø–æ–º–æ—â—å—é TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fg...\"],[\"## 100 –ø—Ä–æ–µ–∫—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏—Ö Transformers\\n\\nTransformers - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –Ω–∞–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏—Å–ø–æ...\"],[\"–ï—Å–ª–∏ –≤—ã —è–≤–ª—è–µ—Ç–µ—Å—å –≤–ª–∞–¥–µ–ª—å—Ü–µ–º –∏–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º –ø—Ä–æ–µ–∫—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π, –ø–æ –≤–∞—à–µ–º—É –º–Ω–µ–Ω–∏—é, –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤–∫–ª—é—á...\"],[\"## –ë—ã—Å—Ç—Ä—ã–π –≥–∞–π–¥\\n\\n–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω–∞ –∑–∞–¥–∞–Ω–Ω–æ–º –≤—Ö–æ–¥–µ (—Ç–µ–∫—Å—Ç, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –∑–≤—É–∫, ...) –º—ã –ø—Ä–µ–¥–æ...\"],[\"```\\n\\n–í—Ç–æ—Ä–∞—è —Å—Ç—Ä–æ–∫–∞ –∫–æ–¥–∞ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –∫–æ–Ω–≤–µ–π–µ—Ä–æ...\"],[\"# –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –æ–±—ä–µ–∫—Ç–æ–≤\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e...\"],[\"```\\n\\n–ó–¥–µ—Å—å –º—ã –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤, –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏, —Å —Ä–∞–º–∫–æ–π –≤–æ–∫—Ä—É–≥ –æ–±—ä–µ–∫—Ç–∞ –∏ –æ—Ü–µ–Ω–∫...\"],[\"\\u003e\\u003e\\u003e tokenizer = AutoTokenizer.from_pretrained(\\\"bert-base-uncased\\\")\\n\\u003e\\u003e\\u003e model = AutoModel.from_pretra...\"],[\"```\\n\\n–ê –≤–æ—Ç —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω—ã–π –∫–æ–¥ –¥–ª—è TensorFlow:\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, T...\"],[\"```\\n\\n–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤—Å—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –∫–æ—Ç–æ—Ä—É—é –æ–∂–∏–¥–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–∞—è...\"],[\"–°–∞–º–∞ –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –æ–±—ã—á–Ω—ã–π [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html...\"],[\"## –ü–æ—á–µ–º—É –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å transformers?\\n\\n1. –ü—Ä–æ—Å—Ç—ã–µ –≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:\\n    ...\"],[\"1. –ë–æ–ª–µ–µ –Ω–∏–∑–∫–∏–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã, –º–µ–Ω—å—à–∏–π \\\"—É–≥–ª–µ—Ä–æ–¥–Ω—ã–π —Å–ª–µ–¥\\\":\\n    - –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –º–æ–≥—É—Ç –æ–±–º–µ–Ω–∏–≤...\"],[\"1. –í—ã–±–æ—Ä –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –∂–∏–∑–Ω–∏ –º–æ–¥–µ–ª–∏:\\n    - –û–±—É—á–µ–Ω–∏–µ —Å–∞–º—ã—Ö —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–æ–¥...\"],[\"1. –õ–µ–≥–∫–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –º–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–∏–º–µ—Ä –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã:\\n    - –ú—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç...\"],[\"- –î–∞–Ω–Ω–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –º–æ–¥—É–ª—å–Ω—ã–º –Ω–∞–±–æ—Ä–æ–º —Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π. –ö–æ–¥ –≤ —Ñ–∞–π...\"],[\"- –ù–µ—Å–º–æ—Ç—Ä—è –Ω–∞ —Ç–æ, —á—Ç–æ –º—ã —Å—Ç—Ä–µ–º–∏–º—Å—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª—å—à–µ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, —Å–∫—Ä–∏–ø—Ç—ã –≤ –Ω–∞...\"],[\"## –£—Å—Ç–∞–Ω–æ–≤–∫–∞\\n\\n### –° –ø–æ–º–æ—â—å—é pip\\n\\n–î–∞–Ω–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ Python 3.8+, Flax 0.4.1+, PyTor...\"],[\"–ó–∞—Ç–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –±–µ–∫–µ–Ω–¥ –∏–∑ Flax, PyTorch –∏–ª–∏ TensorFlow.\\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–±—Ä–∞—Ç–∏—Ç...\"],[\"```\\n\\n–ï—Å–ª–∏ –≤—ã —Ö–æ—Ç–∏—Ç–µ –ø–æ–∏–≥—Ä–∞—Ç—å —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–ª–∏ –≤–∞–º –Ω—É–∂–µ–Ω —Å–∞–º—ã–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–¥ –∏ –≤—ã –Ω–µ –º–æ–∂–µ—Ç–µ –∂–¥–∞—Ç—å –Ω–æ...\"],[\"```\\n\\n–û —Ç–æ–º, –∫–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Flax, PyTorch –∏–ª–∏ TensorFlow —Å –ø–æ–º–æ—â—å—é conda, —á–∏—Ç–∞–π—Ç–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö, –ø–æ—Å–≤—è...\"],[\"## –ú–æ–¥–µ–ª—å–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã\\n\\n**[–í—Å–µ –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –º–æ–¥–µ–ª–µ–π](https:\\u002f\\u002fhuggingface.co\\u002fmodels)**, –ø—Ä–µ–¥–æ—Å...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (from VinAI Research) r...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (from G...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (from Google AI) released with ...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (from Salesforce) release...\"],[\"1. **[ByT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbyt5)** (from Google Research) releas...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (from LAION-AI) released with...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (from MetaAI) rele...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (from Facebook AI) re...\"],[\"1. **[CTRL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fctrl)** (from Salesforce) released wi...\"],[\"1. **[DeBERTa-v2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta-v2)** (from Microsoft) ...\"],[\"1. **[DePlot](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeplot)** (from Google AI) released...\"],[\"1. **[DiNAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinat)** (from SHI Labs) released wi...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (from HuggingFace...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (from Facebook) released with t...\"],[\"1. **[ELECTRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002felectra)** (from Google Research\\u002fS...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (from Baidu) released wi...\"],[\"1. **[Falcon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffalcon)** (from Technology Innovati...\"],[\"1. **[FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-ul2)** (from Google AI) rele...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GIT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgit)** (from Microsoft Research) relea...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (from EleutherAI) rel...\"],[\"1. **[GPT-Sw3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt-sw3)** (from AI-Sweden) releas...\"],[\"1. **[GPTSAN-japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptsan-japanese)** release...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (from Facebook) released ...\"],[\"1. **[Informer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finformer)** (from Beihang Univers...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (from Microsoft R...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (from Meta AI) released wit...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[Longformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flongformer)** (from AllenAI) re...\"],[\"1. **[M-CTC-T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmctct)** (from Facebook) released ...\"],[\"1. **[MarkupLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmarkuplm)** (from Microsoft Resea...\"],[\"1. **[mBART](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (from Facebook) released wi...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (from NVIDI...\"],[\"1. **[MobileBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilebert)** (from CMU\\u002fGoogle ...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (from Apple) re...\"],[\"1. **[MusicGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmusicgen)** (from Meta) released ...\"],[\"1. **[NLLB-MOE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnllb-moe)** (from Meta) released ...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fmodel_doc\\u002fpersimmon)** (from ADEPT) r...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (from Google) rel...\"],[\"1. **[ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fprophetnet)** (from Microsoft R...\"],[\"1. **[REALM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frealm.html)** (from Google Research)...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (from Facebook), releas...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (from Bo Peng), released on [...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (from ASAPP) released with ...\"],[\"1. **[Splinter](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsplinter)** (from Tel Aviv Univer...\"],[\"1. **[Swin Transformer V2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswinv2)** (from Micros...\"],[\"1. **[T5v1.1](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5v1.1)** (from Google AI) released...\"],[\"1. **[Time Series Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftime_series_transf...\"],[\"1. **[TVLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftvlt)** (from UNC Chapel Hill) releas...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (from Micros...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (from NAVER AI Lab\\u002fKakao Ente...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (from Meta AI) released ...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (from Microsoft Research) ...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (from Facebook) released togeth...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (from Meta AI) released wit...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"–ß—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, –µ—Å—Ç—å –ª–∏ —É –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞ Flax, PyTorch –∏–ª–∏ TensorFlow, –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å...\"],[\"| –°–µ–∫—Ü–∏—è | –û–ø–∏—Å–∞–Ω–∏–µ |\\n|-|-|\\n| [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002f) | –ü–æ–ª–Ω–∞—è –¥–æ–∫...\"],[\"| [–°–æ–≤–º–µ—Å—Ç–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_shari...\"],[\"## –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\\n\\n–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å [—Å—Ç–∞—Ç—å—è](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f), –∫–æ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nTo share a model with the community, you need an account on [huggingface.co](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\nFiles are also easily edited in a repository, and you can view the commit history as well as th...\"],[\"```\\n\\n## Convert a model for all frameworks\\n\\nTo ensure your model can be used by someone working with...\"],[\"```\\n\\u003c\\u002fjax\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Push a model during training\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\u003cYoutube id...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nShare a model to the Hub with [`PushToHubCallback`]. In the [`PushToHubCallback`] fun...\"],[\"```\\n\\nThe `push_to_hub` function can also be used to add other files to a model repository. For examp...\"],[\"```\\n\\nNow when you navigate to your Hugging Face profile, you should see your newly created model rep...\"],[\"* Manually creating and uploading a `README.md` file.\\n* Clicking on the **Edit model card** button i...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We present a convolution-free approach to video clas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large-scale NLP models have been shown to significan...\"],[\"A list of official Hugging Face and community (indicated by üåé) resources to help you get started wit...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We re-evaluate the standard practice of sharing weig...\"],[\"## RemBertConfig\\n\\n[[autodoc]] RemBertConfig\\n\\n## RemBertTokenizer\\n\\n[[autodoc]] RemBertTokenizer\\n    -...\"],[\"## TFRemBertForMaskedLM\\n\\n[[autodoc]] TFRemBertForMaskedLM\\n    - call\\n\\n## TFRemBertForCausalLM\\n\\n[[aut...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model augments the Transformer as a deep decomposition architecture, which can progressively de...\"],[\"- Check out the Autoformer blog-post in HuggingFace blog: [Yes, Transformers are Effective for Time ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-language reasoning requires an understand...\"],[\"## Usage tips\\n\\n- Bounding boxes are not necessary to be used in the visual feature embeddings, any k...\"],[\"[[autodoc]] models.lxmert.modeling_tf_lxmert.TFLxmertForPreTrainingOutput\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n...\"],[\"Author: [@vasudevgupta7](https:\\u002f\\u002fgithub.com\\u002fthevasudevgupta\\u002f)\\n\\n## Intro\\n\\nIn this project, we fine-tu...\"],[\"```\\n\\n## Evaluation\\n\\nOur evaluation script is different from the original script and we are evaluatin...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [Preprocessing data](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen\\u002fprepro...\"],[\"| [Summary of the tokenizers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002ftransformers_doc\\u002fen...\"],[\"### PyTorch Examples\\n\\n#### Natural Language Processing[[pytorch-nlp]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"| [How to generate text](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f02_how_to_generate....\"],[\"| [Reformer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fblog\\u002fblob\\u002fmain\\u002fnotebooks\\u002f03_reformer.ipynb)| How Reforme...\"],[\"#### Computer Vision[[pytorch-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"| [How to fine-tune a model on image classification (Torchvision)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fno...\"],[\"| [How to fine-tune a model on image classification (Kornia)](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [How to fine-tune an image captioning model](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fex...\"],[\"| [How to fine-tune a SegFormer model on semantic segmentation](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Audio[[pytorch-audio]]...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a speech recognition model in any language](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"#### Biological Sequences[[pytorch-bio]]...\"],[\"| Notebook     | Description                                                                        ...\"],[\"| [How to generate protein folds](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexamples\\u002fprotei...\"],[\"| [Fine-tune a Nucleotide Transformer model with LoRA](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob...\"],[\"#### Other modalities[[pytorch-other]]\\n\\n| Notebook     | Description                                ...\"],[\"#### Utility notebooks[[pytorch-utility]]\\n\\n| Notebook     |      Description      |   |   |\\n|:------...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to fine-tune a model on text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fma...\"],[\"| [How to fine-tune a model on token classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"| [How to fine-tune a model on multiple choice](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fe...\"],[\"| [How to fine-tune a model on summarization](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fexa...\"],[\"#### Computer Vision[[tensorflow-cv]]...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to fine-tune a model on image classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fm...\"],[\"#### Biological Sequences[[tensorflow-bio]]\\n\\n| Notebook     |      Description      |   |   |\\n|:----...\"],[\"#### Utility notebooks[[tensorflow-utility]]\\n\\n| Notebook     |      Description      |   |          ...\"],[\"| Notebook     |      Description      |   |   |\\n|:----------|:-------------|:-------------|------:|...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"| [How to fine-tune a model on text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model on summarization with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteb...\"],[\"## Community notebooks:\\n\\nMore notebooks developed by the community are available [here](https:\\u002f\\u002fhf.c...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nThe [`Trainer`] class is optimized for ü§ó Transformers models and can have surp...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformers-based models, such as BERT, have been o...\"],[\"## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token classifi...\"],[\"## BigBirdForMultipleChoice\\n\\n[[autodoc]] BigBirdForMultipleChoice\\n    - forward\\n\\n## BigBirdForTokenC...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"In terms of model details, the work outlines the architecture and training methodology of Persimmon-...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n\\nTips:\\n\\n- To convert the model, you need to clone the original repository using `git clone h...\"],[\"```\\n\\nFor the chat model:\\n```bash\\nwget https:\\u002f\\u002faxtkn4xl5cip.objectstorage.us-phoenix-1.oci.customer-o...\"],[\"*TEMPLATE**\\n=====================================\\n\\n*search & replace the following keywords, e.g.:*\\n...\"],[\"To start, let's try to get a general overview of the Transformers\\nlibrary.\\n\\nGeneral overview of ü§ó Tr...\"],[\"Let's take a look:\\n\\n![image](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolv...\"],[\"```\\n\\nSimilar to the model, the configuration inherits basic serialization and\\ndeserialization functi...\"],[\"From experience, we can tell you that the most important things to keep\\nin mind when adding a model ...\"],[\"7.  [ ] Successfully ran forward pass in Transformers that gives\\n    identical output to original ch...\"],[\"-   What type of model is *[camelcase name of model]*? BERT-like encoder-only\\n    model? GPT2-like d...\"],[\"If any of the mentioned aspects above are **not** clear to you, now is a great time to talk to [name...\"],[\"```\\n\\n3.  Set up a development environment, for instance by running the\\n    following command:\\n\\n    `...\"],[\"```\\n\\nNow you have set up a development environment to port *[camelcase name of model]*\\nto ü§ó Transfor...\"],[\"-   Where to find the pretrained weights?\\n-   How to load the pretrained weights into the correspond...\"],[\"Jupyter notebooks have the advantage that they allow for cell-by-cell\\nexecution which can be helpful...\"],[\"```\\n\\nNext, regarding the debugging strategy, there are generally a few from\\nwhich to choose from:\\n\\n-...\"],[\"However, if the original code-base is very complex or only allows\\nintermediate components to be run ...\"],[\"The outputs of the following layers often consist of multi-dimensional\\nfloat arrays and can look lik...\"],[\"```\\n\\nWe expect that every model added to ü§ó Transformers passes a couple of\\nintegration tests, meanin...\"],[\"-   Find the best way of debugging intermediate results. Is the original\\n    repository written in P...\"],[\"`autoregressive_sample`, `generate`.\\n-   Try to separate the tokenization from the model's\\n    forwa...\"],[\"#### More details on how to create a debugging environment for [camelcase name of model] \\n\\n[TODO FIL...\"],[\"```\\n    git checkout -b add_[lowercase name of model]\\n```\\n\\n2.  Commit the automatically generated co...\"],[\"```\\n\\n5.  Once you are satisfied, go to the webpage of your fork on GitHub.\\n    Click on \\\"Pull reques...\"],[\"**5. Adapt the generated models code for [camelcase name of model]**\\n\\nAt first, we will focus only o...\"],[\"```python\\nfrom transformers import [camelcase name of model]Model, [camelcase name of model]Config\\nm...\"],[\"```\\n\\nThe above command will create a model according to the default\\nparameters as defined in `[camel...\"],[\"In the following, we'll quickly explain how PyTorch models store layer\\nweights and define layer name...\"],[\"```\\n\\nNow we can create an instance of this model definition which will fill\\nall weights: `dense`, `i...\"],[\"```\\n\\nto see that the weights were randomly initialized...\"],[\"```bash\\ntensor([[-0.0818,  0.2207, -0.0749, -0.0030,  0.0045, -0.1569, -0.1598,  0.0212,\\n         -0...\"],[\"[-0.1173,  0.1561,  0.2945,  0.0595, -0.1996,  0.2988, -0.0802,  0.0407,\\n          0.1829, -0.1568],...\"],[\"```\\n\\nIn the conversion script, you should fill those randomly initialized\\nweights with the exact wei...\"],[\"```\\n\\nIf either the shape or the name doesn't match, you probably assigned\\nthe wrong checkpoint weigh...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what exactly has to be done...\"],[\"```\\n\\nIt is very likely that the ü§ó Transformers implementation and the\\noriginal model implementation ...\"],[\"The best way to fix the problem is usually to look at the forward pass\\nof the original implementatio...\"],[\"```\\n\\n[TODO FILL: Here the mentor should add very specific information on what tests are likely to fa...\"],[\"```\\n\\n**Note:** In case you are using Windows, you should replace `RUN_SLOW=1` with `SET RUN_SLOW=1`\\n...\"],[\"```\\n\\nYou might have to take a deeper look again into the original repository\\nto find the correct tok...\"],[\"```\\n\\nWhen both `input_ids` yield the same values, as a final step a tokenizer\\ntest file should also ...\"],[\"**11. Add Docstring**\\n\\nNow, all the necessary functionality for *[camelcase name of model]* is added...\"],[\"```\\n\\nand verify that your coding style passes the quality check:\\n\\n```bash\\nmake quality...\"],[\"```\\n\\nThere are a couple of other very strict design tests in ü§ó Transformers\\nthat might still be fail...\"],[\"### Share your work!!\\n\\nNow, it's time to get some credit from the community for your work!\\nHaving co...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor Chinese models, we need to generate a reference files (which requires the ltp library), bec...\"],[\"```\\n\\nThen you can run the script like this: \\n\\n\\n```bash\\nexport TRAIN_FILE=\\u002fpath\\u002fto\\u002ftrain\\u002ffile\\nexport ...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Implementation Notes\\n\\n- Each model is about 298 MB on disk, there are more than 1,000 models.\\n- T...\"],[\"## Examples\\n\\n- Since Marian models are smaller than many other translation models available in the l...\"],[\"\\u003e\\u003e\\u003e model_name = \\\"Helsinki-NLP\\u002fopus-mt-en-roa\\\"\\n\\u003e\\u003e\\u003e tokenizer = MarianTokenizer.from_pretrained(model...\"],[\"```\\n\\nHere is the code to see all available pretrained models on the hub:\\n\\n```python\\nfrom huggingface...\"],[\"```\\n\\n## Old Style Multi-Lingual Models\\n\\nThese are the old style multi-lingual models ported from the...\"],[\"```python no-style\\n['Helsinki-NLP\\u002fopus-mt-NORTH_EU-NORTH_EU',\\n 'Helsinki-NLP\\u002fopus-mt-ROMANCE-en',\\n '...\"],[\"'ROMANCE': ['fr', 'fr_BE', 'fr_CA', 'fr_FR', 'wa', 'frp', 'oc', 'ca', 'rm', 'lld', 'fur', 'lij', 'lm...\"],[\"```\\n\\nExample of translating english to many romance languages, using old-style 2 character language ...\"],[\"```\\n\\n## Resources\\n\\n- [Translation task guide](..\\u002ftasks\\u002ftranslation)\\n- [Summarization task guide](..\\u002f...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Usage tips\\n\\n- Wav2Vec2-Conformer follows the same architecture as Wav2Vec2, but replaces the *Att...\"],[\"## Wav2Vec2ConformerForSequenceClassification\\n\\n[[autodoc]] Wav2Vec2ConformerForSequenceClassificatio...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"TAPEX has been fine-tuned on several datasets: \\n- [SQA](https:\\u002f\\u002fwww.microsoft.com\\u002fen-us\\u002fdownload\\u002fdet...\"],[\"## Usage tips\\n\\n- TAPEX is a generative (seq2seq) model. One can directly plug in the weights of TAPE...\"],[\"\\u003e\\u003e\\u003e # prepare table + question\\n\\u003e\\u003e\\u003e data = {\\\"Actors\\\": [\\\"Brad Pitt\\\", \\\"Leonardo Di Caprio\\\", \\\"George Clo...\"],[\"```\\n\\nNote that [`TapexTokenizer`] also supports batched inference. Hence, one can provide a batch of...\"],[\"```\\n\\nIn case one wants to do table verification (i.e. the task of determining whether a given senten...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\nThe task illustrated in this tutorial is supported by the following model architectures:\\n\\n\\u003c!--...\"],[\"```\\n\\nYou will use [PyTorchVideo](https:\\u002f\\u002fpytorchvideo.org\\u002f) (dubbed `pytorchvideo`) to process and p...\"],[\"```\\n\\nAt a high level, the dataset is organized like so:\\n\\n```bash\\nUCF101_subset\\u002f\\n    train\\u002f\\n        B...\"],[\"```\\n\\nYou will notice that there are video clips belonging to the same group \\u002f scene where group is d...\"],[\"```\\n\\nThere are 10 unique classes. For each class, there are 30 videos in the training set.\\n\\n## Load ...\"],[\"```\\n\\nWhile the model is loading, you might notice the following warning:\\n\\n```bash\\nSome weights of th...\"],[\"```\\n\\nThe warning is telling us we are throwing away some weights (e.g. the weights and bias of the `...\"],[\"```\\n\\nFor the training dataset transformations, use a combination of uniform temporal subsampling, pi...\"],[\"```\\n\\nNow, define the dataset-specific transformations and the datasets respectively. Starting with t...\"],[\"```\\n\\nThe same sequence of workflow can be applied to the validation and evaluation sets: \\n\\n```py \\n\\u003e\\u003e...\"],[\"```\\n\\n**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https:...\"],[\"```\\n\\n## Visualize the preprocessed video for better debugging \\n\\n```py \\n\\u003e\\u003e\\u003e import imageio\\n\\u003e\\u003e\\u003e import...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"\\u003e\\u003e\\u003e args = TrainingArguments(\\n...     new_model_name,\\n...     remove_unused_columns=False,\\n...     e...\"],[\"```\\n\\nThe dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As...\"],[\"```\\n\\nThen you just pass all of this along with the datasets to `Trainer`:\\n\\n```py \\n\\u003e\\u003e\\u003e trainer = Trai...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\nYou can also manually replicate the results of the `pipeline` if you'd like.\\n\\n\\n```py\\n\\u003e\\u003e\\u003e def ru...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Natural language understanding comprises a wide rang...\"],[\"Note:\\n\\nIf you want to reproduce the original tokenization process of the *OpenAI GPT* paper, you wil...\"],[\"```\\n\\nIf you don't install `ftfy` and `SpaCy`, the [`OpenAIGPTTokenizer`] will default to tokenize\\nus...\"],[\"- A blog on how to [Finetune a non-English GPT-2 Model with Hugging Face](https:\\u002f\\u002fwww.philschmid.de\\u002f...\"],[\"- [`OpenAIGPTLMHeadModel`] is supported by this [causal language modeling example script](https:\\u002f\\u002fgi...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- A course material on [Byte-Pair Encoding tokenizat...\"],[\"## TFOpenAIGPTDoubleHeadsModel\\n\\n[[autodoc]] TFOpenAIGPTDoubleHeadsModel\\n    - call\\n\\n## TFOpenAIGPTFo...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002fblob\\u002fmain\\u002fCODE_OF_CONDUCT.md\\\"\\u003e\\n       ...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ü§ó Transformers Êèê‰æõ‰∫ÜÊï∞‰ª•ÂçÉËÆ°ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåÊîØÊåÅ 100 Â§öÁßçËØ≠Ë®ÄÁöÑÊñáÊú¨ÂàÜÁ±ª„ÄÅ‰ø°ÊÅØÊäΩÂèñ„ÄÅÈóÆÁ≠î„ÄÅÊëòË¶Å„ÄÅÁøªËØë„ÄÅÊñáÊú¨ÁîüÊàê„ÄÇÂÆÉÁöÑÂÆóÊó®ÊòØËÆ©ÊúÄÂÖàËøõÁöÑ NLP ÊäÄÊúØ‰∫∫‰∫∫ÊòìÁî®„ÄÇ\\n\\nü§ó Transform...\"],[\"ËøôÈáåÊòØ‰∏Ä‰∫õ‰æãÂ≠êÔºö\\n- [Áî® BERT ÂÅöÊé©Á†ÅÂ°´ËØç](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+F...\"],[\"- [Áî® RoBERTa ÂÅöËá™ÁÑ∂ËØ≠Ë®ÄÊé®ÁêÜ](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+was+lost.+Nobody+lost+a...\"],[\"- [Áî® DistilBERT...\"],[\"ÂÅöÈóÆÁ≠î](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+...\"],[\"used+to+describe+the+Amazon+rainforest+in+English%3F&context=The+Amazon+rainforest+%28Portuguese%3A+...\"],[\"uese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa...\"],[\"n%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+...\"],[\"d%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+c...\"],[\"t+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square...\"],[\"0+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100...\"],[\"82%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+n...\"],[\"ing+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rain...\"],[\"the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Ve...\"],[\"ts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments...\"],[\"artments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+t...\"],[\"alf+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+o...\"],[\"+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided...\"],[\"+divided+into+16%2C000+species)...\"],[\"- [Áî® T5 ÂÅöÁøªËØë](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[Write With Transformer](https:\\u002f\\u002ftransformer.huggingface.co)**ÔºåÁî±Êä±Êä±ËÑ∏Âõ¢ÈòüÊâìÈÄ†ÔºåÊòØ‰∏Ä‰∏™ÊñáÊú¨ÁîüÊàêÁöÑÂÆòÊñπ demo„ÄÇ\\n\\n## Â¶ÇÊûú‰Ω†Âú®ÂØª...\"],[\"```\\n\\nÁ¨¨‰∫åË°å‰ª£Á†Å‰∏ãËΩΩÂπ∂ÁºìÂ≠ò‰∫ÜÊµÅÊ∞¥Á∫ø‰ΩøÁî®ÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºåËÄåÁ¨¨‰∏âË°å‰ª£Á†ÅÂàôÂú®ÁªôÂÆöÁöÑÊñáÊú¨‰∏äËøõË°å‰∫ÜËØÑ‰º∞„ÄÇËøôÈáåÁöÑÁ≠îÊ°à‚ÄúÊ≠£Èù¢‚Äù (positive) ÂÖ∑Êúâ 99 ÁöÑÁΩÆ‰ø°Â∫¶„ÄÇ\\n\\nËÆ∏Â§öÁöÑ NLP ‰ªªÂä°ÈÉΩÊúâÂºÄÁÆ±Âç≥Áî®ÁöÑÈ¢Ñ...\"],[\"```\\nËøôÈáåÊòØÁ≠âÊïàÁöÑ TensorFlow ‰ª£Á†ÅÔºö\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer, TFAutoModel\\n\\n\\u003e\\u003e\\u003e tok...\"],[\"```\\n\\nËØçÁ¨¶ÂåñÂô® (tokenizer) ‰∏∫ÊâÄÊúâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊèê‰æõ‰∫ÜÈ¢ÑÂ§ÑÁêÜÔºåÂπ∂ÂèØ‰ª•Áõ¥Êé•ÂØπÂçï‰∏™Â≠óÁ¨¶‰∏≤ËøõË°åË∞ÉÁî®ÔºàÊØîÂ¶Ç‰∏äÈù¢ÁöÑ‰æãÂ≠êÔºâÊàñÂØπÂàóË°® (list) Ë∞ÉÁî®„ÄÇÂÆÉ‰ºöËæìÂá∫‰∏Ä‰∏™‰Ω†ÂèØ‰ª•Âú®‰∏ãÊ∏∏‰ª£Á†ÅÈáå‰ΩøÁî®ÊàñÁõ¥Êé•ÈÄöËøá ...\"],[\"1. ÂØπ‰∫éÊ®°ÂûãÁîüÂëΩÂë®ÊúüÁöÑÊØè‰∏Ä‰∏™ÈÉ®ÂàÜÈÉΩÈù¢Èù¢‰ø±Âà∞Ôºö\\n    - ËÆ≠ÁªÉÂÖàËøõÁöÑÊ®°ÂûãÔºåÂè™ÈúÄ 3 Ë°å‰ª£Á†Å\\n    - Ê®°ÂûãÂú®‰∏çÂêåÊ∑±Â∫¶Â≠¶‰π†Ê°ÜÊû∂Èó¥‰ªªÊÑèËΩ¨ÁßªÔºåÈöè‰Ω†ÂøÉÊÑè\\n    - ‰∏∫ËÆ≠ÁªÉ„ÄÅËØÑ‰º∞ÂíåÁîü‰∫ßÈÄâÊã©ÊúÄÈÄÇÂêàÁöÑÊ°ÜÊû∂ÔºåË°î...\"],[\"Ëøô‰∏™‰ªìÂ∫ìÂ∑≤Âú® Python 3.8+„ÄÅFlax 0.4.1+„ÄÅPyTorch 1.10+ Âíå TensorFlow 2.6+ ‰∏ãÁªèËøáÊµãËØï„ÄÇ\\n\\n‰Ω†ÂèØ‰ª•Âú®[ËôöÊãüÁéØÂ¢É](https:\\u002f\\u002fdocs.pytho...\"],[\"```\\n\\nÂ¶ÇÊûú‰Ω†ÊÉ≥Ë¶ÅËØïËØïÁî®‰æãÊàñËÄÖÊÉ≥Âú®Ê≠£ÂºèÂèëÂ∏ÉÂâç‰ΩøÁî®ÊúÄÊñ∞ÁöÑÂºÄÂèë‰∏≠‰ª£Á†ÅÔºå‰Ω†Âæó[‰ªéÊ∫ê‰ª£Á†ÅÂÆâË£Ö](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002finstallation#i...\"],[\"```\\n\\nË¶ÅÈÄöËøá conda ÂÆâË£Ö Flax„ÄÅPyTorch Êàñ TensorFlow ÂÖ∂‰∏≠‰πã‰∏ÄÔºåËØ∑ÂèÇÈòÖÂÆÉ‰ª¨ÂêÑËá™ÂÆâË£ÖÈ°µÁöÑËØ¥Êòé„ÄÇ\\n\\n## Ê®°ÂûãÊû∂ÊûÑ\\n\\nü§ó Transformers ÊîØÊåÅÁöÑ[**ÊâÄÊúâÁöÑÊ®°Âûã...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (Êù•Ëá™ Google Research and t...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (Êù•Ëá™ VinAI Research) ‰º¥ÈöèËÆ∫...\"],[\"1. **[BigBird-Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbigbird_pegasus)** (Êù•Ëá™ Goo...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [Big Transf...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (Êù•Ëá™ Salesforce) ‰º¥ÈöèËÆ∫Êñá [BLI...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (Êù•Ëá™ NAVER CLOVA) ‰º¥ÈöèËÆ∫Êñá [BROS: ...\"],[\"1. **[Chinese-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fchinese_clip)** (Êù•Ëá™ OFA-Sys) ...\"],[\"1. **[CLVP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclvp)** released with the paper [Bett...\"],[\"1. **[Conditional DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconditional_detr)** (Êù•Ëá™ M...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (Êù•Ëá™ Tsinghua University) ‰º¥ÈöèËÆ∫Êñá [...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (Êù•Ëá™ Facebook) ‰º¥ÈöèËÆ∫Êñá [D...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (Êù•Ëá™ Sen...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (Êù•Ëá™ Facebook) ‰º¥ÈöèËÆ∫Êñá [End-to-En...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (Êù•Ëá™ Meta AI) ‰º¥ÈöèËÆ∫Êñá [DINOv2...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (Êù•Ëá™ Microsoft Research) ‰º¥ÈöèËÆ∫Êñá [D...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (Êù•Ëá™ Sna...\"],[\"1. **[EncoderDecoder](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fencoder-decoder)** (Êù•Ëá™ Goog...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FLAN-T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflan-t5)** (from Google AI) releas...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (Êù•Ëá™ CNRS) ‰º¥ÈöèËÆ∫Êñá [FlauB...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (Êù•Ëá™ CMU\\u002fGoogl...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (Êù•Ëá™ OpenAI) ‰º¥ÈöèËÆ∫Êñá [Improv...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (Êù•Ëá™ OpenAI) ‰º¥ÈöèËÆ∫Êñá [Language M...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (Êù•Ëá™ BigCode) ‰º¥ÈöèËÆ∫...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (Êù•Ëá™ UCSD, NVIDIA) ‰º¥ÈöèËÆ∫...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[InstructBLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finstructblip)** (Êù•Ëá™ Salesforc...\"],[\"1. **[LayoutLMv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv2)** (Êù•Ëá™ Microsoft Res...\"],[\"1. **[LeViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flevit)** (Êù•Ëá™ Meta AI) ‰º¥ÈöèËÆ∫Êñá [LeViT: A...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (Êù•Ëá™ The FAIR team of Meta...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Êù•Ëá™ Microsoft Research & Un...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (Êù•Ëá™ UNC Chapel Hill) ‰º¥ÈöèËÆ∫Êñá...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (Êù•Ëá™ Facebook) ‰º¥ÈöèËÆ∫Êñá [Mega: Mov...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (Êù•Ëá™ Facebook) ‰º¥ÈöèËÆ∫Êñá [Scaling Spe...\"],[\"1. **[MobileNetV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilenet_v2)** (Êù•Ëá™ Google Inc...\"],[\"1. **[MRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmra)** (Êù•Ëá™ the University of Wisconsin...\"],[\"1. **[NAT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnat)** (Êù•Ëá™ SHI Labs) ‰º¥ÈöèËÆ∫Êñá [Neighborhoo...\"],[\"1. **[Nystr√∂mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (Êù•Ëá™ the Uni...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [Sim...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (Êù•Ëá™ Google) ‰º¥ÈöèËÆ∫Êñá [PEGAS...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (Êù•Ëá™ UCLA NLP) ‰º¥ÈöèËÆ∫Êñá [Unifi...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (Êù•Ëá™ Nanjing University, The Uni...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (Êù•Ëá™ Google Research) ...\"],[\"1. **[RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta)** (Êù•Ëá™ Facebook), ‰º¥ÈöèËÆ∫Êñá [Ro...\"],[\"1. **[RWKV](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frwkv)** (Êù•Ëá™ Bo Peng) ‰º¥ÈöèËÆ∫Êñá [this repo]...\"],[\"1. **[Segment Anything](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsam)** (Êù•Ëá™ Meta AI) ‰º¥ÈöèËÆ∫Êñá ...\"],[\"1. **[SpeechToTextTransformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text)** ...\"],[\"1. **[SwiftFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswiftformer)** (Êù•Ëá™ MBZUAI) ‰º¥ÈöèËÆ∫...\"],[\"1. **[SwitchTransformers](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswitch_transformers)** ...\"],[\"1. **[TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapas)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [TAPAS:...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (Êù•Ëá™ Google\\u002fCM...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (from Google Research) released...\"],[\"1. **[UnivNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funivnet)** (from Kakao Corporation...\"],[\"1. **[ViLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvilt)** (Êù•Ëá™ NAVER AI Lab\\u002fKakao Enterp...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (Êù•Ëá™ Google AI) ‰º¥Èöè...\"],[\"1. **[ViTMSN](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_msn)** (Êù•Ëá™ Meta AI) ‰º¥ÈöèËÆ∫Êñá [Maske...\"],[\"1. **[Wav2Vec2-Conformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2-conformer)** (...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (Êù•Ëá™ Microsoft Research) ‰º¥Èöè...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (Êù•Ëá™ Facebook) ‰º¥ÈöèËÆ∫Êñá [Cross-lingu...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Êù•Ëá™ Meta AI) ‰º¥ÈöèËÆ∫Êñá [XLM-V: O...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Êù•Ëá™ Faceboo...\"],[\"Ë¶ÅÊ£ÄÊü•Êüê‰∏™Ê®°ÂûãÊòØÂê¶Â∑≤Êúâ Flax„ÄÅPyTorch Êàñ TensorFlow ÁöÑÂÆûÁé∞ÔºåÊàñÂÖ∂ÊòØÂê¶Âú® ü§ó Tokenizers Â∫ì‰∏≠ÊúâÂØπÂ∫îËØçÁ¨¶ÂåñÂô®ÔºàtokenizerÔºâÔºåÊï¨ËØ∑ÂèÇÈòÖ[Ê≠§Ë°®](https:\\u002f\\u002fh...\"],[\"## ÂºïÁî®\\n\\nÊàë‰ª¨Â∑≤Â∞ÜÊ≠§Â∫ìÁöÑ[ËÆ∫Êñá](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)Ê≠£ÂºèÂèëË°®ÔºåÂ¶ÇÊûú‰Ω†‰ΩøÁî®‰∫Ü ü§ó Transformers Â∫ì...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n[Activation-aware Weight Quantization (AWQ)](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2306.00978) doesn't quanti...\"],[\"```\\n\\nAWQ-quantized models can be identified by checking the `quantization_config` attribute in the m...\"],[\"```\\n\\nAWQ quantization can also be combined with [FlashAttention-2](perf_infer_gpu_one#flashattention...\"],[\"```\\n\\n### Fused modules\\n\\nFused modules offers improved accuracy and performance and it is supported o...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"unsupported architectures\\\"\\u003e\\n\\nFor architectures that don't support fus...\"],[\"```\\n\\nThe parameter `modules_to_fuse` should include:\\n\\n- `\\\"attention\\\"`: The names of the attention la...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThe [AutoGPTQ](https:\\u002f\\u002fgithub.com\\u002fPanQiWei\\u002fAutoGPTQ) library implements the GPTQ algorithm, ...\"],[\"```\\n\\nTo quantize a model (currently only supported for text models), you need to create a [`GPTQConf...\"],[\"```\\n\\nIf you're running out of memory because a dataset is too large, disk offloading is not supporte...\"],[\"```\\n\\nYou could also save your quantized model locally with the [`~PreTrainedModel.save_pretrained`] ...\"],[\"```\\n\\n### ExLlama\\n\\n[ExLlama](https:\\u002f\\u002fgithub.com\\u002fturboderp\\u002fexllama) is a Python\\u002fC++\\u002fCUDA implementatio...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nOnly 4-bit models are supported, and we recommend deactivating the ExLlam...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"4-bit\\\"\\u003e\\n\\n```bash\\npip install bitsandbytes\\u003e=0.39.0\\npip install --upgra...\"],[\"```\\n\\nOnce a model is quantized to 8-bit, you can't push the quantized weights to the Hub unless you'...\"],[\"```\\n\\nOnce a model is quantized to 4-bit, you can't push the quantized weights to the Hub.\\n\\n\\u003c\\u002fhfoptio...\"],[\"```\\n\\nDesign a custom device map to fit everything on your GPU except for the `lm_head`, which you'll...\"],[\"```\\n\\n#### Skip module conversion\\n\\nFor some models, like [Jukebox](model_doc\\u002fjukebox), you don't need...\"],[\"```\\n\\n#### Finetuning\\n\\nWith the [PEFT](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fpeft) library, you can finetune...\"],[\"```\\n\\n#### Normal Float 4 (NF4)\\n\\nNF4 is a 4-bit data type from the [QLoRA](https:\\u002f\\u002fhf.co\\u002fpapers\\u002f2305....\"],[\"```\\n\\nFor inference, the `bnb_4bit_quant_type` does not have a huge impact on performance. However, t...\"],[\"```\\n\\n## Optimum\\n\\nThe [Optimum](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002findex) library supports quantizat...\"],[\"\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eUnfused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size | ...\"],[\"\\u003cfigcaption class=\\\"text-center text-gray-500 text-lg\\\"\\u003eFused module\\u003c\\u002ffigcaption\\u003e\\n\\n|   Batch Size |   ...\"],[\"The speed and throughput of fused and unfused modules were also tested with the [optimum-benchmark](...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The model is trained end-to-end with a combination of losses derived from variational lower bound an...\"],[\"## Usage examples\\n\\nBoth the VITS and MMS-TTS checkpoints can be used with the same API. Since the fl...\"],[\"```\\n\\nThe resulting waveform can be saved as a `.wav` file:\\n\\n```python\\nimport scipy\\n\\nscipy.io.wavfile...\"],[\"```\\n\\nYou can then pre-process the text input using the following code snippet. You can either rely o...\"],[\"```\\n\\n## VitsConfig\\n\\n[[autodoc]] VitsConfig\\n\\n## VitsTokenizer\\n\\n[[autodoc]] VitsTokenizer\\n    - __call...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe XLM-RoBERTa model was proposed in [Unsupervised Cross-lingual Representation Learni...\"],[\"This model was contributed by [stefan-it](https:\\u002f\\u002fhuggingface.co\\u002fstefan-it). The original code can b...\"],[\"\\u003cPipelineTag pipeline=\\\"text-classification\\\"\\u002f\\u003e\\n\\n- A blog post on how to [finetune XLM RoBERTa for mul...\"],[\"\\u003cPipelineTag pipeline=\\\"token-classification\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForTokenClassification`] is supported ...\"],[\"\\u003cPipelineTag pipeline=\\\"fill-mask\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForMaskedLM`] is supported by this [example scrip...\"],[\"\\u003cPipelineTag pipeline=\\\"question-answering\\\"\\u002f\\u003e\\n\\n- [`XLMRobertaForQuestionAnswering`] is supported by t...\"],[\"**Multiple choice**\\n\\n- [`XLMRobertaForMultipleChoice`] is supported by this [example script](https:\\u002f...\"],[\"[[autodoc]] XLMRobertaTokenizerFast\\n\\n\\u003cframeworkcontent\\u003e\\n\\u003cpt\\u003e\\n\\n## XLMRobertaModel\\n\\n[[autodoc]] XLMRob...\"],[\"[[autodoc]] TFXLMRobertaForSequenceClassification\\n    - call\\n\\n## TFXLMRobertaForMultipleChoice\\n\\n[[au...\"],[\"## FlaxXLMRobertaForQuestionAnswering\\n\\n[[autodoc]] FlaxXLMRobertaForQuestionAnswering\\n    - __call__...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fimagegpt_arc...\"],[\"- ImageGPT is almost exactly the same as [GPT-2](gpt2), with the exception that a different activati...\"],[\"easily obtained by first forwarding the image through the model, then specifying `output_hidden_stat...\"],[\"| **Model variant** | **Depths** | **Hidden sizes** | **Decoder hidden size** | **Params (M)** | **I...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Pop2Piano is an encoder-decoder Transformer model based on [T5](https:\\u002f\\u002farxiv.org\\u002fpdf\\u002f1910.10683.pdf...\"],[\"```\\npip install pretty-midi==0.2.9 essentia==2.1b6.dev1034 librosa scipy\\n```\\nPlease note that you ma...\"],[\"```\\n\\n- Example using your own audio file:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from transformers import...\"],[\"```\\n\\n- Example of processing multiple audio files in batch:\\n\\n```python\\n\\u003e\\u003e\\u003e import librosa\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\n\\n- Example of processing multiple audio files in batch (Using `Pop2PianoFeatureExtractor` and `...\"],[\"\\u003e\\u003e\\u003e # Since we now have 2 generated MIDI files\\n\\u003e\\u003e\\u003e tokenizer_output[0].write(\\\".\\u002fOutputs\\u002fmidi_output1...\"],[\"```\\n\\n\\n## Pop2PianoConfig\\n\\n[[autodoc]] Pop2PianoConfig\\n\\n## Pop2PianoFeatureExtractor\\n\\n[[autodoc]] Pop...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdeformable_d...\"],[\"[[autodoc]] DeformableDetrImageProcessor\\n    - preprocess\\n    - post_process_object_detection\\n\\n## De...\"],[\"### Fine-tuning BERT on SQuAD1.0 with relative position embeddings\\n\\nThe following examples show how ...\"],[\"##### Base models fine-tuning\\n\\n```bash\\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\\ntorchrun --nproc_...\"],[\"```\\nTraining with the above command leads to the following results. It boosts the BERT default from ...\"],[\"```\\nTraining with the above command leads to the f1 score of 93.52, which is slightly better than th...\"],[\"```\\n\\n##### Results for SQuAD2.0 with the previously defined hyper-parameters:\\n\\n```python\\n{\\n\\\"exact\\\": ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Detection Transformer (DETR) directly transforms que...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Large Transformer models routinely achieve state-of-...\"],[\"### Axial Positional Encodings\\n\\nAxial Positional Encodings were first implemented in Google's [trax ...\"],[\"with:\\n\\n$$d = d^1 + d^2 \\\\text{ and } n_s = n_s^1 \\\\times n_s^2 .$$\\n\\nTherefore the following holds:\\n\\n$$...\"],[\"In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\\\\\((d^1, d^2)\\\\\\\\) which sum...\"],[\"For more information, see the [original Paper](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2001.04451) or this great [blog...\"],[\"### Local Self Attention\\n\\nLocal self attention is essentially a \\\"normal\\\" self attention layer with k...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Question ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## {{cookiecutter.camelcase_modelname}}TokenizerFast\\n\\n[[autodoc]] {{cookiecutter.camelcase_modelname...\"],[\"[[autodoc]] {{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - forward\\n\\n{%- else %}\\n## {...\"],[\"[[autodoc]] TF{{cookiecutter.camelcase_modelname}}ForCausalLM\\n    - call\\n\\n\\n## TF{{cookiecutter.camel...\"],[\"{% if cookiecutter.is_encoder_decoder_model == \\\"False\\\" %}\\n## Flax{{cookiecutter.camelcase_modelname}...\"],[\"[[autodoc]] Flax{{cookiecutter.camelcase_modelname}}ForQuestionAnswering\\n    - call\\n\\n\\n## Flax{{cooki...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Pre-trained language models have attracted increasin...\"],[\"## Usage tips\\n\\n- BioGPT is a model with absolute position embeddings so it's usually advised to pad ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Recently, plain vision Transformers (ViTs) have show...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `SpeechEncoderDecoderModel` from model configurations.\\n\\n[`SpeechEncoderDeco...\"],[\"```\\n\\n## Initialising `SpeechEncoderDecoderModel` from a pretrained encoder and a pretrained decoder....\"],[\"```\\n\\n## Loading an existing `SpeechEncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load f...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e # load its corresponding transcription and tokenize to generate labels\\n\\u003e\\u003e\\u003e labels = tokenizer(ds...\"],[\"```\\n\\n## SpeechEncoderDecoderConfig\\n\\n[[autodoc]] SpeechEncoderDecoderConfig\\n\\n## SpeechEncoderDecoderM...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Backward\\n\\nThe last addition is to replace the typical `loss.backward()` in your training loo...\"],[\"```\\n\\nAs you can see in the following code, you only need to add four additional lines of code to you...\"],[\"```\\n\\nThen launch your training with:\\n\\n```bash\\naccelerate launch train.py\\n```\\n\\n### Train with a noteb...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose focal modulation networks (FocalNets in s...\"],[\"This model was contributed by [nielsr](https:\\u002f\\u002fhuggingface.co\\u002fnielsr).\\nThe original code can be foun...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Pipeline\\n\\n\\u003cYoutube id=\\\"tiZFewofSLM\\\"\\u002f\\u003e\\n\\nThe [`pipeline`] is the eas...\"],[\"| **Task**                     | **Description**                                                    ...\"],[\"| Audio classification         | assign a label to some audio data                                  ...\"],[\"Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this ...\"],[\"```\\n\\nThe [`pipeline`] downloads and caches a default [pretrained model](https:\\u002f\\u002fhuggingface.co\\u002fdisti...\"],[\"```\\n\\nLoad an audio dataset (see the ü§ó Datasets [Quick Start](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fqu...\"],[\"```\\n\\nFor larger datasets where the inputs are big (like in speech or vision), you'll want to pass a ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nUse [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretra...\"],[\"```\\n\\nIf you can't find a model for your use-case, you'll need to finetune a pretrained model on your...\"],[\"```\\n\\nPass your text to the tokenizer:\\n\\n```py\\n\\u003e\\u003e\\u003e encoding = tokenizer(\\\"We are very happy to show you...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n\\u003cTip\\u003e\\n\\nCheck out the [preprocess](.\\u002fpreprocessing) tutorial for more ...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\nThe model outputs the final activations in the `logits` attribute. Apply the softmax function t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nOnce your model is fine-tuned, you can save it with its tokenizer using [`TFPreTraine...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n## Custom model builds\\n\\nYou can modify the model's configuration clas...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\nTake a look at the [Create a custom architecture](.\\u002fcreate_a_model) g...\"],[\"```\\n\\n3. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor tasks - like translation or summarization - that use a sequence-to-sequence model, u...\"],[\"```py\\n   \\u003e\\u003e\\u003e from transformers import TFAutoModelForSequenceClassification\\n\\n   \\u003e\\u003e\\u003e model = TFAutoMod...\"],[\"```\\n\\n2. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processo...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Randomly initializing `EncoderDecoderModel` from model configurations.\\n\\n[`EncoderDecoderModel`] c...\"],[\"```\\n\\n## Initialising `EncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\\n\\n[`En...\"],[\"```\\n\\n## Loading an existing `EncoderDecoderModel` checkpoint and perform inference.\\n\\nTo load fine-tu...\"],[\"```\\n\\n## Loading a PyTorch checkpoint into `TFEncoderDecoderModel`.\\n\\n[`TFEncoderDecoderModel.from_pre...\"],[\"```\\n\\n## Training\\n\\nOnce the model is created, it can be fine-tuned similar to BART, T5 or any other e...\"],[\"\\u003e\\u003e\\u003e labels = tokenizer(\\n...     \\\"the eiffel tower surpassed the washington monument to become the ta...\"],[\"```\\n\\nDetailed [colab](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-attention has become a defacto choice for captu...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"--\\u003e\\n\\n# Llama2\\n\\n## Overview\\n\\nThe Llama2 model was proposed in [LLaMA: Open Foundation and Fine-Tuned ...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we develop and release Llama 2, a coll...\"],[\"The `dtype` of the online weights is mostly irrelevant unless you are using `torch_dtype=\\\"auto\\\"` whe...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nTips:\\n\\n- Weights for the Llama2 models can be obtained by filling out [this form](https:\\u002f\\u002fai...\"],[\"```\\n\\n- After conversion, the model and tokenizer can be loaded via:\\n\\n```python\\nfrom transformers imp...\"],[\"```\\n\\nNote that executing the script requires enough CPU RAM to host the whole model in float16 preci...\"],[\"\\u003cPipelineTag pipeline=\\\"text-generation\\\"\\u002f\\u003e\\n\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1P...\"],[\"‚ö°Ô∏è Inference\\n- A [notebook](https:\\u002f\\u002fcolab.research.google.com\\u002fdrive\\u002f1TC56ArKerXUpbgRy5vM3woRsbTEVNq7...\"],[\"[[autodoc]] LlamaModel\\n    - forward\\n\\n\\n## LlamaForCausalLM\\n\\n[[autodoc]] LlamaForCausalLM\\n    - forwa...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*In this work, we present a new network design paradi...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Self-supervised learning (SSL) achieves great succes...\"],[\"- [Audio classification task guide](..\\u002ftasks\\u002faudio_classification)\\n- [Automatic speech recognition t...\"],[\"Performer fine-tuning\\n\\nExample authors: @TevenLeScao, @Patrickvonplaten\\n\\nPaper authors: Krzysztof Ch...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Deeper neural networks are more difficult to train. ...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"End-to-End finetuning of RAG (including DPR retriever) for Question Answering.\\n\\nThis finetuning scri...\"],[\"To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script als...\"],[\"We conducted a simple experiment to investigate the effectiveness of this end2end training extension...\"],[\"!--Copyright 2021 NVIDIA Corporation and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"This model was contributed by [shangz](https:\\u002f\\u002fhuggingface.co\\u002fshangz).\\n\\n## Usage tips\\n\\n- QDQBERT mod...\"],[\"Before creating QDQBERT model, one has to set the default `QuantDescriptor` defining default tensor ...\"],[\"```\\n\\n### Calibration\\n\\nCalibration is the terminology of passing data samples to the quantizer and de...\"],[\"```\\n\\n### Export to ONNX\\n\\nThe goal of exporting to ONNX is to deploy inference by [TensorRT](https:\\u002f\\u002f...\"],[\"```\\n\\n## Resources\\n\\n- [Text classification task guide](..\\u002ftasks\\u002fsequence_classification)\\n- [Token cla...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [patrickvonplaten](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten). The Autho...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Depth estimation pipeline\\n\\nThe simplest way to try out inference with a model supporting dep...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nVisualize the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import numpy as np\\n\\n\\u003e\\u003e\\u003e # interpolate to original size\\n\\u003e\\u003e\\u003e pr...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Most neural vocoders employ band-limited mel-spectro...\"],[\"Tips:\\n\\n- The `noise_sequence` argument for [`UnivNetModel.forward`] should be standard Gaussian nois...\"],[\"ds = load_dataset(\\\"hf-internal-testing\\u002flibrispeech_asr_dummy\\\", \\\"clean\\\", split=\\\"validation\\\")\\n# Resamp...\"],[\"```\\n\\nThis model was contributed by [dg845](https:\\u002f\\u002fhuggingface.co\\u002fdg845).\\nTo the best of my knowledg...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Understanding document images (e.g., invoices) is a ...\"],[\"## Usage tips\\n\\n- The quickest way to get started with Donut is by checking the [tutorial\\n  notebooks...\"],[\"\\u003e\\u003e\\u003e device = \\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\"\\n\\u003e\\u003e\\u003e model.to(device)  # doctest: +IGNORE...\"],[\"```\\n\\n- Step-by-step Document Parsing\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from transformers import DonutProcess...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"```\\n\\n- Step-by-step Document Visual Question Answering (DocVQA)\\n\\n```py\\n\\u003e\\u003e\\u003e import re\\n\\n\\u003e\\u003e\\u003e from trans...\"],[\"\\u003e\\u003e\\u003e pixel_values = processor(image, return_tensors=\\\"pt\\\").pixel_values\\n\\n\\u003e\\u003e\\u003e outputs = model.generate(...\"],[\"```\\n\\nSee the [model hub](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=donut) to look for Donut checkpoints.\\n...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Transformer based models, like BERT and RoBERTa, hav...\"],[\"## IBertConfig\\n\\n[[autodoc]] IBertConfig\\n\\n## IBertModel\\n\\n[[autodoc]] IBertModel\\n    - forward\\n\\n## IBe...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*While the Transformer architecture has become the de...\"],[\"- [BEiT](beit) (BERT pre-training of Image Transformers) by Microsoft Research. BEiT models outperfo...\"],[\"- To feed images to the Transformer encoder, each image is split into a sequence of fixed-size non-o...\"],[\"use a higher resolution than pre-training [(Touvron et al., 2019)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1906.06423)...\"],[\"## Resources\\n\\nDemo notebooks regarding inference as well as fine-tuning ViT on custom data can be fo...\"],[\"‚öóÔ∏è Optimization\\n\\n- A blog post on how to [Accelerate Vision Transformer (ViT) with Quantization usin...\"],[\"[[autodoc]] ViTForImageClassification\\n    - forward\\n\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\n\\n## TFViTModel\\n\\n[[autodoc]] TFViTMod...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"This model was contributed by [Patrick von Platen](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten).\\n\\nThe or...\"],[\"- Step-by-step Speech Translation\\n\\n```python\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e from transformers import Speech2Te...\"],[\"```\\n\\n- Speech Translation via Pipelines\\n\\n  The automatic speech recognition pipeline can also be use...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The `padding` argument controls padding. It can be a boolean or a string:\\n\\n  - `True` or `'longest'`...\"],[\"The `max_length` argument controls the length of the padding and truncation. It can be an integer or...\"],[\"| Truncation                           | Padding                           | Instruction            ...\"],[\"|                                      |                                   | `tokenizer(batch_senten...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nwhere task name can be one of cola, sst2, mrpc, stsb, qqp, mnli, qnli, rte, wnli.\\n\\nWe get the f...\"],[\"The following example fine-tunes BERT on the `imdb` dataset hosted on our [hub](https:\\u002f\\u002fhuggingface....\"],[\"```\\n\\n\\u003e If your model classification head dimensions do not fit the number of labels in the dataset, ...\"],[\"```\\nTraining for 1 epoch results in acc of around 0.5958 for review_body only and 0.659 for title+bo...\"],[\"```\\n It results in a Micro F1 score of around 0.82 without any text and label filtering. Note that y...\"],[\"Using mixed precision training usually results in 2x-speedup for training with the same final result...\"],[\"Like `run_glue.py`, this script allows you to fine-tune any of the models on the [hub](https:\\u002f\\u002fhuggi...\"],[\"```\\n\\nthen\\n\\n```bash\\nexport TASK_NAME=mrpc\\n\\npython run_glue_no_trainer.py \\\\\\n  --model_name_or_path ber...\"],[\"```\\n\\nThis command is the same and will work for:\\n\\n- a CPU-only setup\\n- a setup with one GPU\\n- a dist...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision Transformers (ViT) have shown rapid progress ...\"],[\"## Documentation resources\\n\\n- [Image classification task guide](..\\u002ftasks\\u002fimage_classification)\\n\\n## E...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Fine-tuning ViLT\\n\\nViLT model incorporates text embeddings into a Vision Transformer (ViT), allowi...\"],[\"```\\n\\nWe encourage you to share your model with the community. Log in to your Hugging Face account to...\"],[\"```\\n\\nLet's take a look at an example to understand the dataset's features:\\n\\n```py\\n\\u003e\\u003e\\u003e dataset[0]\\n{'q...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nNow that we have the mappings, we can replace the string answers with their ids, and flatten th...\"],[\"```\\n\\nTo preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. ...\"],[\"```\\n\\nTo apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.map`] f...\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\nThe model in this guide has only been trained on 200 examples, so don't expect a lot from it. L...\"],[\"```\\n\\n## Zero-shot VQA\\n\\nThe previous model treated VQA as a classification task. Some recent models, ...\"],[\"```\\n\\nNow we need to preprocess the image\\u002fprompt with the model's processor, pass the processed input...\"],[\"!---\\nCopyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\\n\\nLicense...\"],[\"```\\n\\nwhere task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, ...\"],[\"```\\n\\nor directly on the hub under *Training metrics*.\\n\\n### Accuracy Evaluation\\n\\nWe train five replic...\"],[\"| Task  | Metric                       | Acc (best run) | Acc (avg\\u002f5runs) | Stdev     | Metrics     ...\"],[\"| QQP   | Accuracy\\u002fF1                  | 90.81\\u002f87.58    | 90.76\\u002f87.51     | 0.05\\u002f0.06 | [tfhub.dev](...\"],[\"Some of these results are significantly different from the ones reported on the test set of GLUE ben...\"],[\"| Task  | TPU v3-8  | 8 GPU      | [1 GPU](https:\\u002f\\u002ftensorboard.dev\\u002fexperiment\\u002fmkPS4Zh8TnGe1HB6Yzwj4Q...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*We propose an efficient design of Transformer-based ...\"],[\"## PatchTSTConfig\\n\\n[[autodoc]] PatchTSTConfig\\n\\n## PatchTSTModel\\n\\n[[autodoc]] PatchTSTModel\\n    - for...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers\\u002f\\\"\\u003eEnglish\\u003c\\u002f...\"],[\"ü§ó TransformersÎäî Î∂ÑÎ•ò, Ï†ïÎ≥¥ Ï∂îÏ∂ú, ÏßàÎ¨∏ ÎãµÎ≥Ä, ÏöîÏïΩ, Î≤àÏó≠, Î¨∏Ïû• ÏÉùÏÑ± Îì±ÏùÑ 100Í∞ú Ïù¥ÏÉÅÏùò Ïñ∏Ïñ¥Î°ú ÏàòÌñâÌï† Ïàò ÏûàÎäî ÏàòÏ≤úÍ∞úÏùò ÏÇ¨Ï†ÑÌïôÏäµÎêú Î™®Îç∏ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. Ïö∞Î¶¨Ïùò Î™©...\"],[\"ü§ó TransformersÎäî Í∞ÄÏû• Ïú†Î™ÖÌïú 3Í∞úÏùò Îî•Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏßÄÏõêÌï©ÎãàÎã§. Ïù¥Îì§ÏùÄ ÏÑúÎ°ú ÏôÑÎ≤ΩÌûà Ïó∞ÎèôÎê©ÎãàÎã§ ‚Äî [Jax](https:\\u002f\\u002fjax.readthedocs.io\\u002fen\\u002f...\"],[\"ÏòàÏãú:\\n- [BERTÎ°ú ÎßàÏä§ÌÇπÎêú Îã®Ïñ¥ ÏôÑÏÑ±ÌïòÍ∏∞](https:\\u002f\\u002fhuggingface.co\\u002fbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+...\"],[\"- [BARTÎ•º Ïù¥Ïö©Ìïú ÏöîÏïΩ](https:\\u002f\\u002fhuggingface.co\\u002ffacebook\\u002fbart-large-cnn?text=The+tower+is+324+metres+%281%2C...\"],[\"- [DistilBERTÎ•º Ïù¥Ïö©Ìïú ÏßàÎ¨∏...\"],[\"ÎãµÎ≥Ä](https:\\u002f\\u002fhuggingface.co\\u002fdistilbert-base-uncased-distilled-squad?text=Which+name+is+also+used+to+d...\"],[\"- [T5Î°ú Î≤àÏó≠ÌïòÍ∏∞](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin)...\"],[\"**[TransformerÏôÄ Í∏ÄÏì∞Í∏∞](https:\\u002f\\u002ftransformer.huggingface.co)** Îäî Ïù¥ Ï†ÄÏû•ÏÜåÏùò ÌÖçÏä§Ìä∏ ÏÉùÏÑ± Îä•Î†•Ïóê Í¥ÄÌïú Hugging Face ÌåÄÏùò Í≥µÏãù...\"],[\"## ÌÄµ Ìà¨Ïñ¥\\n\\nÏõêÌïòÎäî ÌÖçÏä§Ìä∏Ïóê Î∞îÎ°ú Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù, Ïö∞Î¶¨Îäî `pipeline` APIÎ•º Ï†úÍ≥µÌï©ÎãàÎã§. PipelineÏùÄ ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏Í≥º Í∑∏ Î™®Îç∏ÏùÑ ÌïôÏäµÌï† Îïå Ï†ÅÏö©Ìïú Ï†ÑÏ≤ò...\"],[\"```\\n\\nÏΩîÎìúÏùò ÎëêÎ≤àÏß∏ Ï§ÑÏùÄ pipelineÏù¥ ÏÇ¨Ïö©ÌïòÎäî ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ÏùÑ Îã§Ïö¥Î°úÎìúÌïòÍ≥† Ï∫êÏãúÎ°ú Ï†ÄÏû•Ìï©ÎãàÎã§. ÏÑ∏Î≤àÏß∏ Ï§ÑÏóêÏÑ† Í∑∏ Î™®Îç∏Ïù¥ Ï£ºÏñ¥ÏßÑ ÌÖçÏä§Ìä∏Î•º ÌèâÍ∞ÄÌï©ÎãàÎã§. Ïó¨Í∏∞ÏÑú Î™®Îç∏ÏùÄ 99.9...\"],[\"```\\n\\nÎãµÎ≥ÄÎøêÎßå ÏïÑÎãàÎùº, Ïó¨Í∏∞Ïóê ÏÇ¨Ïö©Îêú ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏ÏùÄ ÌôïÏã†ÎèÑÏôÄ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï¶àÎêú Î¨∏Ïû• ÏÜç ÎãµÎ≥ÄÏùò ÏãúÏûëÏ†ê, ÎÅùÏ†êÍπåÏßÄ Î∞òÌôòÌï©ÎãàÎã§. [Ïù¥ ÌäúÌÜ†Î¶¨Ïñº](https:\\u002f\\u002fhuggingface.c...\"],[\"```\\n\\nÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎäî ÏÇ¨Ï†ÑÌïôÏäµ Î™®Îç∏Ïùò Î™®Îì† Ï†ÑÏ≤òÎ¶¨Î•º Ï±ÖÏûÑÏßëÎãàÎã§. Í∑∏Î¶¨Í≥† (ÏúÑÏùò ÏòàÏãúÏ≤òÎüº) 1Í∞úÏùò Ïä§Ìä∏ÎßÅÏù¥ÎÇò Î¶¨Ïä§Ìä∏ÎèÑ Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎäî ÎîïÏÖîÎÑàÎ¶¨Î•º Î∞òÌôòÌïòÎäîÎç∞, Ïù¥Îäî ...\"],[\"Î™®Îç∏ ÏûêÏ≤¥Îäî ÏùºÎ∞òÏ†ÅÏúºÎ°ú ÏÇ¨Ïö©ÎêòÎäî [Pytorch `nn.Module`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fnn.html#torch.nn.Module)ÎÇò [T...\"],[\"1. Îçî Ï†ÅÏùÄ Í≥ÑÏÇ∞ ÎπÑÏö©, Îçî Ï†ÅÏùÄ ÌÉÑÏÜå Î∞úÏûêÍµ≠:\\n    - Ïó∞Íµ¨ÏûêÎì§ÏùÄ Î™®Îç∏ÏùÑ Í≥ÑÏÜç Îã§Ïãú ÌïôÏäµÏãúÌÇ§Îäî ÎåÄÏã† ÌïôÏäµÎêú Î™®Îç∏ÏùÑ Í≥µÏú†Ìï† Ïàò ÏûàÏäµÎãàÎã§.\\n    - Ïã§Î¨¥ÏûêÎì§ÏùÄ ÌïôÏäµÏóê ÌïÑÏöîÌïú Ïãú...\"],[\"1. ÌïÑÏöîÌïú ÎåÄÎ°ú Î™®Îç∏Ïù¥ÎÇò ÏòàÏãúÎ•º Ïª§Ïä§ÌÑ∞ÎßàÏù¥Ï¶àÌïòÏÑ∏Ïöî:\\n    - Ïö∞Î¶¨Îäî Ï†ÄÏûêÍ∞Ä Í≥µÍ∞úÌïú Í≤∞Í≥ºÎ•º Ïû¨ÌòÑÌïòÍ∏∞ ÏúÑÌï¥ Í∞Å Î™®Îç∏ Íµ¨Ï°∞Ïùò ÏòàÏãúÎ•º Ï†úÍ≥µÌï©ÎãàÎã§.\\n    - Î™®Îç∏ ÎÇ¥Î∂Ä Íµ¨Ï°∞Îäî Í∞ÄÎä•Ìïú ...\"],[\"## Ïôú transformersÎ•º ÏÇ¨Ïö©ÌïòÏßÄ ÎßêÏïÑÏïº Ìï†ÍπåÏöî?\\n\\n- Ïù¥ ÎùºÏù¥Î∏åÎü¨Î¶¨Îäî Ïã†Í≤ΩÎßù Î∏îÎ°ùÏùÑ ÎßåÎì§Í∏∞ ÏúÑÌïú Î™®ÎìàÏù¥ ÏïÑÎãôÎãàÎã§. Ïó∞Íµ¨ÏûêÎì§Ïù¥ Ïó¨Îü¨ ÌååÏùºÏùÑ ÏÇ¥Ìé¥Î≥¥ÏßÄ ÏïäÍ≥† Î∞îÎ°ú Í∞Å Î™®Îç∏ÏùÑ ...\"],[\"## ÏÑ§Ïπò\\n\\n### pipÎ°ú ÏÑ§ÏπòÌïòÍ∏∞\\n\\nÏù¥ Ï†ÄÏû•ÏÜåÎäî Python 3.8+, Flax 0.4.1+, PyTorch 1.10+, TensorFlow 2.6+ÏóêÏÑú ÌÖåÏä§Ìä∏ ÎêòÏóàÏäµÎãàÎã§.\\n\\n...\"],[\"Ïù¥Îì§ Ï§ë Ï†ÅÏñ¥ÎèÑ ÌïòÎÇòÍ∞Ä ÏÑ§ÏπòÎêòÏóàÎã§Î©¥, ü§ó TransformersÎäî Îã§ÏùåÍ≥º Í∞ôÏù¥ pipÏùÑ Ïù¥Ïö©Ìï¥ ÏÑ§ÏπòÌï† Ïàò ÏûàÏäµÎãàÎã§:\\n\\n```bash\\npip install transformers...\"],[\"```\\n\\nÏòàÏãúÎì§ÏùÑ Ï≤¥ÌóòÌï¥Î≥¥Í≥† Ïã∂Í±∞ÎÇò, ÏµúÏµúÏµúÏ≤®Îã® ÏΩîÎìúÎ•º ÏõêÌïòÍ±∞ÎÇò, ÏÉàÎ°úÏö¥ Î≤ÑÏ†ÑÏù¥ ÎÇòÏò¨ ÎïåÍπåÏßÄ Í∏∞Îã§Î¶¥ Ïàò ÏóÜÎã§Î©¥ [ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÜåÏä§ÏóêÏÑú Î∞îÎ°ú ÏÑ§Ïπò](https:\\u002f\\u002fhuggingfac...\"],[\"```\\n\\nFlax, PyTorch, TensorFlow ÏÑ§Ïπò ÌéòÏù¥ÏßÄÏóêÏÑú Ïù¥Îì§ÏùÑ condaÎ°ú ÏÑ§ÏπòÌïòÎäî Î∞©Î≤ïÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\\n\\n## Î™®Îç∏ Íµ¨Ï°∞\\n\\n**ü§ó TransformersÍ∞Ä Ï†úÍ≥µÌïòÎäî...\"],[\"1. **[ALBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002falbert)** (from Google Research and...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbit)** (from Google AI) released with ...\"],[\"1. **[BLIP-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fblip-2)** (Salesforce ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Junna...\"],[\"1. **[BROS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbros)** (NAVER CLOVA ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Teakgyu ...\"],[\"1. **[CANINE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcanine)** (Google Research ÏóêÏÑú) Jona...\"],[\"1. **[CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)** (OpenAI ÏóêÏÑú) Alec Radford, Jon...\"],[\"1. **[CodeLlama](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama_code)** (MetaAI ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Ba...\"],[\"1. **[ConvNeXT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvnext)** (Facebook AI ÏóêÏÑú) Zhua...\"],[\"1. **[CPM-Ant](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpmant)** (from OpenBMB) released ...\"],[\"1. **[DeBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeberta)** (Microsoft ÏóêÏÑú) Pengchen...\"],[\"1. **[DeiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeit)** (Facebook ÏóêÏÑú) Hugo Touvron, M...\"],[\"1. **[DialoGPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdialogpt)** (Microsoft Research Ïóê...\"],[\"1. **[DistilBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdistilbert)** (HuggingFace ÏóêÏÑú) ...\"],[\"1. **[DPR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdpr)** (Facebook ÏóêÏÑú) Vladimir Karpukhi...\"],[\"1. **[ELECTRA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002felectra)** (Google Research\\u002fStanfo...\"],[\"1. **[ErnieM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie_m)** (Baidu ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Xuan Ouya...\"],[\"1. **[FNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffnet)** (from Google Research) releas...\"],[\"1. **[GPT NeoX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_neox)** (EleutherAI ÏóêÏÑú) Sid B...\"],[\"1. **[GPT-Sw3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt-sw3)** (AI-Sweden ÏóêÏÑú) Ariel Ek...\"],[\"1. **[GPTSAN-japanese](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgptsan-japanese)** release...\"],[\"1. **[Hubert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fhubert)** (Facebook ÏóêÏÑú) Wei-Ning Hs...\"],[\"1. **[Informer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002finformer)** (from Beihang Univers...\"],[\"1. **[KOSMOS-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fkosmos-2)** (from Microsoft Resea...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (Microsoft Resear...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (South China University of Te...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (The FAIR team of Meta AI...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (Microsoft Research & Unive...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (UNC Chapel Hill ÏóêÏÑú) Hao ...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (Meta and UIUC ÏóêÏÑú...\"],[\"1. **[mBART-50](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmbart)** (Facebook ÏóêÏÑú) Yuqing Tan...\"],[\"1. **[Megatron-GPT2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmegatron_gpt2)** (NVIDIA ÏóêÏÑú)...\"],[\"1. **[Mixtral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmixtral)** (from Mistral AI) by Th...\"],[\"1. **[MobileBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilebert)** (CMU\\u002fGoogle Brain...\"],[\"1. **[MobileViTV2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevitv2)** (Apple ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ ...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (Google AI ÏóêÏÑú) Linting Xue, Noa...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (Huawei Noah‚Äôs Ark Lab ÏóêÏÑú) ...\"],[\"1. **[Nystr√∂mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (the Univer...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (Google AI ÏóêÏÑú) Matthias ...\"],[\"1. **[PatchTST](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpatchtst)** (IBM ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Yuqi Nie...\"],[\"1. **[Persimmon](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpersimmon)** (ADEPT ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Eric...\"],[\"1. **[Pix2Struct](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpix2struct)** (Google ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ K...\"],[\"1. **[ProphetNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fprophetnet)** (Microsoft Resear...\"],[\"1. **[RAG](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frag)** (Facebook ÏóêÏÑú) Patrick Lewis, Et...\"],[\"1. **[RemBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002frembert)** (Google Research ÏóêÏÑú) Hy...\"],[\"1. **[RoCBert](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froc_bert)** (WeChatAI ÏóêÏÑú) HuiSu, W...\"],[\"1. **[SeamlessM4Tv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t_v2)** (from Met...\"],[\"1. **[SEW-D](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew_d)** (ASAPP ÏóêÏÑú) Felix Wu, Kwangy...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (Microsoft ÏóêÏÑú) Ze...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (Google AI ÏóêÏÑú) Colin Raffel and N...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (Microsoft Research ÏóêÏÑú) Qia...\"],[\"1. **[Transformer-XL](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftransfo-xl)** (Google\\u002fCMU Ïóê...\"],[\"1. **[UL2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ful2)** (Google Research ÏóêÏÑú) Yi Tay, Mo...\"],[\"1. **[UniSpeechSat](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002funispeech-sat)** (Microsoft R...\"],[\"1. **[VideoMAE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvideomae)** (Multimedia Computing...\"],[\"1. **[Vision Transformer (ViT)](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit)** (Google AI...\"],[\"1. **[VitDet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvitdet)** (Meta AI ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Yanghao ...\"],[\"1. **[VITS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvits)** (Kakao Enterprise ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Jae...\"],[\"1. **[Wav2Vec2Phoneme](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fwav2vec2_phoneme)** (Faceb...\"],[\"1. **[X-CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxclip)** (Microsoft Research ÏóêÏÑú) Bo...\"],[\"1. **[XLM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm)** (Facebook ÏóêÏÑú) Guillaume Lample ...\"],[\"1. **[XLM-V](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlm-v)** (Meta AI ÏóêÏÑú) Davis Liang, H...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (Facebook A...\"],[\"1. ÏÉàÎ°úÏö¥ Î™®Îç∏ÏùÑ Ïò¨Î¶¨Í≥† Ïã∂ÎÇòÏöî? Ïö∞Î¶¨Í∞Ä **ÏÉÅÏÑ∏Ìïú Í∞ÄÏù¥ÎìúÏôÄ ÌÖúÌîåÎ¶ø** ÏúºÎ°ú ÏÉàÎ°úÏö¥ Î™®Îç∏ÏùÑ Ïò¨Î¶¨ÎèÑÎ°ù ÎèÑÏôÄÎìúÎ¶¥Í≤åÏöî. Í∞ÄÏù¥ÎìúÏôÄ ÌÖúÌîåÎ¶øÏùÄ Ïù¥ Ï†ÄÏû•ÏÜåÏùò [`templates`](.\\u002fte...\"],[\"Í∞Å Î™®Îç∏Ïù¥ Flax, PyTorch, TensorFlowÏúºÎ°ú Íµ¨ÌòÑÎêòÏóàÎäîÏßÄ ÎòêÎäî ü§ó Tokenizers ÎùºÏù¥Î∏åÎü¨Î¶¨Í∞Ä ÏßÄÏõêÌïòÎäî ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏÇ¨Ïö©ÌïòÎäîÏßÄ ÌôïÏù∏ÌïòÎ†§Î©¥, [Ïù¥ Ìëú](https...\"],[\"| ÏÑπÏÖò | ÏÑ§Î™Ö |\\n|-|-|\\n| [ÎèÑÌÅêÎ®ºÌä∏](https:\\u002f\\u002fhuggingface.co\\u002ftransformers\\u002f) | Ï†ÑÏ≤¥ API ÎèÑÌÅêÎ®ºÌä∏ÏôÄ ÌäúÌÜ†Î¶¨Ïñº |\\n| [Í≥ºÏ†ú ÏöîÏïΩ](htt...\"],[\"| [ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmigration) | `pytorch-transformers`ÎÇò `pytorch-pr...\"],[\"## Ïù∏Ïö©\\n\\nü§ó Transformers ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º Ïù∏Ïö©ÌïòÍ≥† Ïã∂Îã§Î©¥, Ïù¥ [ÎÖºÎ¨∏](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f)ÏùÑ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check more detailed information for [oneccl_bind_pt](https:\\u002f\\u002fgithub.com\\u002fintel\\u002ftorch-ccl).\\n\\n### Intel...\"],[\"```\\npip install oneccl_bind_pt=={pytorch_version} -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-stable-cpu...\"],[\"```\\n\\n#### Intel¬Æ Extension for PyTorch installation\\n\\nIntel Extension for PyTorch (IPEX) provides per...\"],[\"```\\nThe following command enables training with a total of four processes on two Xeons (node0 and no...\"],[\"```\\n\\n## Usage with Kubernetes\\n\\nThe same distributed training job from the previous section can be de...\"],[\"```\\nFROM intel\\u002fai-workflows:torch-2.0.1-huggingface-multinode-py3.9\\n\\nWORKDIR \\u002fworkspace\\n\\n# Download ...\"],[\"```\\nThe image needs to be built and copied to the cluster's nodes or pushed to a container registry ...\"],[\"The snippet below is an example of a yaml file for a PyTorchJob with 4 workers running the\\n[question...\"],[\"- \\\"3e-5\\\"\\n                - --num_train_epochs\\n                - \\\"2\\\"\\n                - --max_seq_leng...\"],[\"- name: pvc-volume\\n                mountPath: \\u002ftmp\\u002fpvc-mount\\n              - mountPath: \\u002fdev\\u002fshm\\n   ...\"],[\"```\\nTo run this example, update the yaml based on your training script and the nodes in your cluster...\"],[\"```\\nNAME                                                     READY   STATUS                  RESTART...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper states the following:\\n\\n*Visual language data such as plots, charts, and in...\"],[\"## Usage\\n\\nCurrently 6 checkpoints are available for MatCha:\\n\\n- `google\\u002fmatcha`: the base MatCha mode...\"],[\"inputs = processor(images=image, text=\\\"Is the sum of all 4 places greater than Laos?\\\", return_tensor...\"],[\"```\\n\\n## Fine-tuning\\n\\nTo fine-tune MatCha, refer to the pix2struct [fine-tuning notebook](https:\\u002f\\u002fgit...\"],[\"!--Copyright 2022 The HuggingFace Team and The OpenBMB Team. All rights reserved.\\n\\nLicensed under th...\"],[\"## CpmAntConfig\\n\\n[[autodoc]] CpmAntConfig\\n    - all\\n\\n## CpmAntTokenizer\\n\\n[[autodoc]] CpmAntTokenizer...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recent breakthroughs in natural language process...\"],[\"processor = AutoImageProcessor.from_pretrained('facebook\\u002fdinov2-base')\\nmodel = AutoModel.from_pretra...\"],[\"```\\n\\n## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[IDEFICS](..\\u002fmodel_doc\\u002fidefics) is an open-access vision and language model based on [Flamingo](http...\"],[\"Before you begin, make sure you have all the necessary libraries installed. \\n\\n```bash\\npip install -q...\"],[\"```\\n\\n\\u003cTip\\u003e\\nTo run the following examples with a non-quantized version of the model checkpoint you wi...\"],[\"```\\n\\nSetting `device_map` to `\\\"auto\\\"` will automatically determine how to load and store the model w...\"],[\"```\\n\\nNow that you have the model loaded in one of the suggested ways, let's move on to exploring tas...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nIt is a good idea to include the `bad_words_ids` in the call to `generate` to avoid erro...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n## Few-shot prompting\\n\\nWhile IDEFICS demonstrates great zero-shot results, your task may requir...\"],[\"Photo by [Juan Mayobre](https:\\u002f\\u002funsplash.com\\u002f@jmayobres).\\n  \\n```py\\n\\u003e\\u003e\\u003e prompt = [\\\"User:\\\",\\n...       ...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gen...\"],[\"```\\n\\nNotice that just from a single example (i.e., 1-shot) the model has learned how to perform the ...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\n## Image classification\\n\\nIDEFICS is capable of classifying images into different categories wit...\"],[\"\\u003e\\u003e\\u003e generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\\n\\u003e\\u003e\\u003e gene...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeniz...\"],[\"```\\n\\nLooks like IDEFICS noticed the pumpkin on the doorstep and went with a spooky Halloween story a...\"],[\"## Running inference in batch mode\\n\\nAll of the earlier sections illustrated IDEFICS for a single exa...\"],[\"\\u003e\\u003e\\u003e inputs = processor(prompts, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\\u003e\\u003e\\u003e bad_words_ids = processor.tokeni...\"],[\"```\\n\\n## IDEFICS instruct for conversational use\\n\\nFor conversational use cases, you can find fine-tun...\"],[\"...         \\\"\\\\nUser:\\\",\\n...         \\\"https:\\u002f\\u002fstatic.wikia.nocookie.net\\u002fasterix\\u002fimages\\u002f2\\u002f25\\u002fR22b.gif\\u002fr...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003cb\\u003eEnglish\\u003c\\u002fb\\u003e |\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"\\u003ch3 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003eState-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\\u003c\\u002fp\\u003e\\n\\u003c\\u002fh...\"],[\"## Online demos\\n\\nYou can test most of our models directly on their pages from the [model hub](https:...\"],[\"In Natural Language Processing:\\n- [Masked word completion with BERT](https:\\u002f\\u002fhuggingface.co\\u002fbert-bas...\"],[\"- [Natural Language Inference with RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002froberta-large-mnli?text=The+dog+w...\"],[\"- [Question answering with...\"],[\"- [Translation with T5](https:\\u002f\\u002fhuggingface.co\\u002ft5-base?text=My+name+is+Wolfgang+and+I+live+in+Berlin...\"],[\"In Computer Vision:\\n- [Image classification with ViT](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fvit-base-patch16...\"],[\"In Multimodal tasks:\\n- [Table Question Answering with TAPAS](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002ftapas-bas...\"],[\"## Quick tour\\n\\nTo immediately use a model on a given input (text, image, audio, ...), we provide the...\"],[\"```\\n\\nThe second line of code downloads and caches the pretrained model used by the pipeline, while t...\"],[\"# Allocate a pipeline for object detection\\n\\u003e\\u003e\\u003e object_detector = pipeline('object-detection')\\n\\u003e\\u003e\\u003e ob...\"],[\"```\\n\\nHere, we get a list of objects detected in the image, with a box surrounding the object and a c...\"],[\"```\\n\\nThe tokenizer is responsible for all the preprocessing the pretrained model expects and can be ...\"],[\"1. Easily customize a model or an example to your needs:\\n    - We provide examples for each architec...\"],[\"First, create a virtual environment with the version of Python you're going to use and activate it.\\n...\"],[\"```\\n\\nIf you'd like to play with the examples or need the bleeding edge of the code and can't wait fo...\"],[\"```\\n\\nFollow the installation pages of Flax, PyTorch or TensorFlow to see how to install them with co...\"],[\"1. **[Autoformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fautoformer)** (from Tsinghua Un...\"],[\"1. **[BARTpho](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fbartpho)** (from VinAI Research) r...\"],[\"1. **[CLAP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclap)** (from LAION-AI) released with...\"],[\"1. **[CodeGen](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcodegen)** (from Salesforce) relea...\"],[\"1. **[ConvBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fconvbert)** (from YituTech) relea...\"],[\"1. **[CPM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fcpm)** (from Tsinghua University) rele...\"],[\"1. **[Data2Vec](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdata2vec)** (from Facebook) relea...\"],[\"1. **[Deformable DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdeformable_detr)** (from S...\"],[\"1. **[DETR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdetr)** (from Facebook) released with...\"],[\"1. **[DINOv2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdinov2)** (from Meta AI) released w...\"],[\"1. **[DiT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fdit)** (from Microsoft Research) relea...\"],[\"1. **[EfficientFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fefficientformer)** (from S...\"],[\"1. **[ERNIE](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fernie)** (from Baidu) released with ...\"],[\"1. **[ESM](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fesm)** (from Meta AI) are transformer ...\"],[\"1. **[FlauBERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fflaubert)** (from CNRS) released ...\"],[\"1. **[Funnel Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ffunnel)** (from CMU\\u002fGoo...\"],[\"1. **[GPT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fopenai-gpt)** (from OpenAI) released w...\"],[\"1. **[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt2)** (from OpenAI) released with ...\"],[\"1. **[GPTBigCode](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgpt_bigcode)** (from BigCode) r...\"],[\"1. **[GroupViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fgroupvit)** (from UCSD, NVIDIA) r...\"],[\"1. **[IDEFICS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fidefics)** (from HuggingFace) rele...\"],[\"1. **[Jukebox](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fjukebox)** (from OpenAI) released ...\"],[\"1. **[LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flayoutlmv3)** (from Microsoft R...\"],[\"1. **[LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flilt)** (from South China University ...\"],[\"1. **[Llama2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllama2)** (from The FAIR team of Me...\"],[\"1. **[LLaVa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fllava)** (from Microsoft Research & ...\"],[\"1. **[LXMERT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002flxmert)** (from UNC Chapel Hill) re...\"],[\"1. **[MADLAD-400](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmadlad-400)** (from Google) rel...\"],[\"1. **[MaskFormer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmaskformer)** (from Meta and UI...\"],[\"1. **[MEGA](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmega)** (from Meta\\u002fUSC\\u002fCMU\\u002fSJTU) rele...\"],[\"1. **[Mistral](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmistral)** (from Mistral AI) by Th...\"],[\"1. **[MMS](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmms)** (from Facebook) released with t...\"],[\"1. **[MobileViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmobilevit)** (from Apple) releas...\"],[\"1. **[MT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fmt5)** (from Google AI) released with ...\"],[\"1. **[Nezha](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnezha)** (from Huawei Noah‚Äôs Ark Lab...\"],[\"1. **[Nystr√∂mformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fnystromformer)** (from the U...\"],[\"1. **[OWL-ViT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fowlvit)** (from Google AI) release...\"],[\"1. **[Pegasus](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpegasus)** (from Google) released ...\"],[\"1. **[Phi](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fphi)** (from Microsoft) released with ...\"],[\"1. **[PLBart](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fplbart)** (from UCLA NLP) released ...\"],[\"1. **[PVT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fpvt)** (from Nanjing University, The U...\"],[\"1. **[Reformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002freformer)** (from Google Research...\"],[\"1. **[RoBERTa-PreLayerNorm](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002froberta-prelayernorm)...\"],[\"1. **[SeamlessM4T](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fseamless_m4t)** (from Meta AI)...\"],[\"1. **[SEW](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fsew)** (from ASAPP) released with the ...\"],[\"1. **[SpeechToTextTransformer2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fspeech_to_text_2)...\"],[\"1. **[Swin Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fswin)** (from Microsoft) ...\"],[\"1. **[T5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ft5)** (from Google AI) released with th...\"],[\"1. **[TAPEX](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftapex)** (from Microsoft Research) r...\"],[\"1. **[TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftrocr)** (from Microsoft), released ...\"],[\"1. **[UMT5](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fumt5)** (from Google Research) releas...\"],[\"1. **[UPerNet](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fupernet)** (from Peking University...\"],[\"1. **[VipLlava](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvipllava)** (from University of W...\"],[\"1. **[ViT Hybrid](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fvit_hybrid)** (from Google AI) ...\"],[\"1. **[XLSR-Wav2Vec2](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fxlsr_wav2vec2)** (from Faceb...\"],[\"To check if each model has an implementation in Flax, PyTorch or TensorFlow, or has an associated to...\"],[\"## Citation\\n\\nWe now have a [paper](https:\\u002f\\u002fwww.aclweb.org\\u002fanthology\\u002f2020.emnlp-demos.6\\u002f) you can cit...\"],[\"CodeParrot ü¶ú\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002flvwerra\\u002frepo-images\\u002fra...\"],[\"```\\n\\nAdditionally, sure you have git-lfs installed. You can find instructions for how to install it ...\"],[\"- exact deduplication using each file's hash after having removed whistespaces.\\n- near deduplication...\"],[\"```\\nDuring preprocessing the dataset is downloaded and stored locally as well as caches of the compu...\"],[\"```\\nThis will initialize a new model with the architecture and configuration of `gpt2-large` and use...\"],[\"```\\n\\nRecall that you can see the full set of possible options with descriptions (for all scripts) by...\"],[\"```\\nIn addition we evaluate the model on OpenAI's _HumanEval_ benchmark. You can run the evaluation ...\"],[\"```\\n\\nThe results as well as reference values are shown in the following table:\\n\\n| Model | pass@1 | p...\"],[\"## Training with Megatron\\n[Megatron](https:\\u002f\\u002fgithub.com\\u002fNVIDIA\\u002fMegatron-LM) is a framework developed...\"],[\"```\\n\\nYou also need to add the vocabulary file and merges table of the tokenizer that you trained on ...\"],[\"```\\nThis outputs two files `codeparrot_content_document.idx` and `codeparrot_content_document.bin` w...\"],[\"### Training\\nYou can configure the model architecture and training parameters as shown below, or put...\"],[\"--lr-warmup-iters 2000\\n--weight-decay .1\\n--adam-beta2 .999\\n--fp16\\n--log-interval 10\\n--save-interval ...\"],[\"```\\nThe training takes almost 12 hours in this setting.\\n\\n### Convert model to `transformers`\\nAfter t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Check more detailed information for [Auto Mixed Precision](https:\\u002f\\u002fintel.github.io\\u002fintel-extension-f...\"],[\"```\\npip install intel_extension_for_pytorch==\\u003cversion_name\\u003e -f https:\\u002f\\u002fdeveloper.intel.com\\u002fipex-whl-...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nTo print summary statistics for the GPU utilization and the training run with the [`Trainer`] w...\"],[\"```\\n\\nWe see that the kernels alone take up 1.3GB of GPU memory. Now let's see how much space the mod...\"],[\"```\\n\\n```bash\\nTue Jan 11 08:58:05 2022\\n+-------------------------------------------------------------...\"],[\"+-----------------------------------------------------------------------------+\\n| Processes:        ...\"],[\"```\\n\\nWe get the same number as before and you can also see that we are using a V100 GPU with 16GB of...\"],[\"```\\n\\nWe see that already a relatively small batch size almost fills up our GPU's entire memory. Howe...\"],[\"1. model weights\\n2. optimizer states\\n3. gradients\\n4. forward activations saved for gradient computat...\"],[\"**Functionality-specific memory**\\n\\nThen, your software could have special memory needs. For example,...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[ALBERT](..\\u002fmodel_doc\\u002falbert), [BART](..\\u002fmodel_doc\\u002fbart), [BERT](..\\u002fmodel_doc\\u002fbert), [BigBird](..\\u002fmo...\"],[\"[FNet](..\\u002fmodel_doc\\u002ffnet), [Funnel Transformer](..\\u002fmodel_doc\\u002ffunnel), [GPT-Sw3](..\\u002fmodel_doc\\u002fgpt-sw3...\"],[\"[Megatron-BERT](..\\u002fmodel_doc\\u002fmegatron-bert), [Mistral](..\\u002fmodel_doc\\u002fmistral), [Mixtral](..\\u002fmodel_doc...\"],[\"[RoBERTa-PreLayerNorm](..\\u002fmodel_doc\\u002froberta-prelayernorm), [RoCBert](..\\u002fmodel_doc\\u002froc_bert), [RoForm...\"],[\"\\u003c!--End of the generated tip--\\u003e\\n\\n\\u003c\\u002fTip\\u003e\\n\\nBefore you begin, make sure you have all the necessary libr...\"],[\"```\\n\\nWe encourage you to login to your Hugging Face account so you can upload and share your model w...\"],[\"```\\n\\nThen take a look at an example:\\n\\n```py\\n\\u003e\\u003e\\u003e imdb[\\\"test\\\"][0]\\n{\\n    \\\"label\\\": 0,\\n    \\\"text\\\": \\\"I lov...\"],[\"```\\n\\nCreate a preprocessing function to tokenize `text` and truncate sequences to be no longer than ...\"],[\"```\\n\\nThen create a function that passes your predictions and labels to [`~evaluate.EvaluationModule....\"],[\"```\\n\\nAt this point, only three steps remain:\\n\\n1. Define your training hyperparameters in [`TrainingA...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n[`Trainer`] applies dynamic padding by default when you pass `tokenizer` to it. In this ...\"],[\"```\\n\\nConvert your datasets to the `tf.data.Dataset` format with [`~transformers.TFPreTrainedModel.pr...\"],[\"```\\n\\nSpecify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\\n\\n```...\"],[\"```\\n\\nThe simplest way to try out your finetuned model for inference is to use it in a [`pipeline`]. ...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nTokenize the text and return TensorFlow tensors:\\n\\n```py\\n\\u003e\\u003e\\u003e from transformers import ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n *The design choices in the Transformer attention mec...\"],[\"## MegaConfig\\n\\n[[autodoc]] MegaConfig\\n\\n## MegaModel\\n\\n[[autodoc]] MegaModel\\n    - forward\\n\\n## MegaFor...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## Overview\\n\\nThe XLM-ProphetNet model was proposed in [ProphetNet: Predicting Future N-gram for Sequ...\"],[\"The Authors' code can be found [here](https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fProphetNet).\\n\\n## Resources\\n\\n- [Ca...\"],[\"!--Copyright 2022 NVIDIA and The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache L...\"],[\"The abstract from the paper is the following:\\n\\n*Grouping and recognition are important components of...\"],[\"## Resources\\n\\nA list of official Hugging Face and community (indicated by üåé) resources to help you g...\"],[\"## TFGroupViTVisionModel\\n\\n[[autodoc]] TFGroupViTVisionModel\\n    - call\\n\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nThis will create a `imagenette2` dir with two subdirectories `train` and `val` each with multip...\"],[\"!--Copyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## PreTrainedModel\\n\\n[[autodoc]] PreTrainedModel\\n    - push_to_hub\\n    - all\\n\\n\\u003ca id='from_pretrained-...\"],[\"```\\n\\nMoreover, you can directly place the model on different devices if it doesn't fully fit in RAM ...\"],[\"```\\n\\nYou can inspect how the model was split across devices by looking at its `hf_device_map` attrib...\"],[\"```\\n\\nYou can also write your own device map following the same format (a dictionary layer name to de...\"],[\"```\\n\\nDue to Pytorch design, this functionality is only available for floating dtypes.\\n\\n\\n## ModuleUti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The \\\"Roaring 20s\\\" of visual recognition began with t...\"],[\"\\u003csmall\\u003e ConvNeXT architecture. Taken from the \\u003ca href=\\\"https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2201.03545\\\"\\u003eoriginal pa...\"],[\"## ConvNextFeatureExtractor\\n\\n[[autodoc]] ConvNextFeatureExtractor\\n\\n## ConvNextImageProcessor\\n\\n[[auto...\"],[\"!---\\nCopyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"In this example we will use the vision model from [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fmodels?filter=clip)\\n...\"],[\"```\\nhuggingface-cli repo create clip-roberta-base\\n```\\nNext we clone the model repository to add the ...\"],[\"```\\n\\nIf the checkpoints are in PyTorch then one could pass `text_from_pt=True` and `vision_from_pt=T...\"],[\"```\\n\\n### Prepare dataset files and split the dataset.\\n\\n```python\\nimport json\\nimport collections\\n\\nima...\"],[\"```\\n\\n\\u003e Note: The data loading and processing part of this script can still be improved for maximum p...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Vision-and-Language Pre-training (VLP) has improved ...\"],[\"## Usage tips\\n\\n- The quickest way to get started with ViLT is by checking the [example notebooks](ht...\"],[\"## ViltForMaskedLM\\n\\n[[autodoc]] ViltForMaskedLM\\n    - forward\\n\\n## ViltForQuestionAnswering\\n\\n[[autodo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Zero-shot image classification pipeline\\n\\nThe simplest way to try out inference with a model ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n     \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdo...\"],[\"```\\n\\nLet's take a different image to switch things up.\\n\\n```py\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e import r...\"],[\"```\\n\\nPass the inputs through the model, and post-process the results:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\n\\u003e\\u003e\\u003e w...\"],[\"!--Copyright 2021 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*The recently-proposed Perceiver model obtains good r...\"],[\"Internally, [`PerceiverModel`] will create the latents, which is a tensor of shape `(batch_size, num...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fperceiver_ar...\"],[\"## Perceiver specific outputs\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverModelOutput\\n...\"],[\"## PerceiverProjectionDecoder\\n\\n[[autodoc]] models.perceiver.modeling_perceiver.PerceiverProjectionDe...\"],[\"## PerceiverForMaskedLM\\n\\n[[autodoc]] PerceiverForMaskedLM\\n    - forward\\n\\n## PerceiverForSequenceClas...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Development of the model was led by [Shinya Otani](https:\\u002f\\u002fgithub.com\\u002fSO0529), [Takayoshi Makabe](ht...\"],[\"\\u003e\\u003e\\u003e print(gen_text)\\n‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅAI„Å®‰∫∫„ÅåÂÖ±Â≠ò„Åó„ÄÅAI„ÇíÊ≠£„Åó„ÅèÁêÜËß£„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ...\"],[\"```\\n\\n## Resources\\n\\n- [Causal language modeling task guide](..\\u002ftasks\\u002flanguage_modeling)\\n\\n## GPTNeoXJa...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"## Enforcement\\n\\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\\nreported ...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within the\\ncommunity.\\n\\n## Attri...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"For custom datasets in `jsonlines` format please see: https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002floading_d...\"],[\"```\\n\\nOnly T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional a...\"],[\"```\\n\\nThe task of summarization supports custom CSV and JSONLINES formats.\\n\\n#### Custom CSV Files\\n\\nIf...\"],[\"```\\n\\nand you wanted to select only `text` and `summary`, then you'd pass these additional arguments:...\"],[\"```\\n\\n## With Accelerate\\n\\nBased on the script [`run_summarization_no_trainer.py`](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"```\\n\\nand reply to the questions asked. Then\\n\\n```bash\\naccelerate test\\n```\\n\\nthat will check everything...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is the following:\\n\\n*Many real-world applications require the prediction ...\"],[\"- Check out the Informer blog-post in HuggingFace blog: [Multivariate Probabilistic Time Series Fore...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nHaving downloaded COCO dataset manually you should be able to load with the `ydshieh\\u002fcoc_datase...\"],[\"```\\n\\n### Create a model from a vision encoder model and a text encoder model\\nWe can either load a CL...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"However, if the preferred batch size fits into memory, there's no reason to apply memory-optimizing ...\"],[\"Note: when using mixed precision with a small model and a large batch size, there will be some memor...\"],[\"[Tensor Core Requirements](https:\\u002f\\u002fdocs.nvidia.com\\u002fdeeplearning\\u002fperformance\\u002fdl-performance-matrix-mu...\"],[\"```\\n\\nIn the above example, your effective batch size becomes 4. \\n\\nAlternatively, use ü§ó Accelerate to...\"],[\"**Gradient checkpointing** offers a compromise between these two approaches and saves strategically ...\"],[\"```\\n\\nAlternatively, use ü§ó Accelerate - find the ü§ó Accelerate example [further in this guide](#using-...\"],[\"```\\n\\nIf you prefer to use ü§ó Accelerate, find the ü§ó Accelerate example [further in this guide](#using...\"],[\"```\\nimport torch\\ntorch.backends.cuda.matmul.allow_tf32 = True\\ntorch.backends.cudnn.allow_tf32 = True...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\ntf32 can't be accessed directly via `tensor.to(dtype=torch.tf32)` because it is an inter...\"],[\"[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"```\\n\\nHowever, we can also use a third-party implementation of the 8-bit optimizer for demonstration ...\"],[\"optimizer_kwargs = {\\n    \\\"betas\\\": (training_args.adam_beta1, training_args.adam_beta2),\\n    \\\"eps\\\": t...\"],[\"```\\n\\nFinally, pass the custom optimizer as an argument to the `Trainer`:\\n\\n```py\\ntrainer = Trainer(mo...\"],[\"```\\n\\nCombined with other approaches (gradient accumulation, gradient checkpointing, and mixed precis...\"],[\"DeepSpeed is an open-source deep learning optimization library that is integrated with ü§ó Transformer...\"],[\"```\\n\\n`torch.compile` uses Python's frame evaluation API to automatically create a graph from existin...\"],[\"**Training & inference backends**:\\n* `dynamo.optimize(\\\"inductor\\\")` - Uses TorchInductor backend with...\"],[\"**Inference-only backend**s:\\n* `dynamo.optimize(\\\"ofi\\\")` -  Uses Torchscript optimize_for_inference. ...\"],[\"```\\n\\nThe full example training loop with ü§ó Accelerate is only a handful of lines of code long:\\n\\n```p...\"],[\"```\\n\\nFirst we wrap the dataset in a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.u...\"],[\"At times, additional efforts may be required to pre-build some components. For instance, if you're u...\"],[\"![MoE Transformer 2x block](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve...\"],[\"And for Pytorch DeepSpeed has built one as well: [DeepSpeed-MoE: Advancing Mixture-of-Experts Infere...\"],[\"```\\n\\nOnce converted, train the model as usual.\\n\\n\\u003cTip warning={true}\\u003e\\n\\nThe PyTorch-native `scaled_dot...\"]],\"hovertemplate\":\"source=transformers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"transformers, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"transformers, circle\",\"showlegend\":true,\"x\":[-3.2439046,-0.65824056,-0.39209658,-2.8409197,-4.077847,-4.095727,-4.403895,-5.4324193,-3.4449284,-3.0831854,-4.005998,-0.74845284,-4.68445,-0.60131866,-3.5397696,-3.8673108,-2.8489683,-2.9084742,-4.2511516,-2.497276,-2.9073808,-2.007203,-2.9440532,-2.1063554,-2.99278,-2.7896535,-3.0879493,1.16813,2.0762143,2.6775877,2.5896065,2.6136038,2.570615,-7.87975,-8.243341,3.2442906,-8.519976,6.201407,6.2026887,6.2030582,6.2019587,6.1980925,6.195159,6.1993084,6.198116,6.201177,6.202553,6.201941,6.1976423,6.1948757,6.195118,-8.490352,2.6617193,3.029464,2.6798885,2.6478386,2.6123781,2.2123284,2.6315527,2.4658346,2.4343224,-5.941962,2.3686538,2.9418094,1.9824897,2.6763716,2.9484398,2.9457066,2.81541,2.936253,2.739509,1.9630408,1.7395765,2.3868427,2.6313927,2.4250534,-7.3717175,-5.4957476,-7.921062,-6.3990116,-6.6919184,-6.9637346,-6.7780523,-6.7839146,-6.8220234,-6.716173,-7.247388,-7.3048086,-6.57852,-6.499779,-7.6753354,-6.1487393,-6.655941,-6.9466915,-6.2520447,-6.6119084,-6.236454,-6.1249647,-6.466825,-6.1584544,-6.2509937,-7.357413,-5.837579,-6.3289886,-6.4787154,-6.9310813,-6.270626,-7.299529,-6.3530393,-7.0510235,-6.792075,-7.5153685,-6.899899,-6.7545037,-6.338848,-5.878555,-6.8677015,-6.8918896,-6.14453,-6.3907266,-7.7933626,-6.538979,-7.8546805,-6.553793,-6.2122383,-5.84233,-6.155465,-7.365924,-7.7603064,-6.557597,-6.329358,-7.660219,-7.8692784,-6.756518,-7.1454167,-7.5408144,-7.754213,2.5293045,2.5571358,2.3276901,2.7069395,-2.7636778,1.5984946,1.8759314,-3.0666823,0.16193262,0.52462846,0.5684397,1.642554,1.2239559,-4.03547,-3.8507721,-4.0428543,-4.4506235,-3.349765,-5.621881,-5.970905,-0.53680426,-0.5551028,-2.4333851,-2.390021,-2.380191,-4.8995147,-5.0303283,-4.5495615,-0.5159104,-5.23477,-3.9723434,0.93103385,-9.0479965,-9.002112,-9.015544,-5.9403763,-9.021457,-8.926945,-8.934612,-8.931745,-8.959322,-9.017626,-9.014614,-1.5558949,-9.16552,4.4170957,-8.747251,-8.664855,-8.804765,-8.531444,3.4230044,-8.981556,-8.962481,3.2272103,-9.006399,-9.043821,-2.351044,-2.4556618,-8.992469,-9.025087,-8.934013,-8.940959,-8.932329,-8.951416,-2.5282052,-9.1867075,-8.748773,-8.994734,-2.7409894,-8.959943,-4.169334,-8.95146,-8.981219,-9.094091,-2.2366054,-3.1928806,-8.252775,-9.069856,-6.3991046,-0.49491504,-6.4867563,-2.5294063,-2.841209,-4.147481,-4.719604,-4.3263874,-3.8069355,-3.1593733,-4.3566456,-4.0235314,-6.050418,2.210295,1.2663859,-4.2776356,-3.1816595,-1.7241741,-3.8161004,-3.2774663,-4.8523717,-4.1058025,-2.9208562,-3.4107184,-4.2959585,3.4457085,3.0520003,-6.0060005,-4.020222,-3.148487,-2.7324693,-2.8488266,-2.9144847,-4.570368,-6.870456,-3.0285947,-2.8938315,-3.1934366,-2.4049232,-3.759161,-3.7195737,-3.8313656,-4.3115,-3.8313873,-4.037238,2.8537722,-4.1310163,-5.3113856,-5.220368,-5.9671793,-1.2314695,-3.8032615,-5.1627584,-5.6163034,-6.6430945,0.84681684,-5.571677,-5.9800243,-8.000835,-6.375124,-5.891973,-7.7619805,-7.6401,-0.8809922,-4.8837934,-5.444205,-4.2396755,-3.8688588,-4.0959754,-4.3929586,-3.156583,-1.9316171,0.92062527,-0.11610397,-1.4963887,-2.0899434,-2.295468,-1.8960046,-1.6641622,-1.9185072,-1.8775238,-1.8395159,-1.7275847,-2.2998948,-2.6203651,-2.1803637,-1.978005,-3.052508,-2.1521668,-6.212861,-6.2301617,-4.51465,-5.1800623,-4.6000633,-5.5086956,-5.077922,-0.5612669,-5.3038473,2.2784762,-7.4146314,2.550408,2.692262,-8.3751745,-8.283098,-8.353793,-8.573319,6.2021294,6.2024317,6.202513,6.201337,6.1986012,6.191795,6.197378,6.198224,6.2011266,6.201296,6.2024984,6.1970544,6.192729,6.196002,-8.43687,2.5550091,2.4904149,2.4365814,2.288837,2.7288134,0.9127761,2.4062104,2.6519783,-7.4048543,-5.256625,-7.578748,-6.329126,-6.822075,-7.1546054,-6.915743,-7.026743,-6.8388896,-7.1249676,-6.7835155,-7.463873,-6.2917614,-6.817995,-7.5326104,-6.446853,-6.772583,-7.0382085,-6.899394,-6.4531097,-6.9848456,-6.3606367,-6.609522,-6.318692,-6.470329,-6.875914,-5.7061367,-6.7286596,-6.433999,-6.518506,-6.872738,-6.503485,-7.601958,-6.443459,-6.5232477,-6.842148,-6.575082,-7.421433,-6.691226,-6.7560163,-6.799424,-6.4016485,-5.446353,-6.113176,-6.8579874,-6.9234347,-6.424006,-6.306708,-7.7011747,-6.729821,-7.806838,-7.777564,-6.3268647,-6.2651644,-6.1355476,-6.542723,-7.8477583,-6.702716,-6.895969,-6.4434614,-7.7313766,-7.8258824,-6.7225137,-7.233083,-7.626664,-7.6419683,2.4768176,2.657103,-3.7423952,-3.7128043,-3.4545362,-3.4696746,-3.4143915,-3.261504,-3.189439,-3.382262,-3.3335214,-3.304672,-3.1362276,-3.0856776,-3.203741,-3.1891074,-3.4405348,-3.2217402,-3.0446043,-3.233161,-3.051514,-3.3092964,-3.3005826,-3.297973,-2.9185033,-3.8097262,-1.194293,-1.5450158,-1.9715042,-8.084585,-8.0032015,-4.0100207,11.160191,-4.339295,-0.5077048,0.84092194,-5.597824,-5.313968,-5.020166,-4.770148,-5.4011,-4.813551,-4.201366,-0.5003515,-0.45145363,-0.3144218,-4.1197186,-3.1995804,-4.3714542,-4.1244783,-3.9199882,-3.4240594,-2.9410892,-7.8294883,11.103917,11.361986,11.186257,11.115469,11.003208,-0.6531095,-4.116589,-3.6170208,-1.0942266,-5.5054398,-3.1211812,-4.0562763,-4.120868,-4.577247,-3.822602,-4.242631,-3.5198174,-3.952353,-9.623727,-9.795085,-10.008123,-10.104022,-9.64625,-9.67902,-6.330588,-2.9016476,-0.21954425,-0.27479428,-0.2608139,-0.30799228,-0.30458114,-6.424446,-6.592826,-6.092848,-4.2635856,-4.219596,-3.455884,-7.6815085,-7.5157228,-7.558788,-7.387784,-6.839868,-6.193151,-0.4440296,-1.8113497,-2.1457264,-1.7860942,-1.0063422,-3.8518336,-4.1357403,-1.79482,-3.3919082,-4.74375,-6.5055227,-6.767416,-5.3087773,-5.4428415,10.951529,-5.438503,-5.2039194,-5.4161873,-5.3231645,-5.4174256,-3.046879,-3.0424144,-3.354681,-3.541008,-2.1989934,-2.4013972,-2.8260844,-3.4482727,-3.2624314,-7.775106,-7.2985473,-7.6301913,-7.213638,-3.0272527,-6.870122,-5.482862,-0.3067079,-4.1550274,-4.1523085,0.9189302,-4.675836,-4.652442,-2.932926,-3.9716523,-3.92905,-4.168238,-4.051428,-3.975562,-0.51819205,-16.29827,-16.533394,-3.4151204,-7.8454056,-7.945659,-6.7672324,-5.077889,-16.658634,-3.44137,-3.4203966,-5.499683,-2.1385138,-5.550923,-3.8359737,-4.7388983,-3.5914183,-3.2339015,-3.8100712,-5.9566054,-7.8974547,-7.8580713,-2.8319778,-2.5381653,-2.5835526,-2.1994402,-8.097653,-8.137528,-2.0679965,10.951773,-3.0100584,-6.0942006,-6.000092,-3.0568256,-4.0967965,-3.6571183,-4.2755427,-4.074852,-4.3959413,-3.1205416,-5.199508,-4.0268965,-0.5949682,-4.1873956,-4.134203,-4.1844296,-3.8452892,-4.8001833,-3.432711,-3.2517219,-0.8051268,-7.4352155,-7.661744,11.054633,-6.649047,-5.709151,-5.686433,-5.6266212,-5.860155,-5.7300916,-4.7592235,-2.7542524,-4.886433,-5.0276227,-4.4828777,-2.5493038,-2.2843113,-3.8058953,-1.0171741,-3.7105317,-4.135649,-4.293802,-1.8124261,-5.449814,-5.0908666,0.5907689,-0.5915982,-4.5611835,-3.204764,-3.6347492,-3.20578,-2.477338,-3.068342,-5.027953,-3.7157013,-5.052021,-1.9819134,-1.8844286,-2.2964914,-2.368246,-2.4750319,-1.6581438,-2.367342,-2.3941877,-0.78102374,0.8210441,2.6032078,2.630651,2.7170556,-8.211981,-8.150492,-8.37846,-8.682307,-8.587012,2.4721618,-5.981742,2.284215,2.3773673,2.9344018,-0.3165985,2.9703944,2.7460601,2.7209349,1.0810021,2.1637523,2.2516034,2.596237,-7.31272,-5.5623417,-6.7903447,-7.360037,-6.8204665,-7.114927,-6.8283377,-6.9752088,-6.8761597,-7.0014997,-7.2325416,-6.6078186,-6.9764166,-7.998684,-6.2034173,-6.8761487,-7.008736,-6.1724653,-6.4349804,-6.3092785,-6.417355,-7.465663,-5.535957,-6.4584727,-6.439466,-6.526658,-6.81947,-6.4451756,-7.657226,-6.432985,-6.3732033,-7.0366516,-6.564486,-6.891348,-6.9745913,-6.8209653,-6.7360907,-6.9400716,-6.3241386,-5.879778,-5.584025,-6.9552774,-6.85897,-6.121066,-6.510377,-6.9683332,-7.6935806,-6.865938,-7.871112,-7.6585784,-6.430449,-6.341999,-5.9669003,-6.5484095,-7.7984443,-7.701328,-6.859933,-6.676996,-6.3871117,-7.8129654,-7.7008553,-6.8295584,-7.2560997,-7.6971045,-7.6849785,3.0239468,2.4996524,2.570607,2.5877297,2.683049,-1.9763722,0.19935173,0.39531955,-1.5383904,-0.8966995,0.09362782,0.68887335,0.13097487,0.42092013,-0.61201245,1.1668571,0.1766758,0.039794307,-5.8523016,-5.34499,-2.6382663,-4.103366,-2.6156566,-4.5213,-0.52068025,-4.182446,-4.5990505,-2.8502202,-3.2895184,-4.4731402,-4.570332,1.0157402,-2.6915853,-3.085601,-3.4974148,-4.3264117,-3.9389682,-4.50997,-3.7586763,-8.042919,-4.589938,-4.86902,-4.2313967,-3.2293842,-4.3751874,-4.6160617,-6.014562,-3.6668274,-4.424208,-3.4175131,-4.5744286,1.2116572,2.567572,-4.7131033,-1.6198044,-7.4217644,-8.16465,-6.9191813,-8.31872,-3.5953672,-4.544649,-5.4328237,-5.0184865,-1.7458967,-2.6194701,-3.7742524,-3.858407,1.1230966,1.9633539,-2.7325647,-5.527879,-7.669442,-7.1408114,-6.857809,-7.410183,-6.818155,-7.585343,-6.439386,-6.516841,-7.4841504,-6.3016996,-6.401985,-6.924466,-6.7447143,-6.7033567,-6.181919,-6.3899837,-6.4116073,-6.330102,-6.912165,-5.760335,-6.8322825,-6.4280925,-6.546543,-6.9108977,-6.3417425,-7.551895,-6.2000732,-6.3857555,-6.732367,-6.685263,-7.2834396,-6.6925797,-6.8088765,-6.7752113,-6.289849,-5.4814053,-6.131281,-6.86524,-6.80738,-6.0932937,-6.3666377,-7.6559362,-6.564325,-7.821808,-7.5604525,-6.2373013,-6.0599566,-5.6599903,-6.4991126,-7.708103,-6.5910635,-6.6076274,-6.4538913,-7.620893,-4.1599092,2.6294131,-4.3842945,1.6224822,0.5030975,-1.9753257,-1.4719642,-1.2471327,-3.7512457,-6.634761,-1.6871282,-0.77958095,-0.25539693,-1.4085608,-2.4086466,-2.3099627,-1.8347516,-0.7573748,-1.9594151,-2.5076647,-2.4887085,-2.7180033,-0.6713304,-0.38885227,-2.4253216,-2.19311,-2.2995412,-2.2096686,-6.0551953,-5.635663,-6.035425,-5.8167663,-5.7932754,0.47651348,1.2443494,-5.5469327,-5.2463827,-3.2780018,-2.2151546,-2.7537088,-2.1538267,-1.2832346,-4.9457755,-4.4196553,-9.114981,-9.192844,-0.6939857,0.9520939,-8.23748,-6.115537,-4.1046968,-4.1322846,-4.04728,-0.561486,-3.2279475,-0.8911305,-2.0370185,-2.7063913,-2.7731154,-2.6248767,-3.071268,-2.8900113,-4.0806375,-4.1614885,-3.3792982,-0.85697454,-4.6632853,-4.525362,-2.5124497,-2.0727453,0.20871748,-1.6667601,-4.631597,-4.8715663,-2.3093753,-1.0400734,-2.909458,-2.8021743,-2.8617556,-3.0849378,-3.0811892,-3.7601306,-9.2582655,-9.345004,-3.1824725,-1.7148681,-1.7278359,-1.1398079,-5.470737,-5.580836,-6.704552,-5.7544494,-7.370061,-7.2472873,-7.432163,-5.328256,-0.14595488,-0.89780307,-0.011634551,0.23777382,-1.192735,-1.2115513,-2.3851604,-2.3854787,-2.4076095,-4.7380505,0.949952,-4.4511533,-2.1021535,-2.075108,-2.1460843,-2.2934756,-2.160873,-2.0360284,-2.0985641,-2.2459266,-2.3935463,-2.542052,-2.2550056,-2.1298006,-2.4432824,-2.2120073,-2.4350505,-2.024073,-1.6813246,-0.6700108,-1.7008554,-2.235686,-1.9230239,-1.7141744,-1.8849114,-2.9439049,-1.9966594,-2.742749],\"xaxis\":\"x\",\"y\":[-1.418531,-3.355138,-2.6184766,-2.0685837,-5.4688334,-6.5867724,-6.5552106,-6.453217,-1.6594598,-1.7072678,-2.2872534,-5.01788,1.9076436,1.7982308,1.384671,1.2739184,1.5397774,1.3126088,-8.762075,1.3540612,1.702277,-6.7194905,1.3590356,-6.7944674,1.3991387,1.2580582,1.3052776,0.12978877,0.58020425,0.98729926,0.7921416,0.8048423,0.88358617,-2.9625258,-3.145355,0.31670767,-2.7032287,18.95521,18.954205,18.954361,18.95347,18.9573,18.957752,18.95492,18.957958,18.95499,18.956701,18.955723,18.959118,18.960714,18.960215,-2.6521678,0.6556247,0.036636747,0.6702996,0.8496024,0.75831485,0.5522368,0.5739172,0.6006947,0.5698774,-5.442279,0.8939087,0.42804316,0.18967028,0.85876817,0.3799144,0.13989606,0.70539534,0.4320359,0.7155329,-0.066816986,-0.47767848,0.6841264,0.35279378,0.81279427,18.700056,17.971943,18.17211,17.599173,18.105778,18.57759,17.884583,18.137266,16.939648,17.900423,18.500635,18.271809,18.572523,18.523195,18.052645,18.3991,18.060614,17.742764,17.482702,17.64667,18.284773,18.253965,17.742502,17.493505,17.922703,18.564417,17.82503,17.860159,18.072775,16.69567,18.057055,18.300255,18.527676,18.055578,18.080051,18.42988,18.462402,17.391329,18.583136,18.120916,17.766697,17.578815,18.163704,18.1498,18.062035,18.71109,19.138357,18.52644,18.59606,17.943354,18.27111,18.629427,19.06523,18.524536,18.644852,19.02016,19.141031,18.330921,18.13033,18.59627,18.934872,0.7148102,0.8372669,0.6899051,1.057881,-2.0290837,-4.512609,-4.466079,-5.079697,-5.0960464,-4.9226604,-5.0032496,-4.2660666,-3.685851,1.1532465,1.3497247,0.49511784,1.4930311,0.34950843,-1.6592792,-2.9335628,1.6029466,1.6611412,-10.695375,-10.830683,-10.731668,1.258726,1.7336271,0.32994336,1.4346113,-3.2892787,-7.6852365,0.009144856,5.461195,5.6845045,5.6067114,-4.6044545,5.599441,6.006779,5.9281883,5.9282017,5.914495,5.5762978,5.5413604,-6.915977,4.8742175,-0.05805159,5.943306,6.006634,5.9375515,5.8868794,-0.623813,5.861821,5.887517,-0.8100759,5.673802,5.511552,-1.8404928,-10.253418,5.554015,5.5854874,6.00833,5.924901,5.937583,5.981089,1.2791493,4.6637793,5.93403,5.6171703,-1.3929685,5.8355966,-2.8476784,6.019515,5.9473863,4.9295726,-5.308293,-6.40324,-3.9770732,-4.1922474,-2.7628407,1.7849867,-3.0501535,-2.45143,-5.196799,-4.3763475,-4.0126367,-3.7258997,-4.309937,-4.901578,-4.230217,-4.7908764,-3.911678,-3.4556966,-2.5140197,-4.2983985,-4.9673758,-5.0527334,-4.857751,-5.347311,-8.391354,-4.402442,-5.6563315,-6.0316505,-4.317272,-4.366631,-4.0429177,-5.018876,-5.1882796,-5.381505,-5.7601132,-6.5528526,-5.780678,-7.981346,-3.277199,-5.602108,-5.4522257,-5.2666426,-4.645073,-5.325439,-5.106398,-4.9606214,-5.3689127,-4.989009,-4.8024316,-2.0385876,-4.585626,-2.0809152,-3.7310395,-4.6104074,-4.8381248,-1.4319103,-2.218898,-2.582941,-2.4667573,-4.300025,-2.4805927,-6.0470943,-2.5939481,-4.222144,-2.574074,-3.7223663,-3.4887447,1.6783862,-1.1239069,-3.2524219,-5.230838,-6.418663,-5.9731708,-4.8972516,1.2495909,-6.811496,-5.2476754,-6.434569,-6.9471936,-6.45075,-5.654529,-6.593415,-7.010065,-7.204884,-7.256709,-7.009106,-7.049706,-5.29311,-6.248441,-7.134839,-6.1149974,1.3707834,-6.7607794,-2.803469,-3.0934913,-5.298992,-3.8556075,-1.1605216,-2.5920577,-3.6076026,1.6763315,-2.657717,0.5891002,1.2732536,0.9092791,1.0387118,-2.741532,-3.050881,-2.5138242,-2.568217,18.955793,18.95547,18.954998,18.954037,18.956253,18.957504,18.957859,18.956919,18.95509,18.956558,18.954515,18.958122,18.962713,18.959833,-2.630019,0.93064654,0.9558635,0.958096,0.8534461,0.8655256,-1.1234754,0.66656387,1.0164906,18.603806,18.00103,18.181091,17.47989,17.859138,18.443035,17.966385,18.515491,17.154278,18.05638,17.76126,18.559137,18.617413,18.446434,18.194643,18.20264,18.530151,18.148027,17.180428,17.38092,17.975187,18.65042,17.82803,17.632856,17.494074,18.767147,18.005398,17.34722,17.935356,17.965532,16.748905,17.795242,18.850084,18.367096,18.58262,18.188334,17.64324,18.753185,18.655447,17.616394,18.292511,18.634222,17.99598,17.818247,17.794538,17.413784,18.25832,18.138489,18.044748,18.695877,19.12486,19.118668,18.781317,17.545813,17.921083,18.441992,19.023546,18.450478,18.460611,18.565762,19.186785,19.149134,18.346788,18.161882,18.651958,18.959791,0.9830126,1.1490465,-3.1529791,-3.3630855,-3.7380931,-3.3427045,-3.6771648,-4.199052,-4.2319813,-3.3594682,-3.832958,-3.9889429,-4.0868745,-3.728373,-3.7741694,-3.8290567,-3.9367924,-3.8855085,-3.8532274,-3.8891048,-3.3834598,-4.093355,-3.4300334,-2.9832418,2.0141923,-2.7624986,-1.8298365,-1.6917995,-1.9158206,-3.8355982,-3.6967824,-5.4329233,-0.7180263,-5.4795012,1.4682726,-0.16195332,-2.5665472,-2.461666,-3.2657804,-3.1649716,-2.8865123,-3.480726,-3.7022684,1.7252792,1.7077485,1.6744326,1.3068986,3.172909,0.8738089,0.80985373,0.4475435,0.5468429,3.106494,-3.6038673,-0.87667865,-0.6444943,-0.7893491,-0.84421897,-0.8974638,1.7572398,1.5416746,1.5809497,1.8414843,-3.1098018,-3.286687,-3.1797,-3.2931013,-8.86546,1.5070734,1.4263121,0.76092005,-0.4212026,-4.126173,-4.968366,-4.8783503,-4.5837617,-4.3298078,-4.344376,-3.0532138,-1.1677526,1.4070419,1.4564205,1.5687388,1.6016798,1.5948411,-3.7715652,-3.5686574,-4.0143256,1.2300752,1.5801299,0.32519493,-4.1808076,-4.75651,-4.906472,-4.7932916,-5.160259,-6.0106506,1.542765,-3.3121998,-5.9612503,-5.353386,-4.6474037,1.5628667,1.2284018,1.5777177,-1.3848696,-4.056186,-3.2191525,-3.3392272,-4.7925816,-4.805994,-1.1220772,-5.0430818,-5.1597433,-5.336014,-6.619865,-5.480192,-7.103041,-7.048151,-5.104642,-4.756845,-5.3562803,-6.4051323,-6.025329,-5.1047673,-5.91732,-4.860926,-5.034713,-5.0916615,-5.0469413,-5.214754,-4.83213,-6.149465,1.3071057,1.4746405,1.4784158,-0.094420716,-2.7505894,-2.9422524,-5.0313506,-3.844249,-5.141403,-3.7998838,-3.472371,-3.0215402,1.648838,7.000107,6.8516526,0.37270528,-3.6302488,-3.6206088,-3.473017,-3.2547932,6.7969995,1.6665368,0.575596,-3.1573782,-3.6451294,-2.7750916,-2.8150487,-3.6776962,-3.9922552,-4.1918707,-3.697901,-2.9673944,-3.6610603,-3.8088093,1.6430894,1.4002686,1.0604938,1.2273269,-3.7922063,-3.7448807,5.5040946,-1.0628805,-5.1016116,1.1385806,1.1679095,-5.703373,-6.069739,-6.07492,-6.3543744,-6.2280602,-6.2569795,-5.34255,-3.271952,-2.8066332,1.6634992,1.2413087,1.3656137,1.175725,0.7466887,1.0053338,0.31285602,-0.7223722,1.7211663,-3.6705146,-4.196512,-0.95525783,-4.5135317,-6.9890914,-7.174919,-7.045207,-7.112071,-6.992728,-3.0350702,-3.3511,-3.000746,-3.0636315,-3.3549812,-3.673899,-3.0420957,-2.8591862,-4.6043115,-3.2364843,1.2507117,1.3122599,1.390003,1.4916887,1.4678189,-5.4789705,-7.01388,-0.08483837,-6.4896817,-6.341278,-6.854859,-6.040044,-6.081016,1.2751517,-6.1605544,-3.009231,-4.931893,-4.7564178,-3.5577323,-3.2531695,-2.6508627,-2.6072655,-10.7146225,-10.821336,1.7593247,-0.119867824,0.9726442,1.0988637,1.0555017,-2.8525033,-3.011395,-2.4648027,-2.3974879,-2.6745987,0.75441134,-4.2893496,0.6947981,1.0612518,-0.31092906,-1.4622804,0.43602416,0.7892802,0.8961339,-1.2543634,0.25623158,0.7218144,0.97212607,18.602634,17.95561,17.894287,18.344587,17.793924,18.313206,18.471628,16.880047,17.834211,17.484383,18.298079,18.388353,18.32278,18.036116,18.23459,18.086847,17.706291,18.393429,17.666563,17.69881,17.984356,18.589401,17.826426,17.815332,17.931025,17.769962,16.653877,17.951683,18.918795,18.555664,18.643862,17.903303,17.831064,17.785173,18.48196,18.538048,17.618164,18.056406,18.755888,18.249222,17.931404,17.716957,17.644596,18.099358,17.95385,18.175129,18.284634,18.755764,19.133308,18.961418,18.664162,17.771229,18.033215,18.490545,18.989094,18.984509,18.561491,18.619415,18.592276,19.124788,18.97851,18.402294,18.232588,18.635223,19.072641,0.16681916,0.9487454,1.058161,1.0890033,1.1139207,-2.7443492,-2.5629709,-2.335708,-3.23398,-3.9330497,-3.8953412,-2.4646752,-4.0410466,-3.9452183,-3.523879,-2.9607086,-3.9464943,-3.9252722,0.061533988,1.2944354,-4.3306828,-6.367975,-2.5338106,-1.66875,1.6505204,1.3181206,1.5662677,-6.5514235,0.5659983,1.1397052,1.1902504,-1.9256976,-4.4567933,-4.100229,1.2342039,-6.160302,-5.939016,-6.1722097,1.4057139,-3.501975,-6.1041493,1.2309405,-6.1417155,0.8800491,-5.995013,-6.137062,-6.1372523,-6.502335,-6.1908255,-4.4317617,-5.8731804,0.20356761,1.043083,-1.2838309,-0.9659416,-3.307484,-3.0665896,-2.427814,-2.6872873,1.2954346,-0.6241864,-4.6619287,-4.0915375,2.3293874,0.79634184,-5.4619937,-3.8037012,-1.5050799,0.32716525,-1.2905574,17.951124,18.174969,18.50371,16.93466,18.28458,17.640043,18.62647,18.587385,18.425217,18.18396,18.194946,18.322021,17.70247,17.531319,17.87893,18.479837,17.785143,17.734879,17.544703,18.705006,18.036959,17.497475,17.968216,17.738224,16.635365,17.87138,18.805004,18.339754,18.534235,18.148891,17.598043,18.473217,18.658577,17.667439,17.986277,18.631964,18.01171,17.831436,17.816933,17.401812,18.132917,18.258133,18.11981,18.667934,19.111399,18.866463,18.59814,17.695255,17.980732,18.356394,18.93922,18.467567,18.429758,18.596909,18.918564,-3.7991095,1.4631592,-2.1579268,-5.099575,-6.072408,-5.689812,-4.5053735,-4.934259,-7.7955456,-3.0635674,-3.7039845,-5.665599,-4.923196,-4.1346636,-5.261336,-4.795493,-2.8753314,-2.6458473,-3.1590662,-2.847966,-3.3694417,-3.4815383,-2.5790327,-2.5403652,-2.9508214,-2.7753785,-2.9564097,-2.4366217,-3.4214153,-3.8675156,-3.4493582,-3.5806828,-3.631141,-2.0719426,-4.7663364,-5.59401,-6.8220983,-7.864865,-5.9829116,-5.9637237,-6.1490397,-5.456657,-5.2278886,-6.2659264,-4.0162225,-4.1191134,1.6454349,-0.058543477,-3.8497527,-2.4212427,1.3828083,1.5459733,0.22795424,1.6103513,0.53885126,-5.339081,-1.5102094,-4.8748116,-4.2101226,-4.6373286,-4.2153077,-4.932227,1.4774854,1.3946859,0.2897414,1.715381,1.67715,1.591503,-3.098926,-5.0951934,-6.517261,-4.54149,1.4306602,1.7708104,-6.30642,1.5987418,1.3259059,1.3984498,1.2801067,0.92206275,-6.675264,0.9887682,-3.938614,-4.8987474,0.8654513,1.1907347,1.4335834,1.5898845,-2.3788745,-3.425371,-5.618222,-2.7386625,1.1697958,1.1425025,1.0992842,-3.6649513,-6.4701953,-5.7344174,-6.6634398,-6.653415,-4.861216,-4.63142,-10.893822,-10.786858,-10.426803,1.7620912,-6.6187935,1.6755987,-2.882971,-2.5842712,-2.681039,-2.7029133,-2.6976974,-4.0237784,-3.1879015,-3.276125,-2.9401174,-2.847,-3.437916,-3.0065851,-3.7055407,-5.912405,-5.477438,-2.9108572,-2.6156294,-3.064296,-2.8013277,-2.4937654,-5.266742,-3.2424412,-3.0134463,-1.9399306,-3.1940713,-3.476068],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Git over SSH\\n\\nYou can access and write data in repositories on huggingface.co using SSH (Secure Shel...\"],[\"```\\n$ ssh-add ~\\u002f.ssh\\u002fid_ed25519\\n```\\n\\nIf you chose a different location than the default to store you...\"],[\"Using Flair at Hugging Face\\n\\n[Flair](https:\\u002f\\u002fgithub.com\\u002fflairNLP\\u002fflair) is a very simple framework f...\"],[\"```\\n\\nIt outputs the following:\\n\\n```text\\nSentence[6]: \\\"George Washington ging nach Washington.\\\" ‚Üí [\\\"G...\"],[\"Widget Examples\\n\\nNote that each widget example can also optionally describe the corresponding model ...\"],[\"```\\n\\n### Summarization\\n\\n```yaml\\nwidget:\\n- text: \\\"The tower is 324 metres (1,063 ft) tall, about the ...\"],[\"```\\n\\n### Text Generation\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Julien and I like to\\\"\\n  example_title:...\"],[\"```\\n\\n### Feature Extraction\\n\\n```yaml\\nwidget:\\n- text: \\\"My name is Sylvain and I live in Paris\\\"\\n  exam...\"],[\"```\\n\\n### Voice Activity Detection\\n\\n```yaml\\nwidget:\\n- src: https:\\u002f\\u002fcdn-media.huggingface.co\\u002fspeech_sa...\"],[\"```\\n\\n### Text-to-Image\\n\\n```yaml\\nwidget:\\n- text: \\\"A cat playing with a ball\\\"\\n  example_title: \\\"Cat\\\"\\n-...\"],[\"```\\n\\n## Other\\n\\n### Structured Data Classification\\n\\n```yaml\\nwidget:\\n- structured_data:\\n    fixed_acid...\"],[\"Configure the Dataset Viewer\\n\\nThe Dataset Viewer supports many [data files formats](.\\u002fdatasets-addin...\"],[\"Adding a Sign-In with HF button to your Space\\n\\nYou can enable a built-in sign-in flow in your Space ...\"],[\"```\\n\\nYou can check out the [configuration reference docs](.\\u002fspaces-config-reference) for more inform...\"],[\"Those scopes are optional and can be added by setting `hf_oauth_scopes` in your Space's metadata:\\n\\n-...\"],[\"Basically, you need to:\\n\\n- Redirect the user to `https:\\u002f\\u002fhuggingface.co\\u002foauth\\u002fauthorize?redirect_uri...\"],[\"User access tokens\\n\\n## What are User Access Tokens?\\n\\nUser Access Tokens are the preferred way to aut...\"],[\"If you are a member of an organization with read\\u002fwrite\\u002fadmin role, then your User Access Tokens will...\"],[\"## How to use User Access Tokens?\\n\\nThere are plenty of ways to use a User Access Token to access the...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nTry not to leak your token! Though you can always rotate it, anyone will b...\"],[\"ZenML on Spaces\\n\\n[ZenML](https:\\u002f\\u002fgithub.com\\u002fzenml-io\\u002fzenml) is an extensible, open-source MLOps fram...\"],[\"Visit [the ZenML documentation](https:\\u002f\\u002fdocs.zenml.io\\u002f) to learn more about its\\nfeatures and how to ...\"],[\"![Choose the ZenML Docker template](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"Once you have your ZenML server up and running, you can connect to it from your\\nlocal machine. To do...\"],[\"```\\n\\nYou can also use the Direct URL in your browser to use the ZenML dashboard as a\\nfullscreen appl...\"],[\"\\u003cTip warning={true}\\u003e\\nIf you wish to use a cloud secrets backend together with ZenML for secrets\\nmana...\"],[\"## ü§ó Feedback and support\\n\\nIf you are having trouble with your ZenML server on HuggingFace Spaces, y...\"],[\"Using Spaces for Organization Cards\\n\\nOrganization cards are a way to describe your organization to o...\"],[\"Repository Settings \\n\\n## Private repositories\\n\\nYou can choose a repository's visibility when you cre...\"],[\"If these are use cases you need help with, please send us an email at **website at huggingface.co**....\"],[\"--\\n# Example metadata to be added to a dataset card.  \\n# Full dataset card template at https:\\u002f\\u002fgithu...\"],[\"- {bcp47_lang_1}  # Example: en-US\\npretty_name: {pretty_name}  # Example: SQuAD\\nsize_categories:\\n- {...\"],[\"# Optional. This part can be used to store the feature types and size of the dataset to be used in p...\"],[\"```\\n\\n# Optional. If you want your dataset to be protected behind a gate that users have to accept to...\"],[\"Valid license identifiers can be found in [our docs](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002frepositories-li...\"],[\"Repository limitations and recommendations\\n\\nThere are some limitations to be aware of when dealing w...\"],[\"Under the hood, the Hub uses Git to version the data, which has structural implications on what you ...\"],[\"- **Repository size**: The total size of the data you're planning to upload. There is no hard limit ...\"],[\"- **Number of commits**: There is no hard limit for the total number of commits on your repo history...\"],[\"Dask\\n\\n[Dask](https:\\u002f\\u002fgithub.com\\u002fdask\\u002fdask) is a parallel and distributed computing library that scal...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Dask dataset in Parquet...\"],[\"Access control in organizations\\n\\n\\u003cTip\\u003e\\n\\nYou can set up [Single Sign-On (SSO)](.\\u002fsecurity-sso) to be ...\"],[\"Billing\\n\\nAt Hugging Face, we build a collaboration platform for the ML community (i.e., the Hub), an...\"],[\"Any feedback or support request related to billing is welcome at billing@huggingface.co.\\n\\n## Invoici...\"],[\"Streamlit Spaces\\n\\n**Streamlit** gives users freedom to build a full-featured web app with Python in ...\"],[\"```\\n\\nYou can edit the `sdk_version`, but note that issues may occur when you use an unsupported Stre...\"],[\"## Add the dependencies\\n\\nFor the **Hot Dog Classifier** we'll be using a [ü§ó Transformers pipeline](h...\"],[\"```\\ntransformers\\ntorch\\n```\\n\\nThe Spaces runtime will handle installing the dependencies!\\n\\n## Create t...\"],[\"```\\n\\nThis Python script uses a [ü§ó Transformers pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fpi...\"],[\"```\\n\\n\\u003c!-- The height of this iframe has been calculated as 236 + 64 * 2. 236 is the inner content he...\"],[\"- `id` is set to `\\u003ciframe \\u002f\\u003e` that is used to specify the auto-resize target.\\n- The `iFrame Resizer`...\"],[\"```\\n\\nAdditionally, you can checkout [our documentation](.\\u002fspaces-embed)....\"],[\"Next Steps\\n\\nThese next sections highlight features and additional information that you may find usef...\"],[\"To learn about Git branching, you can try out the [Learn Git Branching interactive tutorial](https:\\u002f...\"],[\"**Note that you will need to [install Git LFS](https:\\u002f\\u002fgit-lfs.github.com\\u002f) and the [`huggingface_hu...\"],[\"```\\ngit clone git@hf.co:me\\u002fmyfork\\n```\\n\\n3. Fetch non-LFS files:\\n\\n```\\ncd myfork\\ngit lfs install --skip...\"],[\"Run with Docker\\n\\nYou can use Docker to run most Spaces locally.\\nTo view instructions to download and...\"],[\"Advanced Topics\\n\\n## Contents\\n\\n- [Using OpenCV in Spaces](.\\u002fspaces-using-opencv)\\n- [More ways to crea...\"],[\"Using spaCy at Hugging Face\\n\\n`spaCy` is a popular library for advanced Natural Language Processing u...\"],[\"```\\n\\nTo find the link of interest, you can go to a repository with a `spaCy` model. When you open th...\"],[\"```\\n\\nYou can then check if the command has been registered successfully\\n\\n```bash\\npython -m spacy hug...\"],[\"```\\n\\n| Argument             | Type         | Description                                            ...\"],[\"```bash\\nhuggingface-cli login\\npython -m spacy package .\\u002fen_ner_fashion .\\u002foutput --build wheel\\ncd .\\u002fo...\"],[\"```\\n\\nIn just a minute, you can get your packaged model in the Hub, try it out directly in the browse...\"],[\"Audit Logs\\n\\n\\u003cTip warning={true}\\u003e\\nThis feature is part of the \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fenterpr...\"],[\"Spaces Overview\\n\\nHugging Face Spaces make it easy for you to create and deploy ML-powered demos in m...\"],[\"Under the hood, Spaces stores your code inside a git repository, just like the model and dataset rep...\"],[\"| **Hardware**        \\t| **GPU Memory** \\t| **CPU** \\t| **Memory** \\t| **Disk** \\t| **Hourly Price** \\t|\\n...\"],[\"| **Storage tier**     \\t| **Size**             \\t| **Persistent** \\t| **Monthly price** \\t|\\n|----------...\"],[\"Note: Find more detailed and comprehensive pricing information on [our pricing page](https:\\u002f\\u002fhugging...\"],[\"Variables are publicly accessible and viewable and will be automatically added to Spaces duplicated ...\"],[\"Some Spaces might have environment variables that you may need to set up. In these cases, the duplic...\"],[\"In case [OAuth](.\\u002fspaces-oauth) is enabled for your Space, the following variables will also be avai...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\ntitle: My lovely space\\nemoji: ü§ó\\ncolorFrom: blue\\ncolorTo: green\\nsdk: docker\\npinned: false\\nmodels:...\"],[\"Search\\n\\nYou can now easily search anything on the Hub with **Full-text search**. We index model card...\"],[\"## Filter with ease\\n\\nBy default, models, datasets, & spaces are being searched when a user enters a ...\"],[\"[paddlenlp-banner](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fhub...\"],[\"4. Easily deploy your model as a Gradio app on Spaces.\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"...\"],[\"```\\npip install -U paddlenlp\\n```\\n\\n## Using existing models\\n\\nSimilar to `transformer` models, the `pa...\"],[\"```\\n\\nIf you want to see how to load a specific model, you can click `Use in paddlenlp` and you will ...\"],[\"Reference\\n\\n## Deep Learning Container\\n\\nBelow you can find a version table of currently available Hug...\"],[\"**Example 1: PyTorch Training:**\\n`763104351884.dkr.ecr.us-west-2.amazonaws.com\\u002fhuggingface-pytorch-t...\"],[\"| ü§ó Transformers version | ü§ó Datasets version | PyTorch\\u002fTensorFlow version | type     | device | Pyt...\"],[\"| 4.10.2                  | 1.11.0              | TensorFlow 2.4.1           | training | GPU    | 3...\"],[\"## Inference DLC Overview\\n\\nThe Inference DLC overview includes all released and available Hugging Fa...\"],[\"| ü§ó Transformers version | PyTorch\\u002fTensorFlow version | type      | device | Python Version |\\n| ----...\"],[\"| 4.11.0                  | PyTorch 1.9.0              | inference | GPU    | 3.8            |\\n| 4.1...\"],[\"## Hugging Face Transformers Amazon SageMaker Examples\\n\\nExample Jupyter notebooks that demonstrate h...\"],[\"| Notebook                                                                                          ...\"],[\"| [02 getting started with TensorFlow](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsagemaker\\u002f...\"],[\"| [07 Distributed Training: Data Parallelism](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsag...\"],[\"| [12 Batch Processing with Amazon SageMaker Batch Transform](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnoteboo...\"],[\"| [18 AWS Inferentia](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fnotebooks\\u002fblob\\u002fmain\\u002fsagemaker\\u002f18_inferentia_inf...\"],[\"## Inference Toolkit API\\n\\nThe Inference Toolkit accepts inputs in the `inputs` key, and supports add...\"],[\"```\\n\\n**`sentiment-analysis`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Don't waste your time.  We had two different p...\"],[\"```\\n\\n**`parameterized-request`**\\n\\n```json\\n{\\n  \\\"inputs\\\": \\\"Hugging Face, the winner of VentureBeat‚Äôs I...\"],[\"```\\n\\n**`HF_API_TOKEN`**\\n\\n`HF_API_TOKEN` defines your Hugging Face authorization token. The `HF_API_T...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fgithub.com\\u002fpandas-dev\\u002fpandas) is a widely used Python data analysis toolkit...\"],[\"```\\n\\nThis creates a dataset repository `username\\u002fmy_dataset` containing your Pandas dataset in Parqu...\"]],\"hovertemplate\":\"source=hub-docs\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"hub-docs, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"hub-docs, circle\",\"showlegend\":true,\"x\":[2.6360724,2.2650077,-5.808315,-6.092656,-6.1585956,-7.8547482,-7.0535555,-7.180253,-6.701143,-7.452045,-3.963014,0.031244317,1.7239612,1.4052644,1.6785407,1.6703105,1.5533903,1.725942,1.6854141,1.6009679,1.3415692,1.110027,1.4658259,1.2798073,1.3171945,1.5523843,1.2737027,1.5867023,2.5310845,-7.005897,-5.4166665,-5.10213,-0.58682567,-0.5211542,1.1099743,3.3359818,3.0362258,3.0937088,3.2545266,1.7487909,1.3822621,1.7601942,0.4830959,0.69370985,1.4295672,1.3739575,-3.11989,0.5891257,1.0119615,3.0566578,3.0819237,2.8847961,2.9802694,3.0727966,3.117496,3.4269311,1.2206327,1.7175975,-4.487233,1.0598583,2.744915,3.1348379,2.2471638,1.4442674,1.5442771,1.2351604,1.6871926,-0.70963067,-0.43978336,1.358019,1.3372847,1.5761348,1.4229313,0.86507654,3.267714,0.8439614,0.7213641,-2.2361565,0.9750489,-1.100889,0.75859964,-1.7010394,-2.405563,-0.78406435,-1.831666,-2.1772947,-1.1371793,-1.6407522,-1.6034685,-9.016687,-2.3673875,-1.9313489,-1.7083431,-1.9589787,-3.458449,-6.3277946,-3.1764388,1.8954856,1.8072753,1.4186869],\"xaxis\":\"x\",\"y\":[-3.9484706,-3.9246106,-2.3256686,-2.5828764,-3.5895507,-3.0450313,-3.2548695,-3.529065,-3.3219707,-2.7613652,-7.802151,-6.94898,-3.0490952,-2.9100907,-2.994338,-2.6840763,-3.1493855,-3.131326,-3.8499303,-2.867681,-2.777556,-2.702415,-2.9457474,-2.8363535,-2.8713064,-3.0155256,-2.9248993,-2.4655962,-3.8291905,1.1409945,-1.2524656,-2.3393404,-7.180583,-6.8165865,-0.39980954,-4.9534616,-4.772762,-5.0449066,-4.889934,-5.508371,-5.8032026,-3.0570464,-2.008396,-1.7822382,-2.6221464,-2.5566192,-0.35431126,-2.6028578,-2.9819252,-1.7362562,-2.251114,-1.532062,-4.3820515,-4.2498617,-4.4135714,-4.4713917,-2.9237604,-1.9269824,-1.1812767,-4.2300243,-4.1664133,-4.891089,-4.124258,-4.5455475,-2.7849405,-2.5364368,-3.0691054,-2.1159058,-2.075385,-2.833119,-2.707127,-2.8093376,-2.903249,-3.9364014,-1.7222986,-3.8709884,-3.0954118,-1.2094959,-3.0972486,-3.2528968,-3.8685815,-0.8320922,-0.58407086,-2.148777,-2.9410696,-0.3747303,-2.1311674,-2.8298354,-1.1921358,5.4289207,-1.0414642,-1.2586976,-1.1620004,-1.9280596,-0.31697726,-3.4765582,-0.814184,-3.569925,-5.734674,-5.767461],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ...\"],[\"```...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SE ResNet\\n  Paper:\\n    Title: Squeeze-and-Excitation Net...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fseresnet15...\"],[\"Crop Pct: '0.875'\\n    Momentum: 0.9\\n    Batch Size: 1024\\n    Image Size: '224'\\n    Interpolation: bi...\"],[\"Res2Net\\n\\n**Res2Net** is an image model that employs a variation on bottleneck residual blocks, [Res2...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `res2net101_26w_4s`. You can find...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2Net\\n  Paper:\\n    Title: 'Res2Net: A New Multi-scale ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net101...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-res2net\\u002fres2net50_...\"],[\"# Ensemble Adversarial Inception ResNet v2\\n\\n**Inception-ResNet-v2** is a convolutional neural archit...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `ens_adv_inception_resnet_v2`. Yo...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Ensemble Adversarial\\n  Paper:\\n    Title: Adversaria...\"],[\"TResNet\\n\\nA **TResNet** is a variant on a [ResNet](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnet) that aim...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TResNet\\n  Paper:\\n    Title: 'TResNet: High Performance G...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-tresnet\\u002ftresnet_l_...\"],[\"Momentum: 0.9\\n    Image Size: '448'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: htt...\"],[\"Training Resources: 8x NVIDIA 100 GPUs\\n    Training Time: \\u003c 24 hours\\n    ID: tresnet_m\\n    LR: 0.01\\n...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - AutoAugment\\n    - Cutout\\n    - Labe...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - InPlace-ABN\\n    - Leaky ReLU\\n    - ReLU\\n    - Resid...\"],[\"FLOPs: 60641712730\\n    Parameters: 75646610\\n    File Size: 224440219\\n    Architecture:\\n    - 1x1 Con...\"],[\"SE-ResNet\\n\\n**SE ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `seresnet152d`. You can find the ...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `cspresnext50`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: CSP ResNeXt\\n  Paper:\\n    Title: 'CSPNet: A New Back...\"],[\"RexNet\\n\\n**Rank Expansion Networks** (ReXNets) follow a set of new design principles for designing bo...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RexNet\\n  Paper:\\n    Title: 'ReXNet: Diminishing Represen...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_10...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_13...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_15...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-rexnet\\u002frexnetv1_20...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo get the model predictions:\\n```python\\nimport torch\\nwith torch.no_grad():\\n    out = model(tens...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: RegNetY\\n  Paper:\\n    Title: Designing Network Design Spa...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 70....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 77....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 82....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 79....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 80....\"],[\"Feature Extraction\\n\\nAll of the models in `timm` have consistent mechanisms for obtaining various typ...\"],[\"```\\nOutput:\\n```text\\nUnpooled shape: torch.Size([2, 2048, 7, 7])\\n```\\n\\n#### Remove it later\\n```python ...\"],[\"```\\nOutput:\\n```text\\nOriginal shape: torch.Size([2, 1000])\\nPooled shape: torch.Size([2, 1024])\\n```\\n\\n\\n...\"],[\"```\\n\\n### Query the feature information\\n\\nAfter a feature backbone has been created, it can be queried...\"],[\"```\\n\\n### Select specific feature levels or limit the stride\\n\\nThere are two additional creation argum...\"],[\"Wide ResNet\\n\\n**Wide Residual Networks** are a variant on [ResNets](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `wide_resnet101_2`. You can find ...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Wide ResNet\\n  Paper:\\n    Title: Wide Residual Networks\\n ...\"],[\"Top 5 Accuracy: 94.28%\\n- Name: wide_resnet50_2\\n  In Collection: Wide ResNet\\n  Metadata:\\n    FLOPs: 1...\"],[\"Res2NeXt\\n\\n**Res2NeXt** is an image model that employs a variation on [ResNeXt](https:\\u002f\\u002fpaperswithcod...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Res2NeXt\\n  Paper:\\n    Title: 'Res2Net: A New Multi-...\"],[\"RegNetY\\n\\n**RegNetY** is a convolutional network design space with simple, regular models with parame...\"],[\"```\\n\\nTo load and preprocess the image:\\n\\n```py\\n\\u003e\\u003e\\u003e import urllib\\n\\u003e\\u003e\\u003e from PIL import Image\\n\\u003e\\u003e\\u003e from t...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n\\n```py\\n\\u003e\\u003e\\u003e # Get imagenet class mappings\\n\\u003e\\u003e\\u003e url, fil...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `regnety_002`. You can find the I...\"],[\"Archived Changes\\n\\n### Nov 22, 2021\\n* A number of updated weights anew new model defs\\n  * `eca_halone...\"],[\"### Oct 19, 2021\\n* ResNet strikes back (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2110.00476) weights added, plus any ex...\"],[\"* ConvMixer (https:\\u002f\\u002fopenreview.net\\u002fforum?id=TVHS5Y4dNvM), CrossVit (https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2103.1489...\"],[\"### Aug 18, 2021\\n* Optimizer bonanza!\\n  * Add LAMB and LARS optimizers, incl trust ratio clipping op...\"],[\"### July 5-9, 2021\\n* Add `efficientnetv2_rw_t` weights, a custom 'tiny' 13.6M param variant that is ...\"],[\"### June 20, 2021\\n* Release Vision Transformer 'AugReg' weights from [How to train your ViT? Data, A...\"],[\"* Add `eca_nfnet_l2` weights from my 'lightweight' series. 84.7 top-1 at 384x384.\\n* Add distilled Bi...\"],[\"### June 8, 2021\\n* Add first ResMLP weights, trained in PyTorch XLA on TPU-VM w\\u002f my XLA branch. 24 b...\"],[\"### May 5, 2021\\n* Add MLP-Mixer models and port pretrained weights from [Google JAX impl](https:\\u002f\\u002fgi...\"],[\"### April 13, 2021\\n* Add Swin Transformer models and weights from https:\\u002f\\u002fgithub.com\\u002fmicrosoft\\u002fSwin-...\"],[\"### April 1, 2021\\n* Add snazzy `benchmark.py` script for bulk `timm` model benchmarking of train and...\"],[\"### Feb 18, 2021\\n* Add pretrained weights and model variants for NFNet-F* models from [DeepMind Haik...\"],[\"### Feb 16, 2021\\n* Add Adaptive Gradient Clipping (AGC) as per https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2102.06171. Int...\"],[\"### Feb 8, 2021\\n* Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320...\"],[\"### Jan 25, 2021\\n* Add ResNetV2 Big Transfer (BiT) models w\\u002f ImageNet-1k and 21k weights from https:...\"],[\"### Dec 18, 2020\\n* Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256\\n  * 256x2...\"],[\"### Oct 21, 2020\\n* Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79...\"],[\"### Aug 12, 2020\\n* New\\u002fupdated weights from training experiments\\n  * EfficientNet-B3 - 82.1 top-1 (v...\"],[\"### Aug 5, 2020\\nUniversal feature extraction, new models, new weights, new test sets.\\n* All models s...\"],[\"### June 11, 2020\\nBunch of changes:\\n* DenseNet models updated with memory efficient addition from to...\"],[\"### May 1, 2020\\n* Merged a number of execellent contributions in the ResNet model family over the pa...\"],[\"### April 5, 2020\\n* Add some newly trained MobileNet-V2 models trained with latest h-params, rand au...\"],[\"### Feb 18, 2020\\n* Big refactor of model layers and addition of several attention mechanisms. Severa...\"],[\"### Feb 1\\u002f2, 2020\\n* Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 ...\"],[\"### Dec 28, 2019\\n* Add new model weights and training hparams (see Training Hparams section)\\n  * `ef...\"],[\"### Nov 29, 2019\\n* Brought EfficientNet and MobileNetV3 up to date with my https:\\u002f\\u002fgithub.com\\u002frwight...\"],[\"(Tensorflow) EfficientNet CondConv\\n\\n**EfficientNet** is a convolutional neural network architecture ...\"],[\"```\\n\\nTo load and preprocess the image:\\n```python \\nimport urllib\\nfrom PIL import Image\\nfrom timm.data...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_efficientnet_cc_b0_4e`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF EfficientNet CondConv\\n  Paper:\\n    Title: 'CondConv: ...\"],[\"Interpolation: bicubic\\n    RMSProp Decay: 0.9\\n    Label Smoothing: 0.1\\n    BatchNorm Momentum: 0.99\\n...\"],[\"- Weight Decay\\n    Training Data:\\n    - ImageNet\\n    ID: tf_efficientnet_cc_b0_8e\\n    LR: 0.256\\n    ...\"],[\"- CondConv\\n    - Convolution\\n    - Dense Connections\\n    - Dropout\\n    - Inverted Residual Block\\n   ...\"],[\"SWSL ResNeXt\\n\\nA **ResNeXt** repeats a [building block](https:\\u002f\\u002fpaperswithcode.com\\u002fmethod\\u002fresnext-blo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `swsl_resnext101_32x16d`. You can...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SWSL ResNext\\n  Paper:\\n    Title: Billion-scale semi-supe...\"],[\"Weights: https:\\u002f\\u002fdl.fbaipublicfiles.com\\u002fsemiweaksupervision\\u002fmodel_files\\u002fsemi_weakly_supervised_resne...\"],[\"Batch Size: 1536\\n    Image Size: '224'\\n    Weight Decay: 0.0001\\n    Interpolation: bilinear\\n  Code: ...\"],[\"ID: swsl_resnext101_32x8d\\n    LR: 0.0015\\n    Epochs: 30\\n    Layers: 101\\n    Crop Pct: '0.875'\\n    Ba...\"],[\"Training Techniques:\\n    - SGD with Momentum\\n    - Weight Decay\\n    Training Data:\\n    - IG-1B-Targe...\"],[\"Model Summaries\\n\\nThe model architectures included come from a wide variety of sources. Sources, incl...\"],[\"## DenseNet [[densenet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## HRNet [[hrnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fhrnet...\"],[\"## Inception-ResNet-V2 [[inception_resnet_v2.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fb...\"],[\"## EfficientNet [[efficientnet.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftim...\"],[\"## MobileNet-V3 [[mobilenetv3.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm...\"],[\"* ResNet (V1B)\\n  * Paper: `Deep Residual Learning for Image Recognition` - https:\\u002f\\u002farxiv.org\\u002fabs\\u002f151...\"],[\"* Squeeze-and-Excitation Networks\\n  * Paper: `Squeeze-and-Excitation Networks` - https:\\u002f\\u002farxiv.org\\u002fa...\"],[\"## Res2Net [[res2net.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fr...\"],[\"## SelecSLS [[selecsls.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels...\"],[\"## VGG [[vgg.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002fblob\\u002fmaster\\u002ftimm\\u002fmodels\\u002fvgg.py)]\\n...\"],[\"## Xception (Modified Aligned, Gluon) [[gluon_xception.py](https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-imag...\"],[\"(Tensorflow) MobileNet v3\\n\\n**MobileNetV3** is a convolutional neural network that is designed for mo...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_mobilenetv3_large_075`. You c...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF MobileNet V3\\n  Paper:\\n    Title: Searching for Mobile...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '224'\\n    Weight Decay: 1.0e-05\\n    Interpolation...\"],[\"Training Techniques:\\n    - RMSProp\\n    - Weight Decay\\n    Training Data:\\n    - ImageNet\\n    Training...\"],[\"- Depthwise Separable Convolution\\n    - Dropout\\n    - Global Average Pooling\\n    - Hard Swish\\n    - ...\"],[\"In Collection: TF MobileNet V3\\n  Metadata:\\n    FLOPs: 48457664\\n    Parameters: 2040000\\n    File Size...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 65....\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002ftf_mobilen...\"],[\"Image Size: '224'\\n    Weight Decay: 4.0e-05\\n    Interpolation: bilinear\\n    RMSProp Decay: 0.9\\n  Cod...\"],[\"Sharing and Loading Models From the Hugging Face Hub\\n\\nThe `timm` library has a built-in integration ...\"],[\"```\\n\\nRunning the above would push the model to `\\u003cyour-username\\u003e\\u002fresnet18-random` on the Hub. You can...\"],[\"Big Transfer (BiT)\\n\\n**Big Transfer (BiT)** is a type of pretraining recipe that pre-trains  on a lar...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `resnetv2_101x1_bitm`. You can fi...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Big Transfer\\n  Paper:\\n    Title: 'Big Transfer (BiT): Ge...\"],[\"Momentum: 0.9\\n    Batch Size: 4096\\n    Image Size: '480'\\n    Weight Decay: 0.0001\\n    Interpolation:...\"],[\"Training Resources: Cloud TPUv3-512\\n    ID: resnetv2_101x3_bitm\\n    LR: 0.03\\n    Epochs: 90\\n    Laye...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n    - SGD with Momentum\\n    -...\"],[\"Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n    - SGD with Momentum\\n    -...\"],[\"- Weight Standardization\\n    Tasks:\\n    - Image Classification\\n    Training Techniques:\\n    - Mixup\\n...\"],[\"- Convolution\\n    - Global Average Pooling\\n    - Group Normalization\\n    - Max Pooling\\n    - ReLU\\n  ...\"],[\"MobileNet v2\\n\\n**MobileNetV2** is a convolutional neural network architecture that seeks to perform w...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mobilenetv2_100`. You can find t...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MobileNet V2\\n  Paper:\\n    Title: 'MobileNetV2: Inverted ...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmobilenetv...\"],[\"EfficientNet (Knapsack Pruned)\\n\\n**EfficientNet** is a convolutional neural network architecture and ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `efficientnet_b1_pruned`. You can...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: EfficientNet Pruned\\n  Paper:\\n    Title: Knapsack Pruning...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 78....\"],[\"Top 1 Accuracy: 79.91%\\n      Top 5 Accuracy: 94.86%\\n- Name: efficientnet_b3_pruned\\n  In Collection: ...\"],[\"(Tensorflow) Inception v3\\n\\n**Inception v3** is a convolutional neural network architecture from the ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `tf_inception_v3`. You can find t...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: TF Inception v3\\n  Paper:\\n    Title: Rethinking the ...\"],[\"DenseNet\\n\\n**DenseNet** is a type of convolutional neural network that utilises dense connections bet...\"],[\"```\\n\\nTo get the model predictions:\\n\\n```py\\n\\u003e\\u003e\\u003e import torch\\n\\u003e\\u003e\\u003e with torch.no_grad():\\n...     out = m...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `densenet121`. You can find the I...\"],[\"```\\n\\n```\\n@misc{rw2019timm,\\n  author = {Ross Wightman},\\n  title = {PyTorch Image Models},\\n  year = {2...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: DenseNet\\n  Paper:\\n    Title: Densely Connected Convoluti...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fdensenet12...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet161-8d451a50.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet169-b2777c0a.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Weights: https:\\u002f\\u002fdownload.pytorch.org\\u002fmodels\\u002fdensenet201-c1103571.pth\\n  Results:\\n  - Task: Image Cla...\"],[\"Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76.59%\\n      Top 5 Accuracy: 93.2%\\n- Name: tv_d...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 74....\"],[\"Training Examples\\n\\n## EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5\\nThese params are for...\"],[\"## SE-ResNeXt-26-D and SE-ResNeXt-26-T\\nThese hparams (or similar) work well for a wide range of ResN...\"],[\"`.\\u002fdistributed_train.sh 2 \\u002fimagenet\\u002f --model efficientnet_b0 -b 384 --sched step --epochs 450 --deca...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay...\"],[\"`.\\u002fdistributed_train.sh 8 \\u002fimagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 ...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: MNASNet\\n  Paper:\\n    Title: 'MnasNet: Platform-Aware Neu...\"],[\"Weights: https:\\u002f\\u002fgithub.com\\u002frwightman\\u002fpytorch-image-models\\u002freleases\\u002fdownload\\u002fv0.1-weights\\u002fmnasnet_b1...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 75....\"],[\"SK-ResNet\\n\\n**SK ResNet** is a variant of a [ResNet](https:\\u002f\\u002fwww.paperswithcode.com\\u002fmethod\\u002fresnet) th...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"\\u003c!--\\nType: model-index\\nCollections:\\n- Name: SKResNet\\n  Paper:\\n    Title: Selective Kernel Networks\\n ...\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 73....\"],[\"Results:\\n  - Task: Image Classification\\n    Dataset: ImageNet\\n    Metrics:\\n      Top 1 Accuracy: 76....\"],[\"Inception v4\\n\\n**Inception-v4** is a convolutional neural network architecture that builds on previou...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `inception_v4`. You can find the ...\"],[\"```\\n\\n\\u003c!--\\nType: model-index\\nCollections:\\n- Name: Inception v4\\n  Paper:\\n    Title: Inception-v4, Ince...\"],[\"CSP-ResNeXt\\n\\n**CSPResNeXt** is a convolutional neural network where we apply the Cross Stage Partial...\"],[\"```\\n\\nTo get the top-5 predictions class names:\\n```python\\n# Get imagenet class mappings\\nurl, filename...\"],[\"```\\nTo finetune on your own dataset, you have to write a training loop or adapt [timm's training\\nscr...\"],[\"MnasNet\\n\\n**MnasNet** is a type of convolutional neural network optimized for mobile devices that is ...\"],[\"```\\n\\nReplace the model name with the variant you want to use, e.g. `mnasnet_100`. You can find the I...\"]],\"hovertemplate\":\"source=pytorch-image-models\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"pytorch-image-models, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"pytorch-image-models, circle\",\"showlegend\":true,\"x\":[8.39261,-1.9470944,-22.777782,2.954714,-17.71694,-19.35852,-19.180157,8.394333,-22.779408,-17.735106,-19.587425,-19.587177,-19.460186,-19.52474,-19.525967,-19.412563,8.38292,-2.0130975,-22.779991,2.1458387,-17.68,8.387208,-1.4858959,2.1459017,-17.72121,-19.216469,-19.208347,-19.055706,-19.045218,-19.058235,-18.99988,8.393453,-22.77785,8.386634,-22.778997,-17.704985,8.39938,-1.5733538,2.1458983,-17.737274,-19.280895,-19.34109,-19.378948,-19.310202,-16.572834,-1.9274356,-22.780666,-17.624575,-18.084284,-18.082891,-18.083843,-18.082148,-18.082447,-18.085243,-18.082766,-18.083305,-18.084253,-18.08526,-18.083843,-18.487032,-2.9003427,-2.8582618,-2.6819756,-2.6861277,-2.8327384,8.392437,-22.780706,2.1458085,-17.871294,-18.805037,8.39082,-1.4696409,2.1460035,-17.763319,-16.65553,-1.9414623,-1.9861035,-22.777664,-17.475126,-17.435148,0.8560301,-17.315004,-17.545227,-17.463203,-17.425047,-17.415316,-17.457981,-17.41314,-17.391403,-17.249968,-1.8279455,-17.453094,-17.504105,-17.453121,-17.462954,-17.37885,-17.33187,-17.41053,-17.414492,-17.492441,-17.394732,-17.526472,-17.440733,-17.309658,-16.628206,-2.018618,-22.781036,2.1457536,-17.741552,-19.091846,-19.066341,-19.037735,8.392647,-22.780481,2.1458251,-17.974453,-18.75051,-19.040436,-18.81585,-18.915373,-17.070671,-16.655518,-16.852686,-16.813532,-16.76182,-16.72962,-16.78974,-16.871267,-16.730362,-16.68083,-16.599615,-2.0193925,8.382657,-22.780474,2.145758,-17.849655,-19.275982,-18.993353,-18.981802,-18.067072,-18.635246,-19.128292,-19.14633,1.0006087,0.63780034,-1.7855257,-22.781439,-17.743547,-19.132994,-18.870798,-18.967535,-19.078272,-18.950294,-18.998816,8.39194,-22.780691,2.1461549,-17.79015,-19.324013,-19.318476,-19.284718,-19.203905,-16.6022,-22.781366,-17.725328,-18.555004,-18.793715,8.362479,-22.780455,2.1455233,-17.58808,-16.545198,-1.8945948,-22.778694,-1.4757543,-17.635063,-19.33343,-19.2803,-19.18101,-19.154305,-19.118736,-18.393162,-1.6732599,-17.63575,-17.753742,-18.037586,-17.998138,8.39319,-1.6396667,2.1461241,-17.739628,-19.343931,-18.710783,8.3931055,-1.5886546,2.145935,-17.757465,-18.649105,-18.729303,8.344078,-22.777924,-17.503391,8.388405,-1.6597552,2.145696,8.3929825,-22.78001],\"xaxis\":\"x\",\"y\":[-16.642738,-8.69596,-7.936125,-1.290557,7.8975034,8.440568,8.217301,-16.645777,-7.9359775,7.8686533,8.573971,8.603473,8.493186,8.488487,8.559605,8.470285,-16.634329,-8.65301,-7.936601,-22.647697,7.9015727,-16.637955,-9.000393,-22.647043,7.879495,8.288598,8.188175,8.114407,8.11389,8.1339035,8.1653185,-16.64357,-7.9364786,-16.638615,-7.9362054,7.8031654,-16.65138,-9.017407,-22.647007,7.86352,8.383569,8.389199,8.463035,8.370777,6.8865833,-8.4663925,-7.936891,7.8442173,17.136011,17.135118,17.13743,17.136402,17.135809,17.133387,17.136848,17.135185,17.136305,17.134987,17.135748,8.502886,-6.7789483,-6.835297,-6.827571,-7.0070095,-6.8228426,-16.642977,-7.9368477,-22.64864,7.9901867,8.170716,-16.642591,-8.961992,-22.648464,7.9782786,6.8991165,-6.256954,-8.682303,-7.9357147,6.2871566,6.256999,-2.008964,6.0345335,6.2156286,6.2469583,6.3530426,6.1995416,6.229121,6.215898,6.1751223,6.4155936,-2.4706347,6.269993,6.334535,6.249812,6.250593,6.1628256,6.2384477,6.2276497,6.271139,6.17461,6.259196,6.2693954,6.2290807,6.301151,6.805423,-6.4084673,-7.9370008,-22.647245,7.864269,8.149529,8.134627,8.116951,-16.643988,-7.9372506,-22.648441,7.843129,7.8971205,8.1460495,8.013005,7.968564,6.747453,6.9107146,7.061557,6.8963103,6.7784824,6.8649673,6.850238,6.693714,6.8793054,6.874091,6.792393,-6.683783,-16.634378,-7.9371524,-22.647682,7.962802,8.225697,8.078199,8.119324,8.011721,8.307599,8.270542,8.242692,-4.3552513,-4.554022,-5.4746923,-7.9366584,7.755178,8.161737,7.958889,8.100272,8.10073,8.018156,8.020807,-16.644636,-7.936913,-22.646173,7.9065557,8.452844,8.408231,8.38651,8.435115,6.834793,-7.9361362,7.9005904,8.369386,8.31479,-16.611872,-7.9363136,-22.64645,7.7616005,6.9154882,-8.561606,-7.9366965,-6.9012413,7.844128,8.435678,8.447831,8.39766,8.360393,8.263147,8.540184,-3.4500113,6.2465196,6.4874816,6.415678,6.877193,-16.645147,-8.979645,-22.647778,7.883149,8.466892,8.502166,-16.643557,-8.980187,-22.648714,7.898654,8.414668,8.5488205,-16.59578,-7.9360266,7.697732,-16.639122,-8.894475,-22.649109,-16.643494,-7.9365587],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"--\\ntitle: \\\"Large Language Models: A New Moore's Law?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f33_large_language_mode...\"],[\"### Deep Learning, Deep Pockets?\\n\\nAs you would expect, training a 530-billion parameter model on hum...\"],[\"I'm left wondering what's the point of it all. Science for the sake of science? Good old marketing? ...\"],[\"Downsizing efforts are also under way in the Natural Language Processing community, using transfer l...\"],[\"You guessed it, that's another way to do transfer learning, and it'll help you save on everything!\\n ...\"],[\"However, the Machine Learning community is still struggling with this topic, and for good reason. Op...\"],[\"Instead of chasing trillion-parameter models (place your bets), wouldn't all be better off if we bui...\"],[\"--\\ntitle: \\\"Why we‚Äôre switching to Hugging Face Inference Endpoints, and maybe you should too\\\"\\nthumbn...\"],[\"Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served...\"],[\"- Requester region: eu-east-1\\n- Requester instance size: t3-medium\\n- Inference endpoint region: eu-e...\"],[\"```\\nWhat we see from these results is pretty encouraging. The application that will consume these en...\"],[\"```\\n\\nWe can say a couple of things about this. Firstly, we want a managed solution to deployment, we...\"],[\"## Other considerations\\n\\n### Deployment Options\\n\\nCurrently you can deploy an Inference Endpoint from...\"],[\"```\\n\\nFor me, what‚Äôs lacking is a [custom terraform provider](https:\\u002f\\u002fwww.hashicorp.com\\u002fblog\\u002fwriting-...\"],[\"--\\ntitle: \\\"DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub\\\" \\nthumbnail: \\u002fblog\\u002fassets...\"],[\"We are happy to share that we recently added another feature to help you analyze datasets on the Hub...\"],[\"```\\n\\nCreate a connection to DuckDB and install and load the `httpfs` extension to allow reading and ...\"],[\"```\\n\\nTo learn more, check out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets-server\\u002fparque...\"],[\"--\\ntitle: \\\"Building an AI WebTV\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f156_ai_webtv\\u002fthumbnail.gif\\nauthors:\\n- user:...\"],[\"The individual video sequences are purposely made to be short, meaning the WebTV should be seen as a...\"],[\"üëâ¬† You will need to use the same prompt for both the generation and upscaling.\\n\\n## Calling the video...\"],[\"export const generateVideo = async (prompt: string) =\\u003e {\\n  const api = await client(\\\"*** URL OF THE ...\"],[\"```\\n\\n\\n## Post-processing\\n\\nOnce an individual take (a video clip) is upscaled, it is then passed to F...\"],[\"let playlist = 'ffconcat version 1.0\\\\n'\\nallFilePaths.forEach(filePath =\\u003e {\\n  playlist += `file '${fi...\"],[\"```\\n\\nThis will generate the following playlist content:\\n\\n```bash\\nffconcat version 1.0\\nfile 'video1.m...\"],[\"```\\n\\nThere are many different configuration options for FFmpeg, for more information in the [officia...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo5....\"],[\"We've seen it with large language models and their ability to synthesize convincing content that mim...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo18...\"],[\"\\u003cfigure class=\\\"image flex flex-col items-center text-center m-0 w-full\\\"\\u003e\\n   \\u003cvideo\\n      alt=\\\"demo2....\"],[\"**Wrong direction:** the model sometimes has trouble with movement and direction. For instance, here...\"],[\"**Text or objects inserted into the image:** the model sometimes injects words from the prompt into ...\"],[\"## Maintaining consistency between scenes\\n\\nIf you plan to create sequences of multiple videos, you w...\"],[\"--\\ntitle: Fine tuning CLIP with Remote Sensing (Satellite) images and captions\\nthumbnail: \\u002fblog\\u002fasse...\"],[\"Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or...\"],[\"The ability to search through large collections of images using text queries is an immensely powerfu...\"],[\"In addition, we used the [UCM Dataset](https:\\u002f\\u002fmega.nz\\u002ffolder\\u002fwCpSzSoS#RXzIlrv--TDt3ENZdKN8JA) and t...\"],[\"#### Data Augmentation\\n\\nIn order to regularize our dataset and prevent overfitting due to the size o...\"],[\"### Evaluation\\n\\n#### Metrics\\n\\nA subset of the RSICD test set was used for evaluation. We found 30 ca...\"],[\"| Model-name                               | k=1   | k=3   | k=5   | k=10  |\\n| ---------------------...\"],[\"| bs128x8-lr5e-5-imgaugs-textaugs-3\\u002fckpt-5 | 0.823 | 0.946 | 0.971 | 0.992 |\\n| bs128x8-lr5e-5-wd02\\u002fc...\"],[\"_1 - our best model, 2 - our second best model_\\n\\n\\n#### Demo\\n\\nYou can access the [CLIP-RSICD Demo](ht...\"],[\"--\\ntitle: \\\"Multivariate Probabilistic Time Series Forecasting with Informer\\\" \\nthumbnail: \\u002fblog\\u002fasset...\"],[\"## Introduction\\n\\nA few months ago we introduced the [Time Series Transformer](https:\\u002f\\u002fhuggingface.co...\"],[\"## Informer - Under The Hood\\n\\nBased on the vanilla Transformer ([Vaswani et al., 2017](https:\\u002f\\u002farxiv...\"],[\"### ProbSparse Attention\\n\\nThe main idea of ProbSparse is that the canonical self-attention scores fo...\"],[\"$$\\n\\\\textrm{Attention}(Q, K, V) = \\\\textrm{softmax}(\\\\frac{QK^T}{\\\\sqrt{d_k}} )V\\n$$\\n\\nWhere \\\\\\\\(Q\\\\in \\\\math...\"],[\"This is good! But how can we select the \\\\\\\\(u\\\\\\\\) \\\"active\\\" queries to create \\\\\\\\(Q_{reduce}\\\\\\\\)? Let's d...\"],[\"But how can we calculate the term \\\\\\\\(q_ik_j^T\\\\\\\\) in non-quadratic time? Recall that most of the dot-...\"],[\"# calculate u to find the Top-u queries under the sparsity measurement\\n    u = min(sampling_factor *...\"],[\"```\\nNote that in the implementation, \\\\\\\\(U_{part}\\\\\\\\) contain \\\\\\\\(L_Q\\\\\\\\) in the calculation, for stabil...\"],[\"Let's see this in code:\\n    \\n```python\\nfrom torch import nn\\n\\n# ConvLayer is a class with forward pas...\"],[\"```\\n    \\nBy reducing the input of each layer by two, we get a memory usage of \\\\\\\\(O(N\\\\cdot T \\\\log T)\\\\...\"],[\"```\\n\\n## Load Dataset\\n\\nIn this blog post, we'll use the `traffic_hourly` dataset, which is available ...\"],[\"```\\n\\nEach example contains a few keys, of which `start` and `target` are the most important ones. Le...\"],[\"```\\n\\nThe initial values are exactly the same as the corresponding training example. However, this ex...\"],[\"```\\n\\nWe now use `datasets`' [`set_transform`](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fdatasets\\u002fv2.7.0\\u002fen\\u002fpackage...\"],[\"```\\n\\n## Define the Model\\n\\nNext, let's instantiate a model. The model will be trained from scratch, h...\"],[\"```\\n\\nThis means that this would look back up to 721 hours (~30 days) for each time step, as addition...\"],[\"```\\n\\nIn this case, there are four additional features, namely \\\"hour of day\\\", \\\"day of week\\\", \\\"day of ...\"],[\"```\\n\\nNote that hours and days are encoded as values between `[-0.5, 0.5]` from GluonTS. For more inf...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\nThe transformations below are annotated with comments, to explain what they do. At a high level...\"],[\"return Chain(\\n        # step 1: remove static\\u002fdynamic fields if not specified\\n        [RemoveFields(...\"],[\"),\\n            # step 4: add temporal features based on freq of the dataset\\n            # these serv...\"],[\"FieldName.TARGET: \\\"values\\\",\\n                    FieldName.OBSERVED_VALUES: \\\"observed_mask\\\",\\n        ...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"return InstanceSplitter(\\n        target_field=\\\"values\\\",\\n        is_pad_field=FieldName.IS_PAD,\\n     ...\"],[\"```\\n\\n## Create DataLoaders\\n\\nNext, it's time to create the DataLoaders, which allow us to have batche...\"],[\"# we initialize a Training instance\\n    instance_splitter = create_instance_splitter(config, \\\"train\\\"...\"],[\"```\\n\\n\\n```python\\ndef create_backtest_dataloader(\\n    config: PretrainedConfig,\\n    freq,\\n    data,\\n  ...\"],[\"transformation = create_transformation(freq, config)\\n    transformed_data = transformation.apply(dat...\"],[\"```\\n\\n\\n```python\\ntrain_dataloader = create_train_dataloader(\\n    config=config,\\n    freq=freq,\\n    da...\"],[\"```\\n\\nAs can be seen, we don't feed `input_ids` and `attention_mask` to the encoder (as would be the ...\"],[\"```\\n\\nNote that the model is returning a loss. This is possible as the decoder automatically shifts t...\"],[\"model, optimizer, train_dataloader = accelerator.prepare(\\n    model,\\n    optimizer,\\n    train_datalo...\"],[\"```\\n\\n```python\\n# view training\\nloss_history = np.array(loss_history).reshape(-1)\\nx = range(loss_hist...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nWe can evaluate the resulting forecast with respect to the ground truth out of sample values pr...\"],[\"```\\n\\n\\n```python\\nplt.scatter(mase_metrics, smape_metrics, alpha=0.2)\\nplt.xlabel(\\\"MASE\\\")\\nplt.ylabel(\\\"s...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002finfor...\"],[\"So the vanilla Transformer still performs best here! In the future, we hope to better benchmark thes...\"],[\"--\\ntitle: \\\"Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub\\\"...\"],[\"For example, the [IMDB dataset](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fimdb) specifies `en` in the YAML met...\"],[\"However, there is a major caveat to this. Most datasets (around 87%) do not specify the language use...\"],[\"### Predicting the Languages of Datasets Using Machine Learning\\n\\nWe‚Äôve already seen that many of the...\"],[\"```\\n\\nHowever, for some of the datasets on the Hub, we might be keen not to download the whole datase...\"],[\"We pass 20 examples to the model representing rows from a dataset. This results in 20 individual lan...\"],[\"We discard the script information since this isn't currently captured consistently as metadata on th...\"],[\"#### Next Steps \\n\\nAs the number of datasets on the Hub grows, metadata becomes increasingly importan...\"],[\"--\\ntitle: \\\"Generating Human-level Text with Contrastive Search in Transformers ü§ó\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"**[Remark]** For users who are not familiar with text generation, please refer more details to [this...\"],[\"```\\n\\n****\\n\\n\\u003cspan id='problems_of_decoding_methods'\\u002f\\u003e\\n\\n### 4. Problems of Existing Decoding Methods:\\n...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** From the result generated by greedy search, we can see obvious pattern ...\"],[\"```\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---------------------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** While nucleus sampling can generate text free of repetitions, the seman...\"],[\"\\u003ccenter class=\\\"half\\\"\\u003e\\n    \\u003cimg src=\\\"assets\\u002f115_introducing_contrastive_search\\u002fformulation.png\\\" width...\"],[\"\\u003cspan id='contrastive_generation'\\u002f\\u003e\\n\\n#### 5.2. Generating Text with Contrastive Search:\\n\\nBelow, we u...\"],[\"```\\n\\nThe arguments are as follows:\\n* `--top_k`: The hyperparameter \\\\\\\\(k\\\\\\\\) in contrastive search.\\n* ...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"\\\"The game of Go is a complex game in which players have to be very careful not to overextend their\\nt...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n**[Remark]** We see that the generated text is of exceptionally high quality. The en...\"],[\"****\\n\\n\\u003cspan id='more_examples'\\u002f\\u003e\\n\\n### 6. More Generated Examples:\\n\\nIn this section, we provide more ...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_greedy_example_one'\\u002f\\u003e\\n\\n##### 6.1.1. Generating Text with Greedy Searc...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"The researchers believe that the unicorns are descendants of the ancient Incas, who lived in the\\nare...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cspan id='gpt2_nucleus_example_one'\\u002f\\u003e\\n\\n##### 6.1.2. Generating Text with Nucleus Sam...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"The scientists think that it could be ancient folklore that has survived and is no longer attributed...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='gpt2_contrastive_example_one'\\u002f\\u003e\\n\\n##### 6.1.3. Generating Text with Contra...\"],[\"```\\nOutput:\\n----------------------------------------------------------------------------------------...\"],[\"After analyzing the data, the team determined that the herd consisted of at least three species of u...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_example_two'\\u002f\\u003e\\n\\n#### 6.2. Example Two - OPT:\\n\\nIn this part, we use th...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output: [click to expand]\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n---...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\u003cspan id='opt_greedy_example_two'\\u002f\\u003e\\n\\n##### 6.2.2. Generating Text with Nucleus Samp...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\n\\n\\u003cspan id='opt_contrastive_example_two'\\u002f\\u003e\\n\\n##### 6.2.3. Generating Text with Contra...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n\\u003cdetails open\\u003e\\n\\u003csummary\\u003e\\u003cb\\u003eModel Output:\\u003c\\u002fb\\u003e\\u003c\\u002fsummary\\u003e\\n\\n```\\nOutput:\\n----------------...\"],[\"```\\n\\u003c\\u002fdetails\\u003e\\n\\n****\\n\\n\\u003cspan id='resources'\\u002f\\u003e\\n\\n### 7. Resources:\\n\\nFor more details of contrastive sea...\"],[\"```\\n\\n\\n\\n****\\n\\n\\u003cspan id='references'\\u002f\\u003e\\n\\n## Reference:\\n\\u003e [1] Su et al., 2022 [\\\"A Contrastive Framework ...\"],[\"--\\ntitle: \\\"AMD + ü§ó: Large Language Models Out-of-the-Box Acceleration with AMD GPU\\\"\\nthumbnail: \\u002fblog...\"],[\"```python\\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\\nimport torch\\n\\nmodel_id = \\\"01-...\"],[\"```\\n\\nOne of the major aspects we have been working on is the ability to run Hugging Face Transformer...\"],[\"* Flash Attention v2 from AMD Open Source efforts in [ROCmSoftwarePlatform\\u002fflash-attention](https:\\u002f\\u002f...\"],[\"We are very excited to make these state of the art acceleration tools available and easy to use to H...\"],[\"\\u003cbr\\u003e\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg alt=\\\"\\\" src=\\\"assets\\u002foptimum_amd\\u002ftrans...\"],[\"Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs...\"],[\"Missing bars for A100 correspond to out of memory errors, as Llama 70B weights 138 GB in float16, an...\"],[\"Of course we'll soon be working on performance optimization for the MI300 lineup, ensuring that both...\"],[\"--\\ntitle: \\\"Machine Learning Experts - Lewis Tunstall\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f60_lewis_tunstall_inte...\"],[\"*Note: Transcription has been slightly modified\\u002freformatted to deliver the highest-quality reading e...\"],[\"This collaboration set the seeds for Leandro and I to eventually join Hugging Face. And I've been he...\"],[\"So for example, if you're trying to build a chatbot you need this model to be very fast and responsi...\"],[\"OpenAI actually provided in their blog posts some examples of the essays that this model had created...\"],[\"### You and other experts at Hugging Face have been working hard on the Hugging Face Course. How did...\"],[\"And he actually used that to apply to Hugging Face.\\n\\n### No way?!\\n\\n**Lewis:** He's joining the Big S...\"],[\"So this accelerates the whole field in a really powerful way. And I can imagine these applications u...\"],[\"### That is super interesting and powerful.\\n\\n**Lewis:** Maybe one thing to mention is that the whole...\"],[\"Although that may work, a lot of the time what happens is you introduce a lot of complexity into the...\"],[\"### If you could go back and do one thing differently at the beginning of your career in machine lea...\"],[\"### What are some of the industries you're most excited to see machine learning applied? \\n\\n**Lewis:*...\"],[\"I think there's hope that in my lifetime I will have a laundry-folding robot.\\n\\n### What have you bee...\"],[\"### What are some of your favorite Machine Learning papers?\\n\\n**Lewis:** Depends on how we measure th...\"],[\"But this example showed that you can actually be quite creative and help mathematicians find new ide...\"],[\"**Lewis:** So when O‚ÄôReilly is telling you ‚ÄúWe're going to get our illustrator now to design the cov...\"],[\"### I love it. Well, it looks absolutely amazing. A lot of these types of books tend to be quite dry...\"],[\"**Lewis:** See ya, Britney. Bye.\\n\\nThank you for listening to Machine Learning Experts!\\n\\n\\u003ca href=\\\"htt...\"],[\"--\\ntitle: 'Welcome fastai to the Hugging Face Hub'\\nthumbnail: \\u002fblog\\u002fassets\\u002f64_fastai\\u002ffastai_hf_blog....\"],[\"Because of all this, and more (the writer of this post started his journey thanks to the fast.ai cou...\"],[\"![Fastai Models in the Hub](assets\\u002f64_fastai\\u002fhf_hub_fastai.png)\\n\\nIn addition to free model hosting a...\"],[\"```\\n\\n## Creating a fastai `Learner`\\n\\nHere we train the [first model in the fastbook](https:\\u002f\\u002fgithub....\"],[\"```\\n\\n3. Use the `token` argument of the `push_to_hub_fastai` function.\\n\\nYou can input `push_to_hub_f...\"],[\"```\\n\\nThe `Learner` is now in the Hub in the repo named [`espejelomar\\u002fidentify-my-cat`](https:\\u002f\\u002fhuggi...\"],[\"First, upload an image of a cat (or possibly a dog?). The [Colab notebook with this tutorial](https:...\"],[\"```\\nIt works üëá!\\n\\n```py\\n_,_,probs = learner.predict(img)\\nprint(f\\\"Probability it's a cat: {100*probs[1...\"],[\"```\\n\\n```python\\nimport torch\\nimport transformers\\nfrom fastai.text.all import *\\n\\nfrom blurr.text.data....\"],[\"```\\n\\nTry it with a couple sentences and review their sentiment (negative or positive) with `learner_...\"],[\"```\\n\\n\\n## What's next?\\n\\nTake the [fast.ai course](https:\\u002f\\u002fcourse.fast.ai\\u002f) (a new version is coming s...\"],[\"--\\ntitle: \\\"StackLLaMA: A hands-on guide to train LLaMA with RLHF\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f138_stack...\"],[\"By combining these approaches, we are releasing the StackLLaMA model. This model is available on the...\"],[\"## Stack Exchange dataset\\n\\nGathering human feedback is a complex and expensive endeavor. In order to...\"],[\"## Efficient training strategies\\n\\nEven training the smallest LLaMA model requires an enormous amount...\"],[\"In this scenario, a rule of thumb is to allocate ~1.2-1.4GB per billion parameters (depending on the...\"],[\"```bash\\naccelerate launch --multi_gpu --num_machines 1  --num_processes 8 my_accelerate_script.py\\nto...\"],[\"```\\n\\n## Supervised fine-tuning\\n\\nBefore we start training reward models and tuning our model with RL,...\"],[\"```python\\n# load model in 8bit\\nmodel = AutoModelForCausalLM.from_pretrained(\\n        args.model_path...\"],[\"```\\n\\nWe train the model for a few thousand steps with the causal language modeling objective and sav...\"],[\"A trick that works well instead of direct feedback is training a reward model on human annotations c...\"],[\"```python\\nclass RewardTrainer(Trainer):\\n    def compute_loss(self, model, inputs, return_outputs=Fal...\"],[\"```\\n\\nWe utilize a subset of a 100,000 pair of candidates and evaluate on a held-out set of 50,000. W...\"],[\"```\\n\\nThe same template was used for SFT, RM and RLHF stages.\\n\\nA common issue with training the langu...\"],[\"# Compute sentiment score\\n    texts = [q + r for q, r in zip(batch[\\\"query\\\"], batch[\\\"response\\\"])]\\n   ...\"],[\"```\\n\\nWe train for 20 hours on 3x8 A100-80GB GPUs, using the ü§ó research cluster, but you can also get...\"],[\"In general in RL, you want to achieve the highest reward. In RLHF we use a Reward Model, which is im...\"],[\"One needs to be careful when generating the responses and we suggest to always use a simple sampling...\"],[\"## Citation\\n\\n```bibtex\\n@misc {beeching2023stackllama,\\n    author       = { Edward Beeching and\\n     ...\"],[\"```\\n\\n## Acknowledgements\\n\\nWe thank Philipp Schmid for sharing his wonderful [demo](https:\\u002f\\u002fhuggingfa...\"],[\"--\\ntitle:  Deploy LLMs with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f155_inference_e...\"],[\"Before we start, let's refresh our knowledge about Inference Endpoints. \\n\\n## What is Hugging Face In...\"],[\"You can get started with Inference Endpoints at: [https:\\u002f\\u002fui.endpoints.huggingface.co\\u002f](https:\\u002f\\u002fui.e...\"],[\"![Select Instance Type](assets\\u002f155_inference_endpoints_llm\\u002finstance-selection.png \\\"Select Instance T...\"],[\"```\\n\\nYou can use different parameters to control the generation, defining them in the `parameters` a...\"],[\"## 3. Stream responses in Javascript and Python\\n\\nRequesting and generating text with LLMs can be a t...\"],[\"```\\n\\nWe can create a `InferenceClient` providing our endpoint URL and credential alongside the hyper...\"],[\"```\\n\\nWe can create a `HfInferenceEndpoint` providing our endpoint URL and credential alongside the h...\"],[\"```\\n\\nReplace the `process.stdout` call with the `yield` or with a function you want to stream the to...\"],[\"--\\ntitle: \\\"2D Asset Generation: AI for Game Development #4\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-games...\"],[\"Requirements:\\n- Your preferred image-editing software, such as [Photoshop](https:\\u002f\\u002fwww.adobe.com\\u002fpro...\"],[\"In this section, I'll walk through how I generated a corn icon for the farming game. As a starting p...\"],[\"\\u003cdiv align=\\\"center\\\"\\u003e\\n  \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"[Dreambooth](https:\\u002f\\u002fdreambooth.github.io\\u002f), [textual inversion](https:\\u002f\\u002ftextual-inversion.github.io...\"],[\"--\\ntitle: \\\"Supercharged Customer Service with Machine Learning\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f61_superchar...\"],[\"Assuming that a) messages of very unsatisfied customers represent only a fraction of all messages an...\"],[\"Let's take a look at all available Datasets on the [Hugging Face Hub](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"Now we can inspect those datasets in more detail by reading through the dataset card, which ideally ...\"],[\"Let's quickly go over the dataset cards of the models above:\\n\\n-   *GLUE* is a collection of small da...\"],[\"As a final note, we recommend making use of Hub's dataset functionality even when working with priva...\"],[\"Let's take a look at all models that have been fine-tuned on Amazon Reviews Multi. You can find the ...\"],[\"However, both of the above resources are currently suboptimal. The model summary is not always kept ...\"],[\"## Training \\u002f Fine-tuning a model with ü§ó Transformers and ü§ó Datasets\\n\\nIn this section, we will jump ...\"],[\"```\\n\\nAlso, we install the ü§ó Transformers and ü§ó Datasets libraries to run this notebook. Since we wil...\"],[\"```\\n\\n\\n\\n### Preprocess the dataset\\n\\nBefore we can start training the model, we should bring the datas...\"],[\"```\\n\\n\\n\\nGreat, that was fast üî•. Let's take a look at the structure of the dataset.\\n\\n\\n```python\\nprint(...\"],[\"```\\n\\n**Output:**\\n```\\n    Stars: 1\\n    Review: This product caused severe burning of my skin. I have ...\"],[\"```\\n\\n\\n\\n\\nAs mentioned before, we will use the `\\\"review_body\\\"` as the model's input and `\\\"stars\\\"` as t...\"],[\"```\\n\\n\\nTo apply this function to all data samples in our dataset, we use the [`map`](https:\\u002f\\u002fhuggingf...\"],[\"```\\n\\n**Output:**\\n```\\n    Input IDS: [1, 329, 714, 2044, 3567, 5127, 265, 312, 1158, 260, 273, 286, 4...\"],[\"```\\n\\n```\\n    Some weights of the model checkpoint at microsoft\\u002fdeberta-v3-base were not used when in...\"],[\"```\\n\\n\\n\\nNext, we load a data collator. A [data collator](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmai...\"],[\"```\\n\\nNext, we define the `compute_metrics` which will be applied to the predicted outputs of the mod...\"],[\"```\\n\\n\\nPutting it all together, we can finally instantiate the Trainer by passing all required compon...\"],[\"**Output:**\\n\\u003cdiv\\u003e\\n\\u003ctable\\u003e\\u003cp\\u003e\\n  \\u003ctbody\\u003e\\n \\u003ctr style=\\\"text-align: left;\\\"\\u003e\\n  \\u003ctd\\u003eStep\\u003c\\u002ftd\\u003e\\n  \\u003ctd\\u003eTrainin...\"],[\"\\u003ctd\\u003e0.910928\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.608400\\u003c\\u002ftd\\u003e\\n  \\u003c\\u002ftr\\u003e\\n  \\u003ctr\\u003e\\n    \\u003ctd\\u003e30000\\u003c\\u002ftd\\u003e\\n    \\u003ctd\\u003e0.806700\\u003c\\u002ftd\\u003e\\n    ...\"],[\"**Output:**...\"],[\"```\\n    ***** Running Evaluation *****\\n      Num examples = 5000\\n      Batch size = 8\\n    Saving mod...\"],[\"```\\n\\n### Evaluate \\u002f Analyse the model\\n\\nNow that we have fine-tuned the model we need to be very care...\"],[\"```\\n    ***** Running Prediction *****\\n      Num examples = 5000\\n      Batch size = 8\\n```\\n\\n\\n**Output...\"],[\"```\\n\\n\\n\\nThe results are very similar to performance on the validation dataset, which is usually a goo...\"],[\"# Second let's compute how many satisfied messages we unnecessarily reply to\\n    satisfied_label_idx...\"],[\"```\\n\\n\\nWe again instantiate the `Trainer` to easily run the evaluation.\\n\\n\\n```python\\ntrainer = Trainer...\"],[\"```\\n\\n\\nand again upload everything on the Hub.\\n\\n\\n```python\\ntrainer.push_to_hub()\\n```\\n\\n**Output:**\\n```...\"],[\"```\\n\\n\\n\\nThe data is now saved [here](https:\\u002f\\u002fhuggingface.co\\u002fpatrickvonplaten\\u002fdeberta_amazon_reviews_v...\"],[\"If you're looking for **highly optimized** solutions which don't require any technical knowledge, yo...\"],[\"--\\ntitle: \\\"Accelerating Document AI\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f112_document-ai\\u002fthumbnail.png\\nauthors:...\"],[\"There are at least six general use cases for building document AI solutions. These use cases differ ...\"],[\"![png](assets\\u002f112_document-ai\\u002focr.png)\\n\\nOCR is a backbone of Document AI use cases as it's essential...\"],[\"A basic approach is applying OCR on a document image, after which a [BERT](https:\\u002f\\u002fhuggingface.co\\u002fdo...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"![png](assets\\u002f112_document-ai\\u002fDIT.png)\\n\\nDocument layout analysis with DiT.\\n\\nDocument layout analysis...\"],[\"The first version of LayoutLM (now known as LayoutLMv1) was released in 2020 and dramatically improv...\"],[\"![png](assets\\u002f112_document-ai\\u002flayoutlm.png)\\n\\nData scientists are finding document layout analysis an...\"],[\"The approach for table detection and structure recognition is similar to document layout analysis in...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"DocVQA is typically evaluated using the Average Normalized Levenshtein Similarity (ANLS) metric. For...\"],[\"\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv itemscope itemprop=\\\"mainEntity\\\" itemty...\"],[\"Data preparation for Document AI is critical and challenging. It's crucial to have properly annotate...\"],[\"The flexibility of building your models leads to many options for data scientists. Our strong recomm...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\n### Next Steps\\n\\nAre you seeing the possibilities of Document AI? ...\"],[\"A table of the currently available Transformers models achieving state-of-the-art performance on Doc...\"],[\"| model | paper | license | checkpoints |\\n| --- | --- | --- | --- |\\n| [Donut](https:\\u002f\\u002fhuggingface.co...\"],[\"| [LayoutLMv3](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flayoutlmv3) | [arxiv](http...\"],[\"| [LiLT](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002flilt) | [arxiv](https:\\u002f\\u002farxiv.or...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n\\u003chtml itemscope itemtype=\\\"https:\\u002f\\u002fschema.org\\u002fFAQPage\\\"\\u003e\\n  \\u003cdiv item...\"],[\"\\u003c\\u002fdiv\\u003e\\n    \\u003c\\u002fdiv\\u003e\\n        \\u003c\\u002fdiv\\u003e\\n\\n \\u003c\\u002fhtml\\u003e...\"],[\"--\\ntitle: \\\"How we sped up transformer inference 100x for ü§ó API customers\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f09...\"],[\"-| Naive version                                                                                    ...\"],[\"## Compilation FTW: the hard to get 10x\\nNow this is where it gets really tricky. In order to get the...\"],[\"To reach that bar, as Machine Learning Engineers at Hugging Face we certainly have an unfair advanta...\"],[\"--\\ntitle: \\\"Introducing HuggingFace blog for Chinese speakers: Fostering Collaboration with the Chine...\"],[\"In addition, the Chinese AI community has been actively engaged in creating trendy Spaces, such as [...\"],[\"We are excited to announce that we will continue to strengthen our ties with the Chinese AI communit...\"],[\"--\\ntitle: \\\"How to generate text: using different decoding methods for language generation with Trans...\"],[\"This blog post gives a brief overview of different decoding strategies\\nand more importantly shows ho...\"],[\"```\\n\\n``` python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\n\\ntorch_dev...\"],[\"```\\n\\n\\n## Greedy Search\\n\\nGreedy search is the simplest decoding method.\\nIt selects the word with the ...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nAlright\\\\! We have generated our first short text with GPT2 üòä. The\\ngenerated words following t...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002fbeam_search.png\\\" alt=\\\"beam search\\\" style=\\\"margin: auto; di...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nAs can be seen, the five beam hypotheses are only marginally different\\nto each other - which s...\"],[\"$$ w_t \\\\sim P(w|w_{1:t-1}) $$\\n\\nTaking the example from above, the following graphic visualizes langu...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nInteresting\\\\! The text seems alright - but when taking a closer look, it\\nis not very coherent...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nOK. There are less weird n-grams and the output is a bit more coherent\\nnow\\\\! While applying t...\"],[\"Let's see how *Top-K* can be used in the library by setting `top_k=50`:\\n\\n\\n\\n``` python\\n# set seed to ...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n\\nNot bad at all\\\\! The text is arguably the most *human-sounding* text so\\nfar. One concern thou...\"],[\"\\u003cimg src=\\\"\\u002fblog\\u002fassets\\u002f02_how-to-generate\\u002ftop_p_sampling.png\\\" alt=\\\"Top p sampling\\\" style=\\\"margin: au...\"],[\"```\\n\\n```\\nOutput:\\n-----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\n```\\nOutput:\\n----------------------------------------------------------------------------------...\"],[\"```\\n\\n\\nCool, now you should have all the tools to let your model write your\\nstories with `transformer...\"],[\"## Appendix\\n\\n`generate` has evolved into a highly composable method, with flags to manipulate the re...\"],[\"--\\ntitle: \\\"Accelerate your models with ü§ó Optimum Intel and OpenVINO\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f113_ope...\"],[\"‚ÄãLet us show you how to get started in minutes!‚Äã\\n\\n## Quantizing a Vision Transformer with Optimum In...\"],[\"```\\n\\nNext, moving to a Python environment, we import the appropriate modules and download the origin...\"],[\"```\\n\\nAs usual with image datasets, we need to apply the same image transformations that were used at...\"],[\"```\\n\\nWe're now ready to quantize the model. The `OVQuantizer.quantize()` method quantizes the model ...\"],[\"```\\n\\nA minute or two later, the model has been quantized. We can then easily load it with our [`OVMo...\"],[\"```\\n\\n‚ÄãTo verify that quantization did not have a negative impact on accuracy, we applied an evaluati...\"],[\"```\\n\\nLooking at the quantized model, we see that its memory size decreased by **3.8x** from 344MB to...\"],[\"--\\ntitle: Guiding Text Generation with Constrained Beam Search in ü§ó Transformers\\nthumbnail: \\u002fblog\\u002fas...\"],[\"However, this is actually a very non-trivial problem. This is because the task requires us to force ...\"],[\"And what if you have multiple constraints with varying requirements? What if you want to force the p...\"],[\"```\\n!pip install -q git+https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers.git\\n```\\n\\n\\n```python\\nfrom transfo...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"force_word = \\\"scared\\\"\\nforce_flexible = [\\\"scream\\\", \\\"screams\\\", \\\"screaming\\\", \\\"screamed\\\"]\\n\\nforce_words_i...\"],[\"```\\n\\n    Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\\n\\n\\n    Output:\\n    -...\"],[\"![Beam search step 1](https:\\u002f\\u002fraw.githubusercontent.com\\u002fhuggingface\\u002fblog\\u002fmain\\u002fassets\\u002f53_constrained_...\"],[\"Let's say that we're trying to force the phrase `\\\"is fast\\\"` in the generated output. \\n\\nIn the tradit...\"],[\"Banks solve this problem by creating a *balance* between fulfilling the constraints and creating sen...\"],[\"And finally notice how we ended up at a sensible output that contains our constraint phrase: `\\\"The d...\"],[\"print(\\\"Output:\\\\n\\\" + 100 * '-')\\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))...\"],[\"```\\n\\n    Output:\\n    -------------------------------------------------------------------------------...\"],[\"```\\n\\nor:\\n```python\\nstarting_text = \\\"The woman\\\"\\ntemplate = [\\\"the\\\", \\\"\\\", \\\"\\\", \\\"University\\\", \\\"\\\", \\\"in\\\"]\\n\\np...\"],[\"Thanks to everybody that gave guidance for this feature contribution: Patrick von Platen for being i...\"],[\"--\\ntitle: \\\"Making a web app generator with open ML models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f153_text_to_webap...\"],[\"Some of those techniques are now available as ready-to-use NPM libraries:\\n\\n- Using AI\\u002fML libraries s...\"],[\"## Architecture\\n\\nWe are going to use NodeJS to create our generative AI web server.\\n\\nThe model will ...\"],[\"```\\n\\nThen, we can install the Hugging Face Inference client:\\n\\n```html\\nnpm install @huggingface\\u002finfer...\"],[\"```\\n\\nYou can now tell the inference client to use our private endpoint and call our model:\\n\\n```javas...\"],[\"```\\n\\nStart your web server:\\n\\n```bash\\nnpm run start\\n```\\n\\nand open `https:\\u002f\\u002flocalhost:3000?prompt=some...\"],[\"```\\n\\n### Preventing hallucination\\n\\nIt can be difficult to reliably prevent hallucinations and failur...\"],[\"```\\n\\n## Adding support for images\\n\\nWe now have a system that can generate HTML, CSS and JS code, but...\"],[\"```\\n\\nYou can also try to be more specific, for example:\\n\\n```\\nOnly generate a few images and use desc...\"],[\"```\\n\\nTo make this work, you will have to make some changes:\\n\\n```javascript\\n...\\n\\n\\u002f\\u002f going to localhos...\"],[\"```\\n\\n## Going further\\n\\nThe final demo Space includes a [more complete example](https:\\u002f\\u002fhuggingface.c...\"],[\"--\\ntitle: 'Liftoff! How to get started with your first ML project üöÄ'\\nthumbnail: \\u002fblog\\u002fassets\\u002f84_firs...\"],[\"\\u003e Compute dense vector representations for sentences, paragraphs, and images\\n\\nIn a nutshell, Sentenc...\"],[\"Comparing sentences by similarity means that if we have a collection of sentences or paragraphs, we ...\"],[\"Second, Sentence Transformers is an accessible entry-point to many important ML concepts that you ca...\"],[\"Third, embeddings are key for several industrial applications. Google searches use embeddings to [ma...\"],[\"1. **Do a brain dump of everything you know the tool‚Äôs capable of**: For Sentence Transformers this ...\"],[\"4. **Ideate:** Spend some time brainstorming on what different combination of the elements from the ...\"],[\"For my first Sentence Transformers project, I remembered that I had a little dataset of popular song...\"],[\"\\u003cdiv class=\\\"hidden xl:block\\\"\\u003e\\n\\u003cdiv style=\\\"display: flex; flex-direction: column; align-items: center...\"],[\"## What can you expect to learn from your first project?\\n\\nSince every project is unique, your learni...\"],[\"Further reading:\\n\\n- [Getting Started with Embeddings](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fgetting-started-wi...\"],[\"--\\ntitle: \\\"Fit More and Train Faster With ZeRO via DeepSpeed and FairScale\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"This blog post will describe how you can benefit from ZeRO regardless of whether you own just a sing...\"],[\"```\\nexport BS=16\\npython -m torch.distributed.launch --nproc_per_node=2 .\\u002ffinetune_trainer.py \\\\\\n--mod...\"],[\"```\\n\\nWe are just using the `DistributedDataParallel` (DDP) and nothing else to boost the performance...\"],[\"Let's look at the results of these six test runs:\\n\\n| Method                    | max BS |  train tim...\"],[\"If you would like to experiment with this benchmark yourself or want to know more details about the ...\"],[\"```\\nexport BS=1\\nCUDA_VISIBLE_DEVICES=0 .\\u002ffinetune_trainer.py \\\\\\n--model_name_or_path t5-3b --n_train ...\"],[\"```\\net voila! We get a batch size of 20 trained just fine. I could probably push it even further. Th...\"],[\"```\\nWe can't compare these to the baseline, since the baseline won't even start and immediately fail...\"],[\"This idea could be difficult to grasp, and you will find my attempt at an explanation [here](https:\\u002f...\"],[\"```\\nRuntimeError: CUDA out of memory. Tried to allocate 1.48 GiB (GPU 0; 23.65 GiB total capacity;\\n1...\"],[\"```\\nThe program wants to allocate ~1.5GB and the GPU still has some 6-7GBs of unused memory, but it ...\"],[\"You can, of course, modify your own trainer to integrate DeepSpeed and FairScale, based on each proj...\"],[\"* Paper: [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https:\\u002f\\u002farxiv.org\\u002fab...\"],[\"We were quite astonished at the amazing level of support we received from the FairScale and DeepSpee...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #1\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f103_ethics-soc-1\\u002fthumbnail.png...\"],[\"To this end, we share some of our recent thinking and work in the new Hugging Face _Ethics and Socie...\"],[\"- We ground the creation of these tools and artifacts in _responsibility_ for the impacts of what we...\"],[\"Building from these basics, we are taking an approach to operationalizing values that center the con...\"],[\"In the coming months, we will be putting together several other pieces on values, tensions, and ethi...\"],[\"--\\ntitle: \\\"Open LLM Leaderboard: DROP deep dive\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fevaluating-mmlu-leaderboard...\"],[\"We added it to the Open LLM Leaderboard three weeks ago, and observed that the f1-scores of pretrain...\"],[\"Normalization happens in several steps, both for generation and gold:\\n1) **Split on separators** `|`...\"],[\"## Diving into the results\\nExtending our investigations, our friends at [Zeno](https:\\u002f\\u002fzenoml.com) j...\"],[\"We hypothesized that both these problems could be fixed by using `\\\\n` instead of `.` as an end of ge...\"],[\"In 10% of the cases, the gold answer is a floating number (for example `12.25`) and model prediction...\"],[\"Thanks to the many community members who pointed out issues on DROP scores, and many thanks to the E...\"],[\"--\\ntitle: \\\"Evaluating Language Model Bias with ü§ó Evaluate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f112_evaluating-ll...\"],[\"The workflow has two main steps:\\n- Prompting the language model with a predefined set of prompts (ho...\"],[\"```python\\n\\u003e\\u003e\\u003e male_prompts = [\\n'The janitor reprimanded the accountant because he',\\n'The carpenter a...\"],[\"```\\n\\nAlthough we define these prompts directly for the sake of example here, more can be extracted d...\"],[\"```\\nAs you can see above, a simple difference in pronoun can result in a higher toxicity ratio for f...\"],[\"```\\n\\nAnd as before, we use GPT-2 to generate completions:\\n```python\\n\\u003e\\u003e\\u003e profession1_completions = [\\\"...\"],[\"```\\nBased on the Regard scores above, the completions for profession 1 (truck drivers) have a more n...\"],[\"```\\n\\nHigher HONEST scores mean more hurtful completions. Based on the model completions above, we ha...\"],[\"*- Written by Sasha Luccioni and Meg Mitchell, drawing on work from the Evaluate crew and the Societ...\"],[\"--\\ntitle: \\\"Can foundation models label data like humans?\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fllm-leaderboard\\u002fle...\"],[\"![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002fllm-leaderboa...\"],[\"## Evaluating preferences of open-source models\\n\\nAny point in a training process where humans are ne...\"],[\"To do this, we curated a held-out set of instruction prompts and completions from a popular set of o...\"],[\"With these completions, we set off to evaluate the quality of the models with Scale AI and GPT-4. \\nT...\"],[\"****************Elo rankings without ties (bootstrapped from 1000 rounds of sampling games)*********...\"],[\"****************Elo rankings w\\u002f ties (bootstrapped from 1000 rounds of sampling games)**************...\"],[\"**Elo rankings w\\u002f ties (bootstrapped from 1000 rounds of sampling games)**\\n\\n*Reminder, in the Likert...\"],[\"```\\n### Question\\n{question}\\n\\n### The Start of Assistant 1's Answer\\n{answer_1}\\n### The End of Assista...\"],[\"```\\n\\nThe histogram of responses from GPT-4 starts to show a clear issue with LLM based evaluation: *...\"],[\"## Related work\\n\\nWe are not the only ones to share the GPT-4 may not be a perfect tool for training ...\"],[\"Below we‚Äôve included a couple examples of what the evaluations look like to give you a sense why and...\"],[\"I am excited about what lies ahead and can't wait to join the team at [Company Name]. Thank you agai...\"],[\"This answer only takes up 34 characters compared to longer explanations like sunlight reaching earth...\"],[\"### Ablations\\n\\n**GPT-4 Elo with score rather than ranking**\\n\\nOther evaluation benchmarks use a ranki...\"],[\"```\\n\\nThis resulted in the histogram of rankings below, which flipped the bias from before (but did n...\"],[\"## Takeaways and discussion\\n\\nThere is a lot here, but the most important insights in our experiments...\"],[\"Continuing with this, it is worth noting that ChatGPT (a slightly less high performance model) actua...\"],[\"- **Correct generation parameters**: in the early stages of our experiments, we had to spend substan...\"],[\"### Resources and citation\\n\\n- More information on our labeling instructions can be found [here](http...\"],[\"```\\n@article{rajani2023llm_labels,\\n  author = {Rajani, Nazneen, and Lambert, Nathan and Han, Sheon a...\"],[\"--\\ntitle: \\\"Student Ambassador Program‚Äôs call for applications is open!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f67_a...\"],[\"üéé Network of peers with whom ambassadors can collaborate. \\n\\nüßëüèª‚Äçüíª Workshops and support from the Hugg...\"],[\"--\\ntitle: \\\"Training a language model with ü§ó¬†Transformers using TensorFlow and TPUs\\\"\\nthumbnail: \\u002fblog...\"],[\"Unlike our Colab example, however, this example is designed to be **scalable** and much closer to a ...\"],[\"## What to expect\\n\\nWe‚Äôre going to train a [RoBERTa](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_d...\"],[\"## Getting the data and training a tokenizer\\n\\nAs mentioned, we used the [WikiText dataset (v1)](http...\"],[\"## Tokenizing the data and creating TFRecords\\n\\nOnce the tokenizer is trained, we can use it on all t...\"],[\"## Training a model on data in GCS\\n\\nIf you‚Äôre familiar with using ü§ó¬†Transformers, then you already k...\"],[\"```\\n\\nBut since we‚Äôre in the TPU territory, we need to perform this initialization under a strategy s...\"],[\"```\\n\\nSimilarly, the optimizer also needs to be initialized under the same strategy scope with which ...\"],[\"```\\n\\nIf `args.dataset` contains the `gs:\\u002f\\u002f` identifier, TensorFlow will understand that it needs to ...\"],[\"[{'score': 0.1003185287117958,\\n  'token': 52,\\n  'token_str': 'be',\\n  'sequence': 'Goal of my life is...\"],[\"```\\n\\n## Conclusion\\n\\nIf there‚Äôs one thing we want to emphasize with this example, it‚Äôs that TPU train...\"],[\"--\\ntitle: \\\"A Dive into Vision-Language Models\\\"\\nthumbnail: \\u002fblog\\u002f\\u002fassets\\u002f128_vision_language_pretrain...\"],[\"## Table of contents\\n\\n1. [Introduction](#introduction)\\n2. [Learning Strategies](#learning-strategies...\"],[\"To predict something like that, the model needs to understand both the input image and the text prom...\"],[\"We‚Äôll cover the following themes in the pre-training objectives: \\n- **Contrastive Learning:** Aligni...\"],[\"Contrastive learning is a commonly used pre-training objective for vision models and has proven to b...\"],[\"### 2) PrefixLM\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocume...\"],[\"Models that leverage a unified multi-modal architecture to fuse visual information into a language m...\"],[\"Models such as [Frozen](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2106.13884) and [ClipCap](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2111.0...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### 4) Masked-Language Modeling \\u002f Image-Text Matching\\n\\nAnother line of vision-language models uses a...\"],[\"For the ITM objective, given an image and caption pair, the task is to predict whether the caption m...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"### Pre-training datasets\\n\\nVision-language models are typically pre-trained on large multi-modal dat...\"],[\"Even image-text datasets consisting solely of human-generated captions, such as Flickr30K, are inher...\"],[\"Models fine-tuned on the question-answering downstream task, such as [ViLT](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f21...\"],[\"Note that vision-language models are used for various classical NLP and computer vision tasks such a...\"],[\"* [CLIP](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002fclip)\\n* [FLAVA](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"* [TrOCR](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmain\\u002fen\\u002fmodel_doc\\u002ftrocr) (an instance of the `Vis...\"],[\"While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide j...\"],[\"Let‚Äôs go ahead and experiment with some of these models. We will use [ViLT](https:\\u002f\\u002fhuggingface.co\\u002fd...\"],[\"```\\n\\nNext, we will download a random image of two cats and preprocess both the image and our  query ...\"],[\"```\\n\\nStraight-forward, right? Let‚Äôs do another demonstration with CLIPSeg and see how we can perform...\"],[\"```\\n\\nSimilar to ViLT, it‚Äôs important to refer to the [original work](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2112.1000...\"],[\"```\\n\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-imag...\"],[\"We also see a massive surge of works that leverage joint vision-language representations for image m...\"],[\"While robotics research hasn‚Äôt leveraged vision-language models on a wide scale yet, we see works su...\"],[\"We are continuing to integrate the most impactful computer vision and multi-modal models and would l...\"],[\"--\\ntitle: \\\"Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Dis...\"],[\"- [Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tw...\"],[\"- [Llama 2](#llama-2)\\n            - [Load checkpoints for the classification mode](#load-checkpoints...\"],[\"\\u003c!-- \\u002fTOC --\\u003e\\n\\n\\n\\n## Introduction \\n\\nIn the fast-moving world of Natural Language Processing (NLP), we...\"],[\"## Hardware Used \\n\\n- Number of nodes: 1 \\n- Number of GPUs per node: 1\\n- GPU type: A6000 \\n- GPU memor...\"],[\"```\\nNote: For reproducing the reported results, please check the pinned versions in the [wandb repor...\"],[\"### [Mistral 7B](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2310.06825)\\n\\nMistral 7B v0.1, with 7.3 billion parameters, is...\"],[\"## Setup\\n\\nRoBERTa has a limitatiom of maximum sequence length of 512, so we set the `MAX_LEN=512` fo...\"],[\"```\\n\\n## Data preparation\\n### Data loading\\n\\nWe will load the dataset from Hugging Face:\\n```python\\nfro...\"],[\"```\\n\\n- Train dataset\\n\\n```\\u003cclass 'pandas.core.frame.DataFrame'\\u003e\\nRangeIndex: 7613 entries, 0 to 7612\\nD...\"],[\"```\\n\\nThe final weights are: \\n```\\nPOS_WEIGHT, NEG_WEIGHT = (1.1637114032405993, 0.8766697374481806)\\n`...\"],[\"```\\n**Note:** The RoBERTa tokenizer has been trained to treat spaces as part of the token. As a resu...\"],[\"```\\n\\n- Now, let's  apply the preprocessing function to the entire dataset: \\n\\n```python\\ncol_to_delete...\"],[\"```\\n\\n\\nYou can follow the same steps for preparing the data for Mistral 7B and Llama 2 models: \\n\\n**No...\"],[\"```\\n\\n- Llama 2:\\n```python\\n# Load Llama 2 Tokenizer\\nfrom transformers import AutoTokenizer, DataColla...\"],[\"```\\n\\n\\n####  LoRA setup for RoBERTa classifier\\n\\nWe import LoRa configuration and set some parameters ...\"],[\"```\\nFor Mistral 7B, we have to add the padding token id as it is not defined by default.\\n\\n```python\\n...\"],[\"```\\n\\nFor Llama 2, we have to add the padding token id as it is not defined by default.\\n\\n```python\\nll...\"],[\"```\\ntrainable params: 8,404,992 || all params: 6,615,748,608 || trainable%: 0.1270452143516515\\n```\\n\\n...\"],[\"```\\n\\n### Custom Trainer for Weighted Loss \\nAs mentioned at the beginning of this post, we have an im...\"],[\"```\\nIt will print the following: \\n```\\ndevice(type='cuda', index=0)\\n```\\n\\nThen, we set the training ar...\"],[\"```\\n\\n#### Mistral-7B\\n\\nSimilar to RoBERTa, we initialize the `WeightedCELossTrainer` as follows: \\n\\n``...\"],[\"```\\n\\n**Note** that we needed to enable half-precision training by setting `fp16` to `True`. The main...\"],[\"```\\n\\n\\n\\n\\n## Hyperparameter Tuning\\n\\nWe have used Wandb Sweep API to run hyperparameter tunning with Ba...\"],[\"For more information, you can check the Wandb experiment report in the [resources sections](#resourc...\"],[\"Finally, we showcase that LoRa method can be applied to both encoder (RoBERTa) and decoder (Llama 2 ...\"],[\"--\\ntitle: \\\"Introducing DOI: the Digital Object Identifier to Datasets and Models\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"## How are DOIs being assigned by Hugging Face? \\n\\nWe have partnered with [DataCite](https:\\u002f\\u002fdatacite...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach...\"],[\"The AMX instructions accelerate matrix multiplication, an operation central to training DL models on...\"],[\"From a networking perspective, we will need the following setup:\\n\\n* Open port 22 for ssh access on a...\"],[\"```\\namx_bf16 amx_tile amx_int8\\n```\\n\\nThen, we install native and Python dependencies.\\n\\n```\\nsudo apt-g...\"],[\"```\\n\\nNext, we create a new ssh key pair called 'cluster' with `ssh-keygen` and store it at the defau...\"],[\"```\\nlocalhost\\nnode1\\nnode2\\nnode3\\n```\\n\\nThe cluster is now ready. Let's start training!\\n\\n## Launching a...\"],[\"```\\n\\nNo need to let the job run to completion, We just run for a minute to make sure that all depend...\"],[\"```\\n\\nNow, we launch the distributed training job.\\n\\n```\\n# Launch distributed training\\nmpirun -f ~\\u002fhos...\"],[\"```\\n\\nOne epoch now takes **7 minutes and 30 seconds**. \\n\\nHere's what the job looks like. The master ...\"],[\"--\\ntitle: Introducing our new pricing\\nthumbnail: \\u002fblog\\u002fassets\\u002f114_pricing-update\\u002fthumbnail.png\\nautho...\"],[\"--\\ntitle: Faster Stable Diffusion with Core ML on iPhone, iPad, and Mac\\nthumbnail: \\u002fblog\\u002fassets\\u002f149_...\"],[\"## New Core ML Optimizations\\n\\nCore ML is a mature framework that allows machine learning models to r...\"],[\"\\u003cimg style=\\\"border:none;\\\" alt=\\\"Illustration of 2-bit palettization. Image credit: Apple WWDC‚Äô23 Sess...\"],[\"## Using Quantized and Optimized Stable Diffusion Models\\n\\n[Last December](https:\\u002f\\u002fhuggingface.co\\u002fblo...\"],[\"| Model                     | Uncompressed      | Palettized                |\\n|---------------------...\"],[\"In order to use 6-bit models, you need the development versions of iOS\\u002fiPadOS 17 or macOS 14 (Sonoma...\"],[\"repo_id = \\\"apple\\u002fcoreml-stable-diffusion-2-1-base-palettized\\\"\\nvariant = \\\"original\\u002fpackages\\\"\\n\\nmodel_p...\"],[\"```\\n\\n## Converting and Optimizing Custom Models\\n\\nIf you want to use a personalized Stable Diffusion ...\"],[\"```bash\\npython -m python_coreml_stable_diffusion.torch2coreml \\\\\\n    --model-version prompthero\\u002fopenj...\"],[\"```\\n\\n\\u003cbr\\u003e\\n\\u003cdiv style=\\\"background-color: #f0fcf0; padding: 8px 32px 1px; outline: 1px solid; border-r...\"],[\"```\\n\\n4. Test the converted models on the desired hardware. As a rule of thumb, the `ORIGINAL` versio...\"],[\"We have plans to evaluate this method soon, and can‚Äôt wait to see how 4-bit optimized models work an...\"],[\"his notebook shows how to deploy a vision model from ü§ó Transformers (written in TensorFlow) to [Vert...\"],[\"```\\n\\n\\n```python\\nimport transformers\\n\\nprint(tf.__version__)\\nprint(transformers.__version__)\\n```\\n\\n## S...\"],[\"```\\n\\n\\n```python\\ndef normalize_img(img, mean=processor.image_mean, std=processor.image_std):\\n    # Sc...\"],[\"predictions = m_call(**images)\\n        indices = tf.argmax(predictions.logits, axis=1)\\n        pred_...\"],[\"```\\n\\n\\n```python\\n# To deploy the model on Vertex AI we must have the model in a storage bucket.\\ntf.sa...\"],[\"```\\n\\n\\n```python\\n# Upload the model to Vertex AI. \\ntf28_gpu_model_dict = {\\n    \\\"display_name\\\": \\\"ViT B...\"],[\"```\\n\\n\\n```python\\n# Deploy the Endpoint. \\ntf28_gpu_deployed_model_dict = {\\n    \\\"model\\\": tf28_gpu_model...\"],[\"```\\n\\n\\n```python\\nfrom google.protobuf import json_format\\nfrom google.protobuf.struct_pb2 import Value...\"],[\"--\\ntitle: \\\"Retrieval Augmented Generation with Huggingface Transformers and Ray\\\"\\nthumbnail: \\u002fblog\\u002fas...\"],[\"Recently, [Huggingface](https:\\u002f\\u002fhuggingface.co\\u002f) partnered with [Facebook AI](https:\\u002f\\u002fai.facebook.co...\"],[\"### Scaling up fine-tuning\\nThis retrieval of contextual documents is crucial for RAG's state-of-the-...\"],[\"![alt_text](assets\\u002f12_ray_rag\\u002fray_arch_updated.png \\\"image_tooltip\\\")\\n_Document retrieval with the Ray...\"],[\"_A performance comparison of different retrieval implementations. For each document retrieval implem...\"],[\"```\\n\\n\\nThen, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](h...\"],[\"```\\n\\n## What‚Äôs next?\\n\\nUsing RAG with [Huggingface transformers](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftrans...\"],[\"If you plan to try RAG+Ray integration out, please feel free to share your experiences on the [Ray D...\"],[\"--\\ntitle: Introducing Pull Requests and Discussions ü•≥\\nthumbnail: \\u002fblog\\u002fassets\\u002f76_community_update\\u002fth...\"],[\"## Pull requests\\n\\n![Pull requests on the Hugging Face Hub](assets\\u002f76_community_update\\u002fnew-pr.png)\\n\\n[...\"],[\"--\\ntitle: \\\"Introducing Agents.js: Give tools to your LLMs using JavaScript\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"```\\n\\nThen the code can be evaluated as such:\\n\\n```ts\\nconst messages = await agent.evaluateCode(code);...\"],[\"```\\n\\n### Usage warning\\n\\nCurrently using this library will mean evaluating arbitrary code in the brow...\"],[\"```\\n\\n## Custom Tools üõ†Ô∏è\\n\\nAgents.js was designed to be easily expanded with custom tools & examples. ...\"],[\"```\\n\\n## Passing input files to the agent üñºÔ∏è\\n\\nThe agent can also take input files to pass along to th...\"],[\"--\\ntitle: \\\"Using Machine Learning to Aid Survivors and Race through Time\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fu...\"],[\"![organization](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisast...\"],[\"![NER](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdisaster-assets...\"],[\"![backend_pipeline](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"In the end, we decided to fine-tune our own model as it would take roughly three minutes to fine-tun...\"],[\"![active_learning](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdis...\"],[\"To address these issues and create open source tools that can be leveraged in the future, we started...\"],[\"![output_satellite](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdi...\"],[\"--\\ntitle: \\\"Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2\\\"\\nthumbnail: \\u002fblog\\u002fa...\"],[\"Another factor to consider is the level of parallelism in the model and the inference task. GPUs are...\"],[\"```\\nsudo apt-get update\\n\\n# Add libtcmalloc for extra performance\\nsudo apt install libgoogle-perftool...\"],[\"```\\nsentence_short = \\\"This is a really nice pair of shoes, I am completely satisfied with my purchas...\"],[\"```\\n\\nOn the c6i (Ice Lake) instance, we only use a vanilla Transformers pipeline. \\n\\n```\\nfrom transfo...\"],[\"```\\n\\nFor the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https...\"],[\"--\\ntitle: Getting Started with Hugging Face Inference Endpoints\\nthumbnail: \\u002fblog\\u002fassets\\u002f109_inferenc...\"],[\"Starting from my [model page](https:\\u002f\\u002fhuggingface.co\\u002fjuliensimon\\u002fautotrain-food101-1471154053), I cl...\"],[\"Let's first deploy a protected endpoint, and then we'll deploy a private one.\\n\\n### Deploying a Prote...\"],[\"```\\nimport requests, json\\n\\nAPI_URL = \\\"https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.c...\"],[\"```\\n5c7fbb4485cd8w7 2022-10-10T08:19:04.915Z 2022-10-10 08:19:04,915 | INFO | POST \\u002f | Duration: 142...\"],[\"```\\n\\nNow, let's increase our security level and deploy a private endpoint.\\n \\n### Deploying a Private...\"],[\"```\\ncurl https:\\u002f\\u002foncm9ojdmjwesag2.eu-west-1.aws.endpoints.huggingface.cloud \\\\\\n-X POST --data-binary ...\"],[\"--\\ntitle: \\\"Non-engineers guide: Train a LLaMA 2 chatbot\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f78_ml_director_insi...\"],[\"## Introduction to Spaces\\n\\nSpaces from Hugging Face is a service that provides easy to use GUI for b...\"],[\"1.2 Give your Space a name and select a preferred usage license if you plan to make your model or Sp...\"],[\"2.2 Choose the LLM you want to train from the ‚ÄúModel Choice‚Äù field, you can select a model from the ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"2.10 Go grab a coffee, depending on the size of your model and training data this could take a few h...\"],[\"3.4 Under ‚ÄúSpace variables‚Äù you can also change model inference parameters including temperature, to...\"],[\"--\\ntitle: \\\"Ethical Guidelines for developing the Diffusers library\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002fethics-...\"],[\"# Safety features and mechanisms\\n\\nIn addition, we provide a non-exhaustive - and hopefully continuou...\"],[\"--\\ntitle: \\\"Introducing BERTopic Integration with the Hugging Face Hub\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_b...\"],[\"## What is BERTopic?\\n\\nBERTopic is a state-of-the-art Python library that simplifies the topic modell...\"],[\"BERTopic provides a powerful tool for users to uncover significant topics within text collections, t...\"],[\"```\\nYou can then load this model in two lines and use it to predict against new data.\\n\\n```python\\nfro...\"],[\"```\\n\\nBy leveraging the power of the Hugging Face Hub, BERTopic users can effortlessly share, version...\"],[\"\\u003cdetails\\u003e\\n  \\u003csummary\\u003eClick here for an overview of all topics.\\u003c\\u002fsummary\\u003e\\n  \\n  | Topic ID | Topic Key...\"],[\"| 10 | news - fake - fake news - stance - fact | 455 | 10_news_fake_fake news_stance | \\n| 11 | relat...\"],[\"| 23 | adversarial - attacks - attack - adversarial examples - robustness | 181 | 23_adversarial_att...\"],[\"| 36 | classification - text classification - label - text - labels | 136 | 36_classification_text c...\"],[\"| 49 | poetry - poems - lyrics - poem - music | 103 | 49_poetry_poems_lyrics_poem | \\n| 50 | image - ...\"],[\"| 62 | change - semantic change - time - semantic - lexical semantic | 82 | 62_change_semantic chang...\"],[\"| 76 | translation - machine translation - machine - smt - statistical | 54 | 76_translation_machine...\"],[\"| 90 | emoji - emojis - sentiment - message - anonymous | 35 | 90_emoji_emojis_sentiment_message | \\n...\"],[\"| 104 | gender - translation - bias - gender bias - mt | 24 | 104_gender_translation_bias_gender bia...\"],[\"Due to the improved saving procedure, training on large datasets generates small model sizes. In the...\"],[\"To illustrate some of the power of BERTopic let's look at an example of how it can be used to monito...\"],[\"[databricks\\u002fdatabricks-dolly-15k](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdatabricks\\u002fdatabricks-dolly-15k) i...\"],[\"```\\n\\nWe can predict on a single example text: \\n\\n```python\\nexample = \\\"Stalemate is a drawn position. ...\"],[\"```\\n\\nWe can then compare the distribution of topics across both datasets. We can see here that there...\"],[\"Some examples of BERTopic models already on the hub:\\n- [MaartenGr\\u002fBERTopic_ArXiv](https:\\u002f\\u002fhuggingfac...\"],[\"--\\ntitle: \\\"OpenRAIL: Towards open and responsible AI licensing frameworks\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"Most current model developers seem to think so, as the majority of openly released models have an op...\"],[\"If specific ad hoc practices devoted to documentation, transparency and ethical usage of ML models a...\"],[\"## **A change of licensing paradigm: OpenRAIL**\\n\\nThe OpenRAIL [approach](https:\\u002f\\u002fwww.licenses.ai\\u002fblo...\"],[\"The effect of copyleft-style behavioral-use clauses spreads the requirement from the original licens...\"],[\"## **OpenRAIL could be for good machine learning what open software licensing is to code**\\n\\nThree ex...\"],[\"The licenses are BigScience's reaction to 2 partially addressed challenges in the licensing space: (...\"],[\"Let's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and...\"],[\"--\\ntitle: Using LoRA for Efficient Stable Diffusion Fine-Tuning\\nthumbnail: \\u002fblog\\u002fassets\\u002flora\\u002fthumbna...\"],[\"![Latent Diffusion Architecture](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"- Training is much faster, as already discussed.\\n- Compute requirements are lower. We could create a...\"],[\"Diffusers now provides a [LoRA fine-tuning script](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fmai...\"],[\"```\\n\\nOne thing of notice is that the learning rate is `1e-4`, much larger than the usual learning ra...\"],[\"First, we'll use the Hub API to automatically determine what was the base model that was used to fin...\"],[\"```\\n\\nThis snippet will print the model he used for fine-tuning, which is `CompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\n## Dreamboothing with LoRA\\n\\nDreambooth allows you to \\\"teach\\\" new concepts to a Stable Diffusion...\"],[\"## Other Methods\\n\\nThe quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual in...\"],[\"--\\ntitle: \\\"Graph Classification with Transformers\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"## Requirements\\nTo follow this tutorial, you need to have installed `datasets` and `transformers` (v...\"],[\"```\\n\\nThis dataset already has three splits, `train`, `validation`, and `test`, and all these splits ...\"],[\"```\\n\\n### Format\\nOn the Hub, graph datasets are mostly stored as lists of graphs (using the `jsonl` f...\"],[\"A single graph is a dictionary, and here is the expected format for our graph classification dataset...\"],[\"- `edge_attr` contains the available attributes (if present) for each edge of the graph, following t...\"],[\"### Preprocessing\\nGraph transformer frameworks usually apply specific preprocessing to their dataset...\"],[\"```\\n\\nIt is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (b...\"],[\"```\\nLet's look at this in more detail. \\n\\nCalling the `from_pretrained` method on our model downloads...\"],[\"```\\nIn the `Trainer` for graph classification, it is important to pass the specific data collator fo...\"],[\"--\\ntitle: Fine-Tune a Semantic Segmentation Model with a Custom Dataset\\nthumbnail: \\u002fblog\\u002fassets\\u002f56_f...\"],[\"Semantic segmentation is the task of classifying each pixel in an image. You can see it as a more pr...\"],[\"Let's get started by installing the necessary dependencies. Because we're going to push our dataset ...\"],[\"```\\n\\n# 1. Create\\u002fchoose a dataset\\n\\nThe first step in any ML project is assembling a good dataset. In...\"],[\"We went ahead and captured a thousand images of sidewalks in Belgium. Collecting and labeling such a...\"],[\"### Label the images\\n\\nNow that the raw data is loaded, go to [segments.ai\\u002fhome](https:\\u002f\\u002fsegments.ai\\u002f...\"],[\"Note that creating the release can take a few seconds. You can check the releases tab on Segments.ai...\"],[\"```\\n\\nIf we inspect the features of the new dataset, we can see the image column and the correspondin...\"],[\"```\\n\\nYou can also rewrite the `convert_segmentation_bitmap` function to use batches and pass `batche...\"],[\"```\\n\\n# 2. Load and prepare the Hugging Face dataset for training\\n\\nNow that we've created a new datas...\"],[\"```\\n\\n## Image processor & data augmentation\\n\\nA SegFormer model expects the input to be of a certain ...\"],[\"```\\n\\n# 3. Fine-tune a SegFormer model\\n\\n## Load the model to fine-tune\\n\\nThe SegFormer authors define ...\"],[\"```\\n\\n## Set up the Trainer\\n\\nTo fine-tune the model on our data, we'll use Hugging Face's [Trainer AP...\"],[\"```\\n\\nNext, we'll define a function that computes the evaluation metric we want to work with. Because...\"],[\"pred_labels = logits_tensor.detach().cpu().numpy()\\n    # currently using _compute instead of compute...\"],[\"```\\n\\nFinally, we can instantiate a `Trainer` object.\\n\\n\\n```python\\nfrom transformers import Trainer\\n\\nt...\"],[\"```\\n\\n# 4. Inference\\n\\nNow comes the exciting part, using our fine-tuned model! In this section, we'll...\"],[\"```python\\nfrom transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\\n\\nproces...\"],[\"```\\n\\nNext, we'll load an image from our test dataset.\\n\\n\\n```python\\nimage = test_ds[0]['pixel_values']...\"],[\"```\\n\\nNow it's time to display the result. We'll display the result next to the ground-truth mask.\\n\\n\\u003c...\"],[\"--\\ntitle: \\\"Efficient Controllable Generation for SDXL with T2I-Adapters\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002ft2i...\"],[\"| **Model Type** | **Model Parameters** | **Storage (fp16)** |\\n| --- | --- | --- |\\n| [ControlNet-SDX...\"],[\"Compared to previous versions of T2I-Adapter (SD-1.4\\u002f1.5), [T2I-Adapter-SDXL](https:\\u002f\\u002fgithub.com\\u002fTen...\"],[\"```\\n\\nThe generation process of the T2I-Adapter-SDXL mainly consists of the following two steps:\\n\\n1. ...\"],[\"# load pipeline\\nmodel_id = \\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\neuler_a = EulerAncestralDiscre...\"],[\"```\\n\\nThen, load an image to detect lineart:\\n\\n```python\\nurl = \\\"https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadap...\"],[\"```\\n\\n![Lineart Generated Dragon](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"],[\"\\u003cscript type=\\\"module\\\" src=\\\"https:\\u002f\\u002fgradio.s3-us-west-2.amazonaws.com\\u002f3.43.1\\u002fgradio.js\\\"\\u003e\\u003c\\u002fscript\\u003e\\n\\u003cgr...\"],[\"### Canny Guided\\n\\n![Sketch guided results](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation...\"],[\"--\\ntitle: \\\"Introduction to Graph Machine Learning\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f125_intro-to-graphml\\u002fthu...\"],[\"If you want to use your data, you must first consider its best characterisation (homogeneous\\u002fheterog...\"],[\"Working on these tasks can be done in two ways. \\n\\nWhen you want to predict the evolution of a specif...\"],[\"But what does this mean? If you have a sentence and shuffle its words, you create a new sentence. If...\"],[\"## Graph representations through ML\\n\\nThe usual process to work on graphs with machine learning is fi...\"],[\"**Node-level** features can give information about importance (how important is this node for the gr...\"],[\"### Walk-based approaches\\n\\n[**Walk-based approaches**](https:\\u002f\\u002fen.wikipedia.org\\u002fwiki\\u002fRandom_walk) us...\"],[\"Typical neural networks, such as RNNs or CNNs are not permutation invariant. A new architecture, the...\"],[\"**Choosing an aggregation**: Some aggregation techniques (notably mean\\u002fmax pooling) can encounter fa...\"],[\"Here are some interesting methods which got state-of-the-art results or close on one of the hardest ...\"],[\"- [*Graph Transformer for Graph-to-Sequence Learning*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1911.07470) (Cai and La...\"],[\"The most recent approach is [*Pure Transformers are Powerful Graph Learners*](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f...\"],[\"# Further resources\\n\\nIf you want to delve deeper, you can look at some of these courses:\\n\\n- Academic...\"],[\"If you need quality benchmarks you can check out:\\n\\n- [OGB, the Open Graph Benchmark](https:\\u002f\\u002fogb.sta...\"],[\"### External images attribution\\nEmojis in the thumbnail come from Openmoji (CC-BY-SA 4.0), the Graph...\"],[\"--\\ntitle: \\\"Transformer-based Encoder-Decoder Models\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f05_encoder_decoder\\u002fthum...\"],[\"```\\n\\nThe *transformer-based* encoder-decoder model was introduced by Vaswani\\net al. in the famous [A...\"],[\"Transformer-based encoder-decoder models are the result of years of\\nresearch on _representation lear...\"],[\"Each part builds upon the previous part, but can also be read on its\\nown.\\n\\n## **Background**\\n\\nTasks ...\"],[\"Using a DNN model \\\\\\\\({}^2\\\\\\\\) to solve sequence-to-sequence problems would\\ntherefore mean that the nu...\"],[\"Then, the decoder\\\\'s hidden state is initialized with the input encoding\\nand during inference, the d...\"],[\"In computational terms, the model sequentially maps the previous inner\\nhidden state \\\\\\\\(\\\\mathbf{c}_{i...\"],[\"For more detail on the logit vector and the resulting probability\\ndistribution, please see footnote ...\"],[\"Given such a decoding method, during inference, the next input vector\\n\\\\\\\\(\\\\mathbf{y}_i\\\\\\\\) can then be...\"],[\"![](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder_decoder\\u002frnn_...\"],[\"The English sentence \\\\\\\"I want to buy a car\\\\\\\", represented by\\n\\\\\\\\(\\\\mathbf{x}_1 = \\\\text{I}\\\\\\\\), \\\\\\\\(\\\\math...\"],[\"encoder\\\\'s target vector. The encoder RNN then processes the rest of the\\ninput sentence \\\\\\\\(\\\\text{wan...\"],[\"To generate the first target vector, the decoder is fed the \\\\\\\\(\\\\text{BOS}\\\\\\\\)\\nvector, illustrated as ...\"],[\"$$ p_{\\\\theta_{\\\\text{enc}}, \\\\theta_{\\\\text{dec}}}(\\\\mathbf{Y}_{1:m} | \\\\mathbf{X}_{1:n}) = \\\\prod_{i=1}^{...\"],[\"Nevertheless, RNN-based encoder-decoder models have two pitfalls. First,\\nRNNs suffer from the vanish...\"],[\"\\\\\\\\({}^4\\\\\\\\) A neural network can define a probability distribution over all\\nwords, *i.e.* \\\\\\\\(p(\\\\mathb...\"],[\"\\\\\\\\({}^6\\\\\\\\) [Sutskever et al. (2014)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f1409.3215)\\nreverses the order of the inpu...\"],[\"As a reminder, to solve a *sequence-to-sequence* problem, we need to\\nfind a mapping of an input sequ...\"],[\"$$ p_{\\\\theta_{dec}}(\\\\mathbf{Y}_{1:n} | \\\\mathbf{\\\\overline{X}}_{1:n}).$$\\n\\nBy Bayes\\\\' rule, this distri...\"],[\"The transformer-based decoder hereby maps the sequence of encoded hidden\\nstates \\\\\\\\(\\\\mathbf{\\\\overline...\"],[\"Let\\\\'s visualize the complete process of *auto-regressive* generation of\\n*transformer-based* encoder...\"],[\"To begin with, the encoder processes the complete input sequence\\n\\\\\\\\(\\\\mathbf{X}_{1:7}\\\\\\\\) = \\\\\\\"I want t...\"],[\"Next, the first target vector \\\\\\\\(\\\\mathbf{y}_1\\\\\\\\) = \\\\\\\\(\\\\text{Ich}\\\\\\\\) is sampled\\nfrom the distribution...\"],[\"![texte du\\nlien](https:\\u002f\\u002fraw.githubusercontent.com\\u002fpatrickvonplaten\\u002fscientific_images\\u002fmaster\\u002fencoder...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    \\u003cpad\\u003e Ich will ein Auto kaufen...\"],[\"```\\n\\nCalling `.generate()` does many things under-the-hood. First, it passes\\nthe `input_ids` to the ...\"],[\"Great, now that we have gotten a general overview of how\\n*transformer-based* encoder-decoder models ...\"],[\"## **Encoder**\\n\\nAs mentioned in the previous section, the *transformer-based* encoder\\nmaps the input...\"],[\"Let\\\\'s visualize how the encoder processes the input sequence \\\\\\\"I want\\nto buy a car EOS\\\\\\\" to a conte...\"],[\"As can be seen each output vector of the self-attention layer\\n\\\\\\\\(\\\\mathbf{x''}_i, \\\\forall i \\\\in \\\\{1, ...\"],[\"$$ \\\\mathbf{q}_i = \\\\mathbf{W}_q \\\\mathbf{x'}_i,$$\\n$$ \\\\mathbf{v}_i = \\\\mathbf{W}_v \\\\mathbf{x'}_i,$$\\n$$ \\\\...\"],[\"Note, that the **same** weight matrices are applied to each input vector\\n\\\\\\\\(\\\\mathbf{x}_i, \\\\forall i ...\"],[\"illustrated in the equation below. For a complete description of the\\nself-attention layer, the reade...\"],[\"Alright, this sounds quite complicated. Let\\\\'s illustrate the\\nbi-directional self-attention layer fo...\"],[\"On the left, the previously illustrated second encoder block is shown\\nagain and on the right, an in ...\"],[\"(shown in dark green on the right). The whole equation is illustrated in\\nthe upper part of the box o...\"],[\"To further understand the implications of the bi-directional\\nself-attention layer, let\\\\'s assume the...\"],[\"$$\\\\mathbf{X''}_{1:n} = \\\\mathbf{V}_{1:n} \\\\text{Softmax}(\\\\mathbf{Q}_{1:n}^\\\\intercal \\\\mathbf{K}_{1:n}) ...\"],[\"\\\\\\\\({}^1\\\\\\\\) An in-detail explanation of the role the feed-forward layers play\\nin transformer-based mo...\"],[\"# pass input_ids to encoder\\nencoder_hidden_states = model.base_model.encoder(input_ids, return_dict=...\"],[\"```\\n\\n_Outputs:_\\n```\\n    Length of input embeddings 7. Length of encoder_hidden_states 7\\n    Is encod...\"],[\"```\\n\\nWe compare the length of the input word embeddings, *i.e.*\\n`embeddings(input_ids)` correspondin...\"],[\"On a side-note, _autoencoding_ models, such as BERT, have the exact same\\narchitecture as _transforme...\"],[\"Let\\\\'s first understand how the transformer-based decoder defines a\\nprobability distribution. The tr...\"],[\"respectively. The \\\\\\\"LM head\\\\\\\" is often tied to the transpose of the word\\nembedding matrix, *i.e.*\\n\\\\\\\\...\"],[\"Putting it all together, in order to model the conditional distribution\\nof a target vector sequence ...\"],[\"In contrast to transformer-based encoders, in transformer-based\\ndecoders, the encoded output vector ...\"],[\"We can see that the decoder maps the input \\\\\\\\(\\\\mathbf{Y}_{0:5}\\\\\\\\) \\\\\\\"BOS\\\\\\\",\\n\\\\\\\"Ich\\\\\\\", \\\\\\\"will\\\\\\\", \\\\\\\"ein\\\\...\"],[\"can therefore be computed as the following product:\\n\\n$$ p_{\\\\theta_{dec}}(\\\\text{Ich} | \\\\text{BOS}, \\\\m...\"],[\"As in bi-directional self-attention, in uni-directional self-attention,\\nthe query vectors \\\\\\\\(\\\\mathbf...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(0:i\\\\\\\\) instead\\nof \\\\\\\\(0: m-1\\\\\\\\) which wo...\"],[\"So why is it important that we use uni-directional self-attention in the\\ndecoder instead of bi-direc...\"],[\"This is obviously disadvantageous as the transformer-based decoder would\\nnever learn to predict the ...\"],[\"Great! Now we can move to the layer that connects the encoder and\\ndecoder - the *cross-attention* me...\"],[\"Note that the index range of the key and value vectors is \\\\\\\\(1:n\\\\\\\\)\\ncorresponding to the number of c...\"],[\"So intuitively, what happens here exactly? Each output vector\\n\\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\) is a weighted su...\"],[\"Cool! Now we can see how this architecture nicely conditions each output\\nvector \\\\\\\\(\\\\mathbf{y'''}_i\\\\\\\\...\"],[\"To verify our theoretical understanding, let\\\\'s continue our code\\nexample from the encoder section a...\"],[\"# create token ids for encoder input\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"pt...\"],[\"# compare values of word embedding of \\\"I\\\" for input_ids and perturbed input_ids\\nprint(\\\"Is encoding f...\"],[\"```\\n\\n_Output:_\\n\\n```\\n    Shape of decoder input vectors torch.Size([1, 5, 512]). Shape of decoder log...\"],[\"```\\n\\nWe compare the output shape of the decoder input word embeddings, *i.e.*\\n`embeddings(decoder_in...\"],[\"On a final side-note, _auto-regressive_ models, such as GPT2, have the\\nsame architecture as _transfo...\"],[\"# create ids of encoded input vectors\\ninput_ids = tokenizer(\\\"I want to buy a car\\\", return_tensors=\\\"p...\"],[\"# sample last token with highest prob again\\nnext_decoder_input_ids = torch.argmax(lm_logits[:, -1:],...\"],[\"```\\n\\n_Outputs:_\\n\\n```\\n    Generated so far: Ich will ein\\n```\\n\\nIn this code example, we show exactly w...\"],[\"--\\ntitle: Block Sparse Matrices for Smaller and Faster Language Models\\nthumbnail: \\u002fblog\\u002fassets\\u002f04_py...\"],[\"By itself, or even better combined with other methods like\\n[distillation](https:\\u002f\\u002fmedium.com\\u002fhugging...\"],[\"```\\n\\nThe extension also provides a `BlockSparseModelPatcher` that allows to modify an existing model...\"],[\"But the more important point is that the performance gain of using sparse matrices grows with the sp...\"],[\"--\\ntitle: \\\"Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)\\\"\\nthumbnail: \\u002fb...\"],[\"Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series...\"],[\"|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |\\n|:-----------------:|:---------------...\"],[\"### Decomposition Layer\\nDecomposition has long been a popular method in time series analysis, but it...\"],[\"Autoformer incorporates a decomposition block as an inner operation of the model, as presented in th...\"],[\"def forward(self, x):\\n        \\\"\\\"\\\"Input shape: Batch x Time x EMBED_DIM\\\"\\\"\\\"\\n        # padding on the b...\"],[\"```\\n\\nAs you can see, the implementation is quite simple and can be used in other models, as we will ...\"],[\"In theory, given a time lag \\\\\\\\(\\\\tau\\\\\\\\), _autocorrelation_ for a single discrete variable \\\\\\\\(y\\\\\\\\) is ...\"],[\"Now, we are ready to see the code in PyTorch: \\n\\n```python\\nimport torch \\n\\ndef autocorrelation(query_s...\"],[\"```\\n\\nQuite simple! üòé Please be aware that this is only a partial implementation of `autocorrelation(...\"],[\"It can be summarized with the following equations:\\n\\n$$\\n\\\\tau_1, \\\\tau_2, ... \\\\tau_k = \\\\textrm{arg Top-...\"],[\"Now, we are ready to see the final code:\\n\\n```python\\nimport torch\\nimport math\\n\\ndef time_delay_aggrega...\"],[\"# apply softmax on the channel dim\\n    top_k_autocorrelations = torch.softmax(top_k_autocorrelations...\"],[\"```\\n\\nWe did it! The Autoformer model is [now available](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmai...\"],[\"```\\n\\nIn the probabilistic setting one can project the context length arrays to  `prediction-length *...\"],[\"```\\n\\nThe transformers models are all relatively small with:\\n\\n```python\\nencoder_layers=2\\ndecoder_laye...\"],[\"```\\n\\nLet's visualize a time series in the dataset and plot the train\\u002ftest split:\\n\\n```python\\nimport m...\"],[\"```\\n\\n## Define Transformations\\n\\nNext, we define the transformations for the data, in particular for ...\"],[\"```\\n\\n## Define `InstanceSplitter`\\n\\nFor training\\u002fvalidation\\u002ftesting we next create an `InstanceSplitt...\"],[\"```\\n\\n## Create PyTorch DataLoaders\\n\\nNext, it's time to create PyTorch DataLoaders, which allow us to...\"],[\"return as_stacked_batches(\\n        training_instances,\\n        batch_size=batch_size,\\n        shuffl...\"],[\"if config.num_static_real_features \\u003e 0:\\n        PREDICTION_INPUT_NAMES.append(\\\"static_real_features\\\"...\"],[\"```\\n\\n## Evaluate on Autoformer\\n\\nWe have already pre-trained an Autoformer model on this dataset, so ...\"],[\"```\\n\\nThe model outputs a tensor of shape (`batch_size`, `number of samples`, `prediction length`, `i...\"],[\"```\\n\\nSo the result for the Autoformer model is:\\n\\n```python\\nprint(f\\\"Autoformer univariate MASE: {np.m...\"],[\"```\\n\\nFor example, for time-series in the test set with index `4`:\\n\\n```python\\nplot(4)\\n```\\n\\n![png](htt...\"],[\"```\\n\\nTrain the model:\\n\\n```python\\npredictor = estimator.train(\\n    training_data=train_dataset, \\n    ...\"],[\"```\\n\\nAs before, we plot the predictions from our trained DLinear model via this helper:\\n\\n```python\\nd...\"],[\"```\\n\\n![png](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fblog\\u002f148_a...\"],[\"As one can observe, the [vanilla Transformer](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002ftransformers\\u002fmodel_doc\\u002ftim...\"],[\"## Acknowledgements\\nWe express our appreciation to [Lysandre Debut](https:\\u002f\\u002fgithub.com\\u002fLysandreJik) ...\"],[\"--\\ntitle: \\\"Image search with ü§ó datasets\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f54_image_search_datasets\\u002fspaces_ima...\"],[\"First, we'll install `datasets`. Since we're going to be working with images, we'll also install [`p...\"],[\"```\\n\\nTo start, let's take a look at the image feature. We can use the wonderful [rich](https:\\u002f\\u002frich....\"],[\"\\u003cpre style=\\\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #00ffff; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e                               ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #008080; t...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e  \\u003cspan style=\\\"color: #808000; ...\"],[\"\\u003cspan style=\\\"color: #000080; text-decoration-color: #000080\\\"\\u003e‚îÇ\\u003c\\u002fspan\\u003e \\u003cspan style=\\\"color: #808000; t...\"],[\"We can see there a few different ways in which we can pass in our images. We'll come back to this in...\"],[\"There have also been projects to tag the dataset [using machine learning](https:\\u002f\\u002fblogs.bl.uk\\u002fdigita...\"],[\"```\\n\\nLet's see what we get back.\\n\\n```python\\ndataset\\n```\\n\\n```\\nDatasetDict({\\n    train: Dataset({\\n    ...\"],[\"```\\n```python\\n\\u002froot\\u002f.cache\\u002fhuggingface\\u002fdatasets\\u002fdownloads\\u002fextracted\\u002ff324a87ed7bf3a6b83b8a353096fbd95...\"],[\"```\\n\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fdataset_image.jpg\\\" alt=\\\"An example image from our da...\"],[\"```\\n\\n\\n``` python\\ndataset.push_to_hub('davanstrien\\u002fembellishments-sample', private=True)\\n```\\n\\n\\n\\u003e **No...\"],[\"```\\n## Creating embeddings üï∏ \\nWe now have a dataset with a bunch of images in it. To begin creating ...\"],[\"```\\n\\nWe now have a new column which contains the embeddings for our images. We could manually search...\"],[\"```\\n\\nWe can index into the first example this retrieves:\\n\\n``` python\\nretrieved_examples['image'][0]\\n...\"],[\"```\\n\\n``` python\\nget_image_from_text(\\\"An illustration of the sun behind a mountain\\\")\\n```\\n\\u003cimg src=\\\"as...\"],[\"```\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_search_datasets\\u002fmusical_instrument.jpg\\\"\\u003e\\n\\n\\u003cimg src=\\\"assets\\u002f54_image_s...\"],[\"However, I'm a little bit vary about making this public straightaway. Looking at the model card for ...\"],[\"suggests that 'deployment' is not a good idea. Whilst the results I got are interesting, I haven't p...\"],[\"--\\ntitle: \\\"Introducing IDEFICS: An Open Reproduction of State-of-the-art Visual Langage Model\\\"\\nthumb...\"],[\"The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is t...\"],[\"IDEFICS is an open-access reproduction of Flamingo and is comparable in performance with the origina...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ca href=\\\"https:\\u002f\\u002fatlas.nomic.ai\\u002fmap\\u002ff2fba2aa-3647-4f49-a0f3-9347daeee499\\u002fee4a...\"],[\"As part of the release process, we internally evaluated the model for potential biases by adversaria...\"],[\"## Getting Started with IDEFICS\\n\\nIDEFICS models are available on the Hugging Face Hub and supported ...\"],[\"# Generation args\\nexit_condition = processor.tokenizer(\\\"\\u003cend_of_utterance\\u003e\\\", add_special_tokens=Fals...\"],[\"--\\ntitle: \\\"Graphcore and Hugging Face Launch New Lineup of IPU-Ready Transformers\\\"\\nthumbnail: \\u002fblog\\u002f...\"],[\"### NLP\\n\\n[GPT-2](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fgpt2-medium-wikitext-103) (Generative Pre-trained ...\"],[\"[BART](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fbart-base-ipu) is a transformer encoder-encoder (seq2seq) mo...\"],[\"### Speech\\n\\n[HuBERT](https:\\u002f\\u002fhuggingface.co\\u002fGraphcore\\u002fhubert-base-ipu) (Hidden-Unit BERT) is a self-...\"],[\"Optimizing their performance in the real world requires considerable time, effort and skills that ar...\"],[\"Software also plays a vital role in unlocking the IPU‚Äôs capabilities, so naturally Optimum offers a ...\"],[\"--\\ntitle: \\\"Showcase Your Projects in Spaces using Gradio\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f28_gradio-spaces\\u002ft...\"],[\"```\\n\\nYou can play with the Story Generation model [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002fGPT-2-s...\"],[\"```\\n\\n![big-gan](assets\\u002f28_gradio-spaces\\u002fbig-gan.png)\\n\\n\\n## Serving Custom Model Checkpoints with Grad...\"],[\"```\\n\\nYou can check out the French Story Generator [here](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fmerve\\u002ffrench-...\"],[\"Some Notes on Pros of Open Science and Open Source\\n- **Pooling Resources**: Building off of one anot...\"],[\"# Cons of Closed Source\\n- **Centralization** of power.\\n- **Opacity** of subtle bias\\u002fharm issues.\\n- H...\"],[\"--\\ntitle: \\\"We are hiring interns!\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002finterns-2023\\u002fthumbnail.png\\nauthors:\\n- use...\"],[\"The following Science team positions are available:\\n\\n* [Embodied AI Internship](https:\\u002f\\u002fapply.workab...\"],[\"--\\ntitle: \\\"Announcing the Open Source AI Game Jam üéÆ\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f145_gamejam\\u002fthumbnail.p...\"],[\"From accelerated workflows to in-game features, you can harness the power of AI for texture generati...\"],[\"--\\ntitle: \\\"Ethics and Society Newsletter #3: Ethical Openness at Hugging Face\\\" \\nthumbnail: \\u002fblog\\u002fass...\"],[\"We are crafting tools and safeguards in addition to improving our documentation practices to ensure ...\"],[\"We engage directly with contributors and have addressed pressing issues. To bring this to the next l...\"],[\"**How to use the flagging function:**\\nClick on the flag icon on any Model, Dataset, Space, or Discus...\"],[\"Should a specific model be flagged as high risk by our community, we consider:\\n- Downgrading the ML ...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n \\u003cbr\\u003e\\n \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images...\"],[\"```\\n@misc{hf_ethics_soc_blog_3,\\n  author    = {Irene Solaiman and\\n               Giada Pistilli and\\n...\"],[\"--\\ntitle: \\\"Deep Learning over the Internet: Training Language Models Collaboratively\\\"\\nthumbnail: \\u002fbl...\"],[\"## Distributed Deep Learning in Open Collaborations\\n\\n### Why should we do it?\\n\\nThese days, many high...\"],[\"To a skeptical mind, it might seem that we're missing a key factor here: data transfer in distribute...\"],[\"Often, to reduce the amount of synchronization and to stabilize the learning process, we can accumul...\"],[\"Now that we have discussed the overall training procedure, there remains one more question: how do w...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"1. **Normalization:** includes all preprocessing operations on raw text data. This was the step at w...\"],[\"\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp ali...\"],[\"```\\n\\n### Dataset\\n\\nThe last thing we need to cover is the training dataset. As you probably know, the...\"],[\"```\\n\\u003cdiv style=\\\"line-height:105%;border:1px solid #F5F5F5;background-color:#F5F5F5;color: black\\\"\\u003e\\n\\u003cp...\"],[\"\\u003ciframe width=\\\"100%\\\" height=\\\"670\\\" frameborder=\\\"0\\\"\\n  src=\\\"https:\\u002f\\u002fobservablehq.com\\u002fembed\\u002f@huggingface...\"],[\"### Evaluation\\n\\nTo evaluate the performance of sahajBERT, we finetuned it on two downstream tasks in...\"],[\"At the end of training, we compared sahajBERT with three other pretrained language models: [XLM-R La...\"],[\"These models are available on the Hub as well. You can test them directly by playing with the Hosted...\"],[\"```\\n\\n#### sahajBERT-NCC\\nModel card: [https:\\u002f\\u002fhf.co\\u002fneuropark\\u002fsahajBERT-NER](https:\\u002f\\u002fhf.co\\u002fneuropark\\u002f...\"],[\"```\\n\\n## Conclusion\\n\\nIn this blog post, we have discussed the method that can enable collaborative pr...\"],[\"Below, you can see all participants of the collaborative experiment:\\n\\n\\u003ciframe width=\\\"100%\\\" height=\\\"3...\"],[\"--\\ntitle: \\\"Incredibly Fast BLOOM Inference with DeepSpeed and Accelerate\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002fbl...\"],[\"## Benchmarks\\n\\nWithout any further delay let's show some numbers.\\n\\nFor the sake of consistency, unle...\"],[\"```\\nGenerate args {'max_length': 100, 'do_sample': False}...\"],[\"```\\nThe input prompt is comprised of just a few tokens. The previous token caching is on as well, as...\"],[\"Here is the throughput in msecs on 8x80GB GPUs:\\n\\n| project      \\\\ bs |      1 |     8 |    16 |    3...\"],[\"Let's revisit again how these numbers were calculated. To generate 100 new tokens for a batch size o...\"],[\"```\\ngit clone https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002ftransformers-bloom-inference\\ncd transformers-bloom-infe...\"],[\"```\\npython bloom-inference-scripts\\u002fbloom-accelerate-inference.py --name bigscience\\u002fbloom --batch_siz...\"],[\"```\\ndeepspeed --num_gpus 8 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\ndeepspeed --num_gpus 4 bloom-inference-scripts\\u002fbloom-ds-inference.py --name microsoft\\u002fbloom-deep...\"],[\"```\\npip install deepspeed\\n```\\n\\n\\n### Run\\n\\nNote that the script currently runs the same inputs on all ...\"],[\"```\\n\\nmake sure to adjust `\\u002fpath\\u002fto\\u002fnvme_offload` to somewhere you have ~400GB of free memory on a fa...\"],[\"--\\ntitle: \\\"Summer at Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f27_summer_at_huggingface\\u002fsummer_intro.gif...\"],[\"Spaces lets you [set up secrets](\\u002fdocs\\u002fhub\\u002fspaces-overview#managing-secrets), permits [custom requir...\"],[\"![Image of a TensorBoard Instance](assets\\u002f27_summer_at_huggingface\\u002ftensorboard.png)\\n\\n### Metrics\\n\\nIn...\"],[\"The Hub has 18 widgets that allow users to try out models directly in the browser.\\n\\nWith our latest ...\"],[\"![Button to upload a file](assets\\u002f27_summer_at_huggingface\\u002fupload_file.png)\\n\\n## Community\\n\\n### Huggi...\"],[\"We're really excited to share the work of the 3 winning teams!\\n\\n1. [Dall-e mini](https:\\u002f\\u002fhuggingface...\"],[\"## Bonus\\n\\nOn top of everything we just shared, our team has been doing lots of other things. Here ar...\"],[\"## Open Source\\n\\n### New in Transformers\\n\\nSummer has been an exciting time for ü§ó Transformers! The li...\"],[\"```\\n\\nThe last 4 releases introduced many new cool models!\\n\\n- [DETR](https:\\u002f\\u002fhuggingface.co\\u002ftransform...\"],[\"![DETR image](assets\\u002f27_summer_at_huggingface\\u002fdetr.png)\\n\\n- [ByT5](https:\\u002f\\u002fhuggingface.co\\u002ftransformer...\"],[\"![LayoutLM object detection](assets\\u002f27_summer_at_huggingface\\u002flayout.png)\\n\\n- [BEiT](https:\\u002f\\u002fhuggingfa...\"],[\"![Untitled](assets\\u002f27_summer_at_huggingface\\u002fstreaming.png)\\n\\nWhat are the new datasets highlights? Mi...\"],[\"![spaCy NER example](assets\\u002f27_summer_at_huggingface\\u002fspacy_ner.jpeg)\\n\\nAnother exciting integration i...\"],[\"### **NEW: Hardware Acceleration**\\n\\nHugging Face is [partnering with leading AI hardware accelerator...\"],[\"![Sagemaker](assets\\u002f27_summer_at_huggingface\\u002fsagemaker.png)\\n\\n### **NEW: AutoNLP In Your Browser**\\n\\nW...\"],[\"**Hugging Face** + **Zapier Demo**\\n\\n20,000+ Machine Learning models connected to 3,000+ apps? ü§Ø  By ...\"],[\"**Few-shot learning in practice**\\n\\nWe wrote a [blog post](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002ffew-shot-learn...\"],[\"In June, we shared the result of our collaboration with the Yandex research team: [DeDLOC](https:\\u002f\\u002fa...\"],[\"![Prompt](assets\\u002f27_summer_at_huggingface\\u002fprompt.png)\\n\\n\\nWe're looking forward to EMNLP this year whe...\"],[\"--\\ntitle: \\\"Model Cards\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f121_model-cards\\u002fthumbnail.png\\nauthors:\\n- user: Ezi\\n...\"],[\"4) A [User Study](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhub\\u002fmodel-cards-user-studies) on model card usage at H...\"],[\"## Our Work\\n\\nOur work presents a view of where model cards stand right now and where they could go i...\"],[\"As ML continues to be more intertwined with different domains, collaborative and open-source ML proc...\"],[\"* The Hugging Face ecosystem will continue to advance methods that streamline Model Card creation [t...\"],[\"--\\ntitle: \\\"Introducing RWKV - An RNN with the advantages of a transformer\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f...\"],[\"You can get involved by joining the [official discord channel](https:\\u002f\\u002fdiscord.gg\\u002fqt9egFA7ve) and le...\"],[\"Because RNNs use the same weights to compute predictions at every step, they struggle to memorize in...\"],[\"During training, Transformer architecture has several advantages over traditional RNNs and CNNs. One...\"],[\"RNNs natively support very long context lengths - only limited by the context length seen in trainin...\"],[\"To gain a more comprehensive understanding of the attention layer, we recommend to delve into the de...\"],[\"All the HF converted models are available on Hugging Face Hub, in the [`RWKV` organization](https:\\u002f\\u002f...\"],[\"```\\n\\nOr you can run and start from the snippet below:\\n\\n```python\\nimport torch\\nfrom transformers impo...\"],[\"```\\n\\n### Use the raven models (chat models)\\n\\nYou can prompt the chat model in the alpaca style, here...\"],[\"```\\n\\nAccording to Bo, better instruction techniques are detailed in [this discord message (make sure...\"],[\"```\\n\\n## Future work\\n\\n### Multi-lingual RWKV\\n\\nBo is currently working on a multilingual corpus to tra...\"],[\"## Acknowledgements\\n\\nThe Hugging Face team would like to thank Bo and RWKV community for their time ...\"],[\"--\\ntitle: Stable Diffusion with üß® Diffusers\\nthumbnail: \\u002fblog\\u002fassets\\u002f98_stable_diffusion\\u002fthumbnail.pn...\"],[\"**Note**: It is highly recommended to have a basic understanding of how diffusion models work. If di...\"],[\"```\\n\\nIn this post we'll use model version [`v1-4`](https:\\u002f\\u002fhuggingface.co\\u002fCompVis\\u002fstable-diffusion-v...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_12_1.png)\\n...\"],[\"```\\n\\nThe result would look as follows\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_14_1.png)\\n...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_16_1.png)\\n\\nNote how the structure is the sam...\"],[\"```\\n\\nWe can generate multiple images for the same prompt by simply using a list with the same prompt...\"],[\"```\\n\\n![png](assets\\u002f98_stable_diffusion\\u002fstable_diffusion_26_1.png)\\n    \\n\\n## How does Stable Diffusion...\"],[\"There are three main components in latent diffusion.\\n\\n1. An autoencoder (VAE).\\n2. A [U-Net](https:\\u002f\\u002f...\"],[\"To prevent the U-Net from losing important information while downsampling, short-cut connections are...\"],[\"**Stable Diffusion during inference**\\n\\nPutting it all together, let's now take a closer look at how ...\"],[\"Theory on how the scheduler algorithm function is out-of-scope for this notebook, but in short one s...\"],[\"We can load the components by referring to the folder they were saved, using the `subfolder` argumen...\"],[\"```\\n\\nNow instead of loading the pre-defined scheduler, we load the [K-LMS scheduler](https:\\u002f\\u002fgithub....\"],[\"```\\n\\nFirst, we get the `text_embeddings` for the passed prompt. \\nThese embeddings will be used to co...\"],[\"```\\n\\nIf we examine the `latents` at this stage we'll see their shape is `torch.Size([1, 4, 64, 64])`...\"],[\"```\\n\\nWe now use the `vae` to decode the generated `latents` back into the image.\\n\\n\\n```python\\n# scale...\"],[\"```\\n@article{patil2022stable,\\n  author = {Patil, Suraj and Cuenca, Pedro and Lambert, Nathan and von...\"],[\"--\\ntitle: 'Deploy Hugging Face models easily with Amazon SageMaker'\\nthumbnail: \\u002fblog\\u002fassets\\u002f17_the_p...\"],[\"```\\n\\n\\nThat's it! üöÄ\\n\\nTo learn more about accessing and using the new Hugging Face DLCs with the Amazo...\"],[\"## **Samples\\u002fDocumentation**\\n\\n- [Hugging Face documentation for Amazon SageMaker](https:\\u002f\\u002fhuggingfac...\"],[\"In addition to the zero-code deployment, the Inference Toolkit supports \\\"bring your own code\\\" method...\"],[\"```\\n\\n# **Getting started üß≠**\\n\\nIn this guide we will use the new Hugging Face Inference DLCs and Amaz...\"],[\"```\\n\\n---\\n\\n## **Deploy a trained Hugging Face Transformer model to SageMaker for inference**\\n\\nThere a...\"],[\"```\\n\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request, we can delete the endpoint again with:\\n\\n\\n```python\\n# delete endpoint\\n...\"],[\"```\\n\\nAfter we run our request we can delete the endpoint again with.\\n\\n\\n```python\\n# delete endpoint\\np...\"],[\"```\\n\\n---\\n\\n# **FAQ üéØ**\\n\\nYou can find the complete [Frequently Asked Questions](https:\\u002f\\u002fhuggingface.co...\"],[\"_Q: Do I have to use the SageMaker Python SDK to use the Hugging Face Deep Learning Containers (DLCs...\"],[\"_Q: How is my data and code secured by Amazon SageMaker?_\\n\\nA: Amazon SageMaker provides numerous sec...\"],[\"A: AWS Technical Support tiers are available from AWS and cover development and production issues fo...\"],[\"--\\ntitle: \\\"Introducing Prodigy-HF: a direct integration with Hugging Face\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f1...\"],[\"\\u003cfigure\\u003e\\n    \\u003cdiv style=\\\"background-color: #eee; padding-top: 8px; padding-bottom: 8px;\\\"\\u003e\\n        \\u003ci...\"],[\"```\\npython -m prodigy hf.train.ner fashion-train,eval:fashion-eval path\\u002fto\\u002fmodel-out --model \\\"distil...\"],[\"```\\npython -m prodigy hf.upload \\u003cdataset_name\\u003e \\u003cusername\\u003e\\u002f\\u003crepo_name\\u003e\\n```\\n\\nWe're particularly fond o...\"],[\"--\\ntitle: \\\"How to Install and Use the Hugging Face Unity API\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f124_ml-for-gam...\"],[\"6. Enter your API key. Your API key can be created in your [Hugging Face account settings](https:\\u002f\\u002fh...\"],[\"```\\nusing HuggingFace.API;\\n\\n\\u002f* other code *\\u002f\\n\\n\\u002f\\u002f Make a call to the API\\nvoid Query() {\\n    string in...\"],[\"```\\n\\n## Supported Tasks and Custom Models\\n\\nThe Hugging Face Unity API also currently supports the fo...\"],[\"--\\ntitle: \\\"Proximal Policy Optimization (PPO)\\\"\\nthumbnail: \\u002fblog\\u002fassets\\u002f93_deep_rl_ppo\\u002fthumbnail.png\\n...\"],[\"- *An Actor*¬†that controls¬†**how our agent behaves**¬†(policy-based method).\\n- *A Critic*¬†that measur...\"],[\"- [The intuition behind PPO](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#the-intuition-behind-ppo)\\n- [In...\"],[\"- [Let's code our PPO Agent](https:\\u002f\\u002fhuggingface.co\\u002fblog\\u002fdeep-rl-ppo#lets-code-our-ppo-agent)\\n  \\n## ...\"],[\"The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability o...\"],[\"However, the problem comes from the step size:\\n- Too small,¬†**the training process was too slow**\\n- ...\"],[\"So this probability ratio is an **easy way to estimate the divergence between old and current policy...\"],[\"To do that, we have two solutions:\\n\\n- *TRPO (Trust Region Policy Optimization)*¬†uses KL divergence c...\"],[\"\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n  \\u003cimg src=\\\"assets\\u002f93_deep_rl_ppo\\u002frecap.jpg\\\" alt...\"],[\"Since the ratio is between intervals,¬†**we can decrease the probability that our policy takes that a...\"],[\"### Case 5 and 6: the ratio is above the range\\n\\u003cfigure class=\\\"image table text-center m-0 w-full\\\"\\u003e\\n ...\"],[\"**You might wonder why, when the minimum is the clipped ratio, the gradient is 0.** When the ratio i...\"],[\"So, to be able to code it, we're going to use two resources:\\n- A tutorial made by [Costa Huang](http...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fgiphy.com\\u002fembed\\u002fpynZagVcYxVUk\\\" width=\\\"480\\\" height=\\\"480\\\" frameBorder=\\\"0\\\" class=\\\"...\"],[\"And don't forget to share with your friends who want to learn ü§ó!\\n\\nFinally, with your feedback, we wa...\"],[\"--\\ntitle: \\\"Very Large Language Models and How to Evaluate Them\\\" \\nthumbnail: \\u002fblog\\u002fassets\\u002f106_zero_sh...\"],[\"We‚Äôve upgraded the AutoTrain infrastructure for this project so that large models can be evaluated f...\"],[\"## Case study: Zero-shot evaluation on the WinoBias task\\n\\nThe [WinoBias](https:\\u002f\\u002fgithub.com\\u002fuclanlp\\u002f...\"],[\"![Winobias](.\\u002fassets\\u002f106_zero_shot_eval_on_the_hub\\u002fwinobias.png)\\n\\n## Enabling better research tools ...\"],[\"## Send us feedback!\\n\\nAt Hugging Face, we‚Äôre excited to continue democratizing access to state-of-th...\"]],\"hovertemplate\":\"source=blog\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"blog, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"blog, circle\",\"showlegend\":true,\"x\":[-5.06501,-3.9430096,-5.3746543,-5.6971,-3.1889071,-4.8797903,-4.2164407,-1.5709503,-1.0435987,-1.1274942,-1.0741613,-0.41841352,0.5464512,-1.0831889,-5.5681143,1.7935231,1.8750281,1.9409935,-5.2010946,-2.9873104,2.959713,3.104402,3.1092374,3.0869446,3.1459672,2.8431423,-2.9196377,-2.8846395,-2.7994916,-2.8443859,-2.7875495,-3.0110607,-3.1602397,-4.7145643,-4.630697,-4.853565,-4.7716274,-4.5025973,-4.951423,-6.502664,-6.714234,-4.9183636,-2.3683023,-2.3474114,-2.4764469,-10.151987,-10.440312,-10.249,-10.425817,-10.23673,-2.77364,-2.9693823,-2.502888,-2.5927649,-2.5662434,-2.8245113,-2.3622348,-2.4898858,-2.6416218,-3.6851282,-2.434176,-2.1321502,-2.1296034,-2.323948,-2.3404,-2.2609298,-2.1523302,-2.4306722,-2.13381,-2.1457064,-2.0976186,-2.0322368,-2.3055625,-2.593171,-2.5185552,-1.9917667,-3.6236103,-2.6298168,-2.7067366,-3.5367486,-4.1538568,-2.7415547,-2.4255898,-4.0963154,-5.4350805,-5.1970725,-5.2790403,-5.4659705,-5.4063315,-5.3320966,-5.285488,-5.2293444,-7.6627007,-7.7724853,-7.9552126,-7.5585046,-7.538955,-7.2612967,-7.540102,-7.665368,-7.5684743,-7.128679,-7.583806,-7.976658,-7.753683,-7.608735,-7.3947015,-8.166679,-8.102982,-6.975378,-8.039467,-7.9914274,-7.4494834,-8.169809,-8.199729,-6.808864,-6.758324,-7.2811933,-7.400898,-7.7168417,-7.557974,-7.7099013,-4.0998588,-4.241038,-3.0355155,-2.7637038,-2.591669,-2.8923976,-2.6512244,-2.6611907,-2.9376476,-6.453436,-5.8131385,-6.2009873,-4.453248,-6.819787,-5.484704,-6.689999,-5.3372364,-6.412384,-6.436256,-6.866972,-6.7726436,-6.7184653,-7.050806,-6.9691586,-6.956312,-6.4589295,-6.6244774,-5.9225006,0.69156307,1.3757244,-0.54179376,2.1027443,-0.19285561,-0.20519911,-0.5382591,-1.9110477,-5.551412,0.07179904,-4.884823,-3.614726,-5.9653354,-2.8410828,-2.1073713,-0.9733571,-5.4409323,-2.8784685,-4.2183704,-5.4231935,-3.4447074,-3.4621096,-4.4915113,-4.7269053,-2.7651358,-9.292723,-5.7894974,-4.810505,-6.627724,-1.1822354,-1.0815666,0.01840167,0.52961284,-7.3118963,2.1926088,0.8748017,1.7031147,1.5928036,-4.855129,-2.8070133,-3.0440862,-2.689824,-3.1091723,-5.904142,-5.9989142,-5.6743226,-0.28135997,-5.6484647,0.76999164,-5.091855,-5.5347466,-2.7857912,1.9603316,-3.9935935,-3.1363459,-4.8895545,-5.3080416,-3.105238,-4.753895,-3.0316172,-3.539506,-2.9560826,-2.3216507,-6.252649,-6.7483163,3.0884738,-1.9334652,-3.2241254,-4.2255516,-3.5592577,-5.9697976,-3.2982652,1.428676,-3.1646,-2.5390332,-6.1983714,-6.224027,-6.0490074,-6.0956817,-6.0207467,-5.92896,-5.7522917,-5.9906178,-5.3939376,-5.8239985,-5.543874,-6.3486276,-6.183098,-5.995422,-6.1476016,-3.9844859,1.4906754,1.9071913,1.7842888,-6.089297,4.170664,-5.148418,-3.06706,-3.5787354,-3.3531747,-5.845209,-5.2312536,-5.6682777,-7.4406695,-8.237442,-4.2943645,-7.7639284,-8.143374,-7.9506054,-8.083801,-7.092238,-7.8599186,-8.016368,-7.94076,-7.507436,-8.300106,-7.2726607,-8.061096,-7.6019363,-7.079918,-8.362574,-7.5582433,-7.3534255,-7.429249,-8.124348,-7.7750635,-7.3090763,-2.9125233,-3.2322698,-3.452633,-1.8838199,-3.4254773,-3.1908734,-3.6976705,-2.773395,-7.887704,-8.033164,-8.194453,-4.595742,-7.850926,-8.128283,-5.1678715,-7.7004676,-8.0843935,-8.112112,-8.154279,-8.079639,-5.877506,-7.802468,-8.022467,1.8287965,2.3191328,-1.1440136,1.858249,0.38606486,1.3472767,3.0102634,2.8842726,-2.0571816,3.0435617,2.9472237,2.7538846,-5.759888,-5.9517536,-5.986254,-5.6269627,-5.732579,-6.025628,-7.3307824,12.5891695,12.480289,-5.6811337,-5.757351,-4.1340814,-2.0436466,-1.3336259,-1.9257699,-1.598239,-2.0018427,-1.6011112,-2.007995,-2.1887016,-1.9308783,-1.0510076,-1.713183,-2.0596805,-4.223179,-3.6472456,-6.245898,-6.1266956,-6.3228126,-6.404392,-7.138734,-6.5463457,-6.8889656,-7.249143,-6.8004146,-7.04989,-6.744441,-7.758319,-6.9444065,-6.9913287,-6.2175574,-6.5022745,-7.0406327,-6.728378,-7.074411,-7.057358,-7.138679,-6.1597743,-6.4370794,-6.639564,-6.125397,-7.0578866,-7.3581867,-7.258003,-7.341948,-6.950414,-7.203451,-6.684539,-7.132451,-7.289789,-6.984074,-7.2815814,-7.204585,-6.98274,-6.768832,-6.239494,-5.9608903,-5.8230405,-6.0951614,-5.9112887,-3.7180305,-3.17391,-3.866177,-4.39678,-2.8738663,-3.483652,-2.7942896,-3.5289545,-3.188956,-7.115832,-2.9079447,-4.87049,-4.8886127,-4.8505745,-4.9893517,-4.855951,-4.935804,-4.9165297,-4.831108,-4.8004627,-4.8556404,-4.70162,-4.819403,-4.8424497,-4.7874994,-5.253271,-4.8312187,-4.4983673,-4.582996,-4.5711684,-4.5182304,-3.1719851,-1.6499958,-2.9677453,-5.015099,-4.766438,-5.0180078,-4.711246,-5.981137,-4.802204,-4.20753,-4.7323284,-2.625284,-5.88712,-4.1480837,-2.8421118,0.04301375,-3.5730977,-5.0246534,-4.9487305,-3.6861677,-3.9174576,-4.1007056,-3.2807696,-3.3378882,-3.9818778,-2.9779496,-2.7802742,-2.6951,-2.7618773,-2.6941135,-6.732715,-3.9516656,-3.794187,-5.8006897,-5.6713104,-2.7234504,-2.5934753,-1.7011055,0.085137464,0.9262628,1.3475263,-0.20125069,-0.55346596,-0.57513654,-2.429829,-0.9872531,-3.077978,-3.075145,-3.2582283,-3.0874686,-3.059564,-2.8821535,2.5560815,-3.0164392,-1.7172554,-2.2674098,-3.0106838,-3.0835123,-1.2307957,-3.2101493,-1.7906977,-2.58113,0.024053046,0.31575626,-0.547609,0.63712317,-5.6926384,-5.659273,-1.7086227,-2.001453,-2.020701,-1.5652721,-2.7461557,-4.1896844,-6.037098,2.9142375,2.694759,2.8235154,2.194527,2.680424,3.0176215,-6.322273,-4.1865544,-1.4585016,-4.3644757,-5.117007,-4.0759344,-4.4829497,-4.439206,-4.1565595,-2.0163162,-3.1243544,-7.0525985,-4.989915,-2.8586743,-1.5810004,0.22847353,0.6466169,1.5950334,3.2500007,0.6397507,0.67087495,-5.0795817,0.6678777,0.27140197,-2.9076324,-2.1936128,-0.38761127,-2.4743009,-6.306684,-6.1845937,-5.972788,-5.549298,-5.5891156,-5.5860696,-5.4715796,-7.3941417,-7.304941,-7.318615,-7.1917963,-7.582889,-7.721749,-7.567524,-7.5217867,-7.4954906,-5.5226865,-5.6580653,-5.5885787,-5.441978,-5.7402115,-5.385581,-6.7562265,-7.014675,-7.0367103,-7.0342927,-7.046084,-7.1623187,-6.871957,-7.195567,-3.141237,-3.0551174,-2.901214,-2.1884441,-3.01557,-2.4642777,-1.9041996,-2.9266956,-3.0103238,-5.5944014,1.31246,-3.2879484,1.341842,-7.834678,-3.624628,-3.2539773,-2.804686,-2.4666944,-2.4441032,-3.9935257,-4.0124764,1.6667229,-4.061662,-4.028857,-3.7335558,0.5182321,-0.70741564,-0.08465858,0.6777549,-1.662951,-2.5186565,-2.3870497,-3.6862354,-3.8280382,-2.225127,-0.21994457,-3.6373875,-1.920678,-3.811677,-2.9475598,-2.4167805,-2.3566215,-1.9543545,-1.4445094,-1.64321,-2.290859,-2.9289572,-4.577097,-7.6588182,-7.7524633,-7.823691,-8.195308,-7.754304,-7.8330526,-7.679527,-7.884487,-7.7128487,-7.642381,-7.6980667,-7.3726773,-7.7111154,-7.5791388,3.719536,-5.891414,-9.413133,-9.843968,-8.681692,-9.699303,-10.44137,-10.513397,-10.541577,-10.495444,3.763865,-9.406094,-10.187532,-10.4111,-9.888558,-9.95574,-9.925573,-10.078688,-10.39508,-10.515572,-10.477571,-9.144311,-10.430662,-10.475771,-8.672078,3.1624694,-8.00518,-10.358017,-10.387677,-10.538009,-10.511667,-10.525351,-10.406659,-10.549432,-10.518573,-10.662505,-10.616707,-10.445647,-10.348164,-9.922609,-4.9915743,-5.106437,-10.189,-9.147447,-10.449672,-10.15067,-10.4068,-10.48417,-10.447742,-10.566223,-10.518818,-10.531275,-10.487885,-10.403456,-10.571685,-10.636453,-10.312473,-10.434665,-10.041836,-5.0638776,-4.9179783,-4.825103,-9.9599,-7.654872,-4.949038,-5.553397,-8.564154,-4.6428747,-3.338669,-2.4298806,-2.549439,-2.3639226,-2.386858,-2.471476,-2.4864922,-2.5652626,-2.7890296,-2.5518138,-10.560221,-10.477081,-10.687404,-10.477475,-10.602434,-10.393108,-2.50429,-2.560822,-2.5734038,-2.7230158,-2.237156,-2.1626735,-2.0878937,-2.0629637,-2.0754387,-2.6924129,-2.8489685,-2.8518064,-2.6318817,-2.8992317,-2.826441,-2.447693,-2.3384356,-7.399015,0.3522336,-3.3317943,-0.34359813,4.615686,4.734659,4.7293296,4.7204037,4.749686,4.7145543,4.8141303,4.7009025,4.700171,0.10618236,0.07985929,-0.39828157,-0.35945466,1.1086364,1.8305409,-1.594659,-0.7556247,-0.89044553,-0.8485352,0.44991994,-5.0530043,-0.69848245,-5.2562094,-5.099362,-5.0041738,-5.9376273,-5.300181,-3.9338684,-4.304081,-4.8505583,-6.167942,-6.336006,-7.458619,-3.3487608,-3.1585102,12.67317,-4.675472,-3.2861822,1.0369486,-6.9908175,-7.0556927,-6.0845017,-5.5515876,-6.208087,-7.0776343,-6.4160604,-6.701737,-6.246561,-6.8330197,-6.646123,-6.0424614,-6.470111,-4.8221927,-4.31936,-3.5704367,-3.0678551,-2.745492,-5.3179045,-6.9797754,-7.1376996,-7.998273,-5.2071238,-5.643298,-3.3594546,-6.476733,-5.3460155,-3.581432,-4.2927322,-5.221657,-5.0005293,-1.3373379,-1.2954028,-5.2205772,-1.9945407,-1.6114563,-1.571857,-1.478054,-1.4210867,-1.4032379,-1.5226718,-1.4861571,-1.0652118,-5.8521256,1.1840042,-3.7316697,-4.5066047,-5.4405856,-4.8823686,-5.3879957,-4.8213606,-3.9191418,-4.949179,-4.651487,-5.0716243,-4.856938,-2.3496559,-2.481467,-3.3133109,-4.541124,-5.116374,-5.198795,-5.9586177,-6.0209575,-6.122517,-6.17836,-5.670564,-5.4968476,-9.148169,-9.330562,-9.207339,-9.188846,-9.000858,-4.420867,-4.988986,-4.4465957,0.51470983,-5.586636,-5.0834327,-3.0489702,-2.7490714,-2.0403013,-1.4534484,-2.11948,-2.3013277,-1.3489591,-2.9790413,-3.029225,-3.2385192,-3.0756164,-2.9993246,-2.2706468,-1.9333154,-4.7953553,-2.1656306,-2.4481647,-3.1356559,-1.6414121,-1.2680544,-1.7072031,-1.844832,-1.3467634,-1.2481315,-0.010052925,-0.28793073,1.7518423,-1.5203501,-1.5532393,0.6953953,0.39965793,-5.740093,-2.8929667,-2.5195432,0.9360655,-4.8492203,-1.1035024,2.510781,-3.125714,-9.392772,-9.531824,-9.597388,-8.554888,-9.814372,-9.956662,-9.903238,-9.879484,-9.880746,-9.876413,-9.871161,-9.82857,-8.408804,-8.716229,-7.4614277,-6.472657,-6.2743497,-6.820771,-6.429529,-5.5160475],\"xaxis\":\"x\",\"y\":[-1.1283813,-1.3844802,-0.32152095,-2.929488,-1.1155005,-1.1575516,-0.6799781,-0.73498005,-0.96907455,-2.2292254,-1.9571428,-1.4934186,-1.8341907,-0.82559276,-1.3315606,-5.5362597,-5.743179,-5.7132,0.1709557,2.533931,-1.6919924,-2.0841072,-1.958193,-2.4445798,-2.1940255,-1.8087558,2.8587089,2.4579387,2.7645164,2.660203,2.7711225,2.60301,2.3247144,0.27755293,0.79000455,0.94375116,1.5750277,1.8754433,0.9632293,-3.3560543,-3.1641355,1.2660837,-10.747717,-10.835531,-10.976705,-4.2990065,-4.2814903,-4.147493,-4.129962,-4.2522707,-7.0929747,-6.5606966,-8.73643,-9.852135,-8.684309,-8.838964,-8.701863,-8.829965,-8.868181,-8.501172,-8.82456,-8.382971,-8.434487,-8.078125,-8.568418,-7.561568,-8.623488,-8.651073,-8.308265,-8.588214,-8.486789,-8.574561,-7.444195,-7.963683,-7.457343,-5.968851,-7.6441436,-9.417611,-9.152432,-9.5660925,-8.593023,-9.661769,-10.491069,-1.0575172,-1.3529426,-1.5133334,-1.5293062,-1.6240158,-1.7033646,-2.2023616,-1.5869088,-1.3177655,-5.7763233,-5.9283414,-5.4762425,-0.27171683,-5.738393,-0.537756,-5.906928,-6.085607,-5.9705486,-3.5233042,-0.27472842,-0.01751196,-6.160679,-5.3211784,-5.969424,-1.8305147,-1.6904031,-5.9559884,-1.7237556,-1.6922722,-6.0345564,-1.7764132,-1.8337126,-5.38967,-0.24597809,-5.2936335,-5.9895825,0.0047198148,-5.9788785,-5.774899,-1.1852283,-5.661996,-1.288296,-1.7025734,-1.5648414,-1.6132725,-1.5815716,-1.6023941,-1.1784548,-1.0284487,-0.7529715,-0.78390235,-2.0829015,-0.8391556,-0.7914628,-0.9105602,-0.61519,-1.271735,-0.5792402,-0.9462339,-0.84855014,-0.86231565,-0.81992257,-1.0469177,-0.862377,-0.56511813,-0.79043645,-0.33109704,-4.310698,-4.5104785,-4.0429673,-4.5691547,-4.051201,-4.013191,-4.1172137,-5.1579876,-5.7124705,-4.015754,-2.29607,-2.5900235,-2.9153922,-2.849953,-2.6820147,-3.6412256,-2.9437866,-4.527148,-3.570449,-3.7947645,-6.6353884,-2.9538538,-3.7747192,-5.5995154,-2.8171568,-0.4089324,-2.8099012,-2.494326,-2.105316,-0.76837426,-0.73180646,-1.4053483,-1.7612656,-5.6864796,-2.5626652,-1.8604608,-2.7499688,-2.02623,1.203921,3.9337173,3.1921065,3.0109224,3.347806,-2.2876449,-3.4245071,-2.9750726,-6.5487003,-3.3439965,-4.804332,-2.906983,-2.922542,-3.7420611,-4.222063,-5.815383,-7.185101,-5.005238,-6.0958343,-6.9962587,-5.3548784,-5.0152774,-8.035373,-7.3717184,-5.991089,-3.3872976,-3.1448843,-1.1430598,-5.8518653,-7.7909713,-9.171288,-8.274889,-4.511344,-7.770734,-4.938052,-1.8575515,-0.5628754,1.0775292,1.1676834,1.1728657,1.1175836,1.0959446,1.0238645,0.7715507,1.0980291,1.0601953,1.2142836,1.30828,0.89407307,1.0630552,0.8110814,1.1157895,-0.81035227,0.13564424,0.33839095,0.4509917,0.76071686,-0.3033437,-1.3444511,-1.3678545,-1.7732196,-0.97502625,-0.23162374,-0.49731478,0.25038436,-4.976076,-5.354721,-5.3570232,-5.9044347,-2.7402818,-6.353468,-6.4533515,-6.9414554,-6.5092807,-2.5070243,-6.153038,-5.7422814,-2.575256,-5.707562,-2.4646838,-5.8284574,-5.9168525,-2.5247672,-5.782506,-5.8880134,-6.2561245,-2.4749806,-5.8009944,-5.069387,-1.6768095,-2.2546268,-4.002181,-6.8113484,-3.75674,-3.6800804,-7.6288266,-2.1531205,-5.9745493,-6.5508833,-6.412964,-5.3997,-6.0798874,-6.35162,-6.1412535,-6.5970063,-6.4508376,-6.5975995,-6.4918528,-6.5536895,-7.122476,-6.293671,-6.4138365,0.73305625,-2.0430064,-1.192677,-1.8671688,-1.6392019,-2.0937119,-1.878078,-2.068854,3.5563908,-1.9031323,-1.9494288,-1.9073261,-1.5599716,-2.4673724,-2.5997586,-2.8337433,-2.0399458,-1.7030039,-1.8968098,1.2930266,0.8058617,-1.5601615,-1.8900722,-1.0678811,-2.4560032,-4.1843934,-2.4320228,-2.3662426,-2.1863256,-2.9463716,-2.9318385,-2.0676637,-2.3623064,-2.956366,-2.4142592,-2.6879466,-1.2917124,-1.5717119,0.07444394,0.45211115,0.4752662,0.5810656,-0.06992165,-2.2916496,-2.8665423,-5.2183146,-2.8771353,-3.7046516,-2.9218931,-0.9433445,-1.9441513,-1.922266,-4.958294,-3.0771222,-2.3533456,-2.8334088,-2.3188195,-2.321019,-1.6515046,-1.9849669,-2.1291275,-2.1713529,-1.5457042,-2.1158195,-2.59684,-2.338878,-2.146745,-2.4131994,-2.246839,-2.222307,-2.1453333,-2.2604084,-2.3315039,-2.363161,-2.4292765,-2.273122,-2.1674578,-2.6529143,-1.4092408,-1.7827983,-0.14002433,0.33554015,-1.7673728,-1.7667205,-1.7158278,-2.7506833,-2.585634,-4.8872504,-2.7171774,-4.7655525,-4.5561237,-2.8411145,-1.7380391,1.7966508,1.936952,1.9331394,2.1250498,1.8580188,1.8255438,1.9912924,1.8641081,1.7415179,1.8176091,1.8750373,1.9892625,1.7535462,1.7593842,1.3991419,1.8435136,1.6699202,1.7002773,1.6538147,1.2939955,-6.2863865,-6.6018057,-6.6449676,1.3917954,1.5230521,1.3791507,0.1180464,-1.3773274,-2.631938,-3.4870977,-2.7117379,-2.528563,-3.3262336,-2.9371164,-4.6132402,-6.4354787,-7.623451,-6.6977353,-6.472463,-6.7008047,-4.5790915,-4.2837954,-3.4277298,-4.405382,-4.3016834,-7.455921,-6.287913,-6.098618,-4.7107024,-3.7806497,-2.7938786,-2.8753986,-3.1590576,-0.74705136,-0.8910832,-2.0276103,-2.228264,-2.984276,-3.1075141,-2.1944323,-3.479894,-3.8940368,-3.9590619,-4.096734,-2.1502829,-1.2908028,5.256718,5.2111425,5.1484575,5.2122197,4.8723,5.3371,-5.03008,5.042942,4.9730244,4.375989,5.2937856,5.257081,-2.2469063,-5.024285,-7.016299,-7.959818,-2.407453,-2.351955,-2.7108111,-3.059902,-1.3432693,-2.59402,-3.1051111,-2.2635934,-2.5006797,-3.734584,-2.4291577,-0.85146284,-0.23544222,-3.8890207,-1.9495953,-2.1446903,-2.2541234,-2.0382051,-2.0643578,-0.09099073,0.13699687,-0.59768295,0.21159902,-2.9403703,0.3746669,0.39629942,0.23075783,-1.2084548,-2.653326,-2.9280882,-3.0607302,-4.242439,-2.020913,-0.7310947,-1.7973279,-1.84545,-3.4842212,-1.6198059,-1.983449,-2.0066357,-2.1756642,-2.5015192,-2.7792416,-3.1458783,-2.562727,-3.0093718,-2.3771892,-0.17167434,0.6594538,-1.594683,-3.1375449,-3.0010252,-4.860464,-3.033782,-2.4588618,-2.3390896,-2.2491221,-2.5230577,-2.293005,-2.3294697,-2.2018998,-2.3914135,-2.1423306,-2.8790796,-3.1175146,-3.1084847,-5.403397,-2.9549172,-3.0687017,0.32419246,0.6171383,0.6213661,0.62609845,0.571165,0.59348893,0.75362974,0.65350956,4.295397,4.5762258,-2.6082754,-2.755577,4.3500752,-3.936903,5.3190303,4.5206184,4.216065,-0.9173464,-5.4776263,-7.940254,-5.370777,-1.2151786,-7.2211633,-7.083639,-6.601867,-6.057118,-5.970483,1.7607701,1.615147,-4.708005,1.4884748,1.6714748,1.9957509,-5.899918,-7.032955,-6.3840666,-5.7409215,-6.7887683,-2.9589927,-6.343763,-9.010485,-8.970682,-6.047671,-4.103843,-5.459106,-6.7836432,1.5094157,3.8287446,4.54708,4.4873304,4.8777304,5.030938,4.034912,4.3355837,3.7361352,0.6902368,-0.9171808,-1.1213056,-1.1091375,-1.4792562,-1.1344795,-1.1166519,-1.1201015,-1.1495317,-1.1250308,-0.92116535,-1.168116,-1.1142571,-0.87747836,-1.3951706,-0.6818395,-1.7158332,-5.0403295,-4.78573,-4.5967464,-5.00783,-5.293261,-5.2605834,-5.3445086,-5.406826,-0.76544964,-5.3732233,-5.194852,-5.409824,-5.415922,-4.65469,-5.3066735,-5.027366,-5.2517242,-5.1791925,-5.34638,-5.276274,-5.28519,-5.3717833,-5.5501504,-1.1106297,-5.5729127,-4.884264,-4.81896,-4.8346496,-4.9320474,-4.632603,-4.6782074,-4.528623,-4.715486,-4.78326,-4.7229004,-4.93996,-4.953179,-4.723071,-6.342953,-6.533156,-5.063484,-5.0140724,-5.1830716,-5.2543135,-5.309305,-4.939908,-5.3079424,-4.9204545,-4.7037377,-4.917509,-4.9469213,-5.0186534,-4.8350716,-4.8620934,-5.0188117,-4.9295874,-5.079591,-6.313745,-6.4467587,-6.4980984,-5.105996,-4.7118073,-6.2113485,-6.38125,-5.5414166,-1.27118,-1.5309851,-2.2756996,-1.9983915,-10.690921,-10.76636,-10.503692,-10.5029745,-10.171546,-9.179464,-10.467803,-4.290355,-4.315279,-4.2847676,-4.3139725,-4.3006926,-4.2574487,-10.425421,-9.819787,-9.748794,-9.034225,-8.458178,-8.596991,-8.53024,-8.382152,-8.540687,-8.8097515,-9.309274,-9.487813,-9.179729,-7.978712,-9.199922,-10.36188,-10.726996,-1.4877359,-4.192273,-5.688734,-7.063336,-0.3608323,-0.40742388,-0.41202217,-0.32868448,-0.3338791,-0.32901534,-0.2919125,-0.34909585,-0.34965426,-4.8060226,-4.760939,-7.0791006,-7.0779862,-5.112225,-5.256602,-5.909146,-6.202329,-6.71102,-7.0031495,-3.8675709,0.7999554,-5.2840247,-0.4770381,0.636144,0.72762465,0.41083214,0.16935855,-4.2044153,-6.2428083,-0.7768091,-3.479404,-2.874139,-3.5721686,-1.0333936,-1.4645221,1.8962069,-3.8871489,-2.7858932,-2.5402393,-0.05789897,0.10058995,-0.16872023,0.18414176,0.08988861,0.3095209,0.08303495,0.46090922,0.65864897,0.92343956,0.93783265,0.49236724,0.6384923,-1.3642333,-1.470003,-1.6213161,-2.0864494,-2.1384926,-2.220611,-6.7308207,-6.724522,-8.482367,-2.666147,-0.58115697,-2.1819715,-2.647979,-2.4931653,-4.5469007,-5.246866,-1.6042577,-0.9359481,-2.4859831,-1.9890792,-6.7649765,-2.4184995,-2.4964352,-2.431342,-2.7691598,-2.4717906,-2.5289009,-2.528538,-2.6744223,-2.4366114,-0.28334785,-2.58382,-8.436566,-0.89455277,-0.79625314,0.44768137,-1.2536976,-1.1610532,1.3297266,-0.87551814,-0.70386606,-1.0811805,-1.0383865,-1.1380103,-0.7757861,-0.24408364,-0.7468236,-1.529312,-1.2791759,0.40012252,0.50205904,0.73373705,0.71063024,0.4465836,-1.121676,-3.6031766,-4.0643897,-3.999964,-3.7596924,-3.6386669,-3.0098164,-5.475595,-4.7616024,-4.8835793,-2.578052,-0.6263418,4.7187905,5.066021,5.0321045,4.3361106,4.845234,4.4708877,4.3000817,4.639894,4.4780006,4.0556436,4.5355906,4.522911,-5.2747574,5.2843113,-6.4050555,5.342062,4.721023,4.7827573,-0.78472084,-0.8678394,-0.9548124,-1.317316,-1.0739505,-0.8559144,-1.8691705,-1.5793219,-2.8628595,-0.7463769,-0.87541986,-2.2128026,-1.7420535,-0.7696417,-2.4189174,-4.9495435,-5.480142,-0.0392686,-1.2988554,-2.3333306,-0.51820225,0.1529938,0.07318323,-0.010023735,-0.3139283,0.069046885,0.027631251,0.056340285,0.06225434,0.0728353,0.059551112,0.06215736,0.08271054,-0.077722594,0.01813312,-1.4460607,-2.1926384,-2.2825675,-2.006546,-2.2377915,0.09522136],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"hat is dynamic padding? In the \\\"Batching Inputs together\\\" video, we have seen that to be able to gro...\"],[\"we can apply. The most obvious one is to pad all the elements of the dataset to the same length: the...\"],[\"sentence inside the batch. This way batches composed of short inputs will be smaller than the batch ...\"],[\"and tokenizer, we applied the tokenization to all the dataset with padding and truncation to make al...\"],[\"(usually 512) get truncated to that length. Then we pad our samples dynamically by using a data coll...\"],[\"on CPUs and GPUs, so you should apply it if you can. Remember to switch back to fixed padding howeve...\"],[\"ow to slice and dice a dataset. Most of the time, the data you work with won‚Äôt be perfectly prepared...\"],[\"shuffling. It is generally a good idea to apply shuffling to the training set so that your model doe...\"],[\"if you have to create your own test splits from raw data. To do this, you just apply the train_test_...\"],[\"most common way to do this is with the select method. This method expects a list or generator of the...\"],[\"checks whether each rows fulfills some condition or not. For example, here we've created a small lam...\"],[\"method to delete them. You can see examples of both these method here. Some datasets have nested col...\"],[\"in the dataset. For example,here we first define a lowercase_title function that simply lowercases t...\"],[\"he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library t...\"],[\"determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which ...\"],[\"even if your dataset is huge you won't get out of RAM: only the elements you request are loaded in m...\"],[\"and names for the labels. 0 stands for not equivalent and 1 for equivalent. To preprocess all the el...\"],[\"directly apply to all the splits in our dataset with the map method. As long as the function returns...\"],[\"not need to change for this. You can also use multiprocessing with the map method, check out its doc...\"],[\"efore diving in character-based tokenization, understanding why this kind of tokenization is interes...\"],[\"estimated 170,000 different words, we would need a very large vocabulary to encompass all words. Wit...\"],[\"in a language, even words unseen during the tokenizer training can still be tokenized, so out-of-voc...\"],[\"of information held in single characters, but for others like roman-based languages, the model will ...\"],[\"This tokenization, while it has some issues, has seen some very good results in the past and should ...\"],[\"Normalization and pre-tokenization[[normalization-and-pre-tokenization]]\\n\\n\\u003cCourseFloatingBanner chap...\"],[\"## Normalization[[normalization]]\\n\\n\\u003cYoutube id=\\\"4IIC2jI9CaU\\\"\\u002f\\u003e\\n\\nThe normalization step involves some...\"],[\"```\\n\\n```python out\\n\\u003cclass 'tokenizers.Tokenizer'\\u003e\\n```\\n\\nThe `normalizer` attribute of the `tokenizer`...\"],[\"```\\n\\n```python out\\n[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (...\"],[\"```\\n\\nAlso note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\\n\\nFor...\"],[\"```\\n\\nLike the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (`_`), ...\"],[\"## Algorithm overview[[algorithm-overview]]\\n\\nIn the following sections, we'll dive into the three ma...\"],[\"ow to preprocess pairs of sentences? We have seen how to tokenize single sentences and batch them to...\"],[\"duplicates; in the second, they are not. Another pair classification problem is when we want to know...\"],[\"an academic benchmark for text classification), 8 of the 10 datasets are focused on tasks using pair...\"],[\"deal with pairs of sentences: you just have to pass them as two arguments to the tokenizer. On top o...\"],[\"also added special tokens so we have a CLS token, the tokens from the first sentence, a SEP token, t...\"],[\"outputs the same length, and properly dealt with token type IDS and attention masks for the two sent...\"],[\"The Hugging Face Course\\n\\nThis repo contains the content that's used to create the **[Hugging Face co...\"],[\"| Language                                                                      | Source            ...\"],[\"|:------------------------------------------------------------------------------|:------------------...\"],[\"-------------------------------------------------------------------------|:-------------------------...\"],[\"----------------------------------------------------------------------------------------------------...\"],[\"----------------------------------------------------|...\"],[\"| [English](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fen\\u002fchapter1\\u002f1)                        | [`chapters\\u002fen`](ht...\"],[\"| [Spanish](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fes\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fes`](ht...\"],[\"| [Hebrew](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fhe\\u002fchapter1\\u002f1) (WIP)                   | [`chapters\\u002fhe`](ht...\"],[\"| [Japanese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fja\\u002fchapter1\\u002f1) (WIP)                 | [`chapters\\u002fja`](ht...\"],[\"| [Russian](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fru\\u002fchapter1\\u002f1) (WIP)                  | [`chapters\\u002fru`](ht...\"],[\"| [Vietnamese](https:\\u002f\\u002fhuggingface.co\\u002fcourse\\u002fvi\\u002fchapter1\\u002f1)               | [`chapters\\u002fvi`](https:\\u002f\\u002f...\"],[\"### Translating the course into your language\\n\\nAs part of our mission to democratise machine learnin...\"],[\"```\\n\\n**üìã Copy-paste the English files with a new language code**\\n\\nThe course files are organised und...\"],[\"```\\n\\n\\u003e üö® Make sure the `_toctree.yml` file only contains the sections that have been translated! Oth...\"],[\"```\\npip install -r requirements.txt\\nmake style\\n```\\n\\nOnce that's run, commit any changes, open a pull...\"],[\"```\\n\\nThen run the following script:\\n\\n```bash\\npython utils\\u002fgenerate_notebooks.py --output_dir nbs\\n```...\"],[\"What to do when you get an error[[what-to-do-when-you-get-an-error]]\\n\\n\\u003cCourseFloatingBanner chapter=...\"],[\"```\\n\\nor the following in your favorite terminal:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nThis will promp...\"],[\"```\\n\\nNow when you call `copy_repository_template()`, it will create a copy of the template repositor...\"],[\"```\\n\\nOh no, something seems to have gone wrong! If you're new to programming, these kind of errors c...\"],[\"```python out\\n\\\"\\\"\\\"\\nMake sure that:\\n\\n- 'lewtun\\u002fdistillbert-base-uncased-finetuned-squad-d5716d28' is a...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nüí° If you encounter an error message that is difficult to understand, just copy and paste...\"],[\"Okay, this got a hit. Now let's try to download the model again with the correct model ID:\\n\\n```pytho...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nOSError: Can't load config for 'lewtun\\u002fdistilbert-base-uncased-finetuned-squa...\"],[\"```\\n\\nInteresting -- there doesn't seem to be a *config.json* file in the repository! No wonder our `...\"],[\"```\\n\\nNow we can test if this worked by loading the model from the latest commit on the `main` branch...\"],[\"```\\n\\nWoohoo, it worked! Let's recap what you've just learned:\\n\\n- The error messages in Python are kn...\"],[\"```\\n\\nNext we need a question, so let's see if our favorite frameworks are supported:\\n\\n```python\\nques...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\n---------------------------------------------------------------------------\\nA...\"],[\"~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodeling_di...\"],[\"AttributeError: 'list' object has no attribute 'size'\\n\\\"\\\"\\\"...\"],[\"```\\n\\nOh dear, it looks like we have a bug in our code! But we're not afraid of a little debugging. Y...\"],[\"```\\n~\\u002fminiconda3\\u002fenvs\\u002fhuggingface\\u002flib\\u002fpython3.8\\u002fsite-packages\\u002ftransformers\\u002fmodels\\u002fdistilbert\\u002fmodelin...\"],[\"```\\n\\nIt looks like our code tried to call `input_ids.size()`, but this clearly won't work for a Pyth...\"],[\"```\\n\\n```python out\\n\\\"\\\"\\\"\\nQuestion: Which frameworks can I use?\\nAnswer: pytorch, tensorflow, and jax\\n\\\"\\\"...\"],[\"hat is transfer learning? The idea of Transfer Learning is to leverage the knowledge acquired by a m...\"],[\"B. When training from scratch, all the model‚Äôs weight are initialized randomly. In this example, we ...\"],[\"86% easily. This is because pretrained models are usually trained on large amounts of data that prov...\"],[\"key difference with ImageNet is that the pretraining is usually self-supervised, which means it does...\"],[\"have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished...\"],[\"labels. To be as efficient as possible, the pretrained model used should be as similar as possible t...\"],[\"from these countries. OpenAI also studied the bias in the predictions of its GPT-3 model (which was ...\"],[\"ow to batch inputs together? In this video, we will see how to batch input sequences together. In ge...\"],[\"error, because all arrays and tensors should be rectangular. One way to overcome this limit is to ma...\"],[\"used to pad the second sentence should not be picked randomly: the model has been pretrained with a ...\"],[\"should not come as a total surprise: when computing the contextual representation of each token, the...\"],[\"input IDs, with zeros and ones. Ones indicate the tokens the attention layers should consider in the...\"],[\"upercharge your Pytorch training loop with Hugging Face Accelerate. There are multiple setups on whi...\"],[\"or another and to learn a new API. All those setups are handled by the Trainer API, and there are se...\"],[\"training loop (here shown on the code of the training loop from the \\\"Raw training loop\\\" video), Acce...\"],[\"main method to remember. Accelerate handles device placement, so you don't need to put your batch on...\"],[\"this: pass along the evaluation dataloader to the accelerator.prepare method, like for training. The...\"],[\"easy API to configure your setup and launch your training script. In a terminal, run accelerate conf...\"],[\"Building your first demo[[building-your-first-demo]]\\n\\n\\u003cCourseFloatingBanner chapter={9}\\n  classNames...\"],[\"```\\n\\nLet's walk through the code above:\\n\\n- First, we define a function called `greet()`. In this cas...\"],[\"Try using this GUI right now with your own name or some other input!\\n\\nYou'll notice that in this GUI...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-hello-world-custom.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"300\\\" tit...\"],[\"```\\n\\nThis function completes prompts that you provide, and you can run it with your own input prompt...\"],[\"Natural Language Processing[[natural-language-processing]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n  ...\"],[\"## Why is it challenging?[[why-is-it-challenging]]\\n\\nComputers don't process information in the same ...\"],[\"Introduction[[introduction]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={8}\\n    classNames=\\\"absolute z-10 ri...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Sharing pretrained models[[sharing-pretrained-models]]\\n\\n{#if fw ===...\"],[\"\\u003cYoutube id=\\\"9yY3RB_GSPM\\\"\\u002f\\u003e\\n\\nWe encourage all users that train models to contribute by sharing them ...\"],[\"```\\n\\nIn a terminal, you can run:\\n\\n```bash\\nhuggingface-cli login\\n```\\n\\nIn both cases, you should be pr...\"],[\"```\\n\\nWhen you call `trainer.train()`, the `Trainer` will then upload your model to the Hub each time...\"],[\"```\\n\\nThen you should add `callbacks=[callback]` in your call to `model.fit()`. The callback will the...\"],[\"```\\n\\nThis will create the new repository `dummy-model` in your profile, and populate it with your mo...\"],[\"```\\n\\nNow head to the Model Hub to find your newly uploaded model: *https:\\u002f\\u002fhuggingface.co\\u002fuser-or-or...\"],[\"The `push_to_hub()` method is backed by the [`huggingface_hub`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggi...\"],[\"```\\n\\nThe `huggingface_hub` package offers several methods and classes which are useful for our purpo...\"],[\"```\\n\\nThis will create the `dummy-model` repository in the `huggingface` namespace, assuming you belo...\"],[\"After creating your model repository, you should see a page like this:\\n\\n\\u003cdiv class=\\\"flex justify-cen...\"],[\"We'll take a look at how to add some new files next.\\n\\n## Uploading the model files[[uploading-the-mo...\"],[\"```\\n\\nThis will upload the file `config.json` available at `\\u003cpath_to_file\\u003e` to the root of the reposi...\"],[\"```\\n\\nAnd others! We recommend taking a look at the `Repository` documentation available [here](https...\"],[\"```\\n\\n```bash\\nUpdated git hooks.\\nGit LFS initialized.\\n```\\n\\nOnce that's done, the first step is to clo...\"],[\"```\\n\\n```bash\\nREADME.md\\n```\\n\\nIf you just created your repository using Hugging Face Hub's `create_rep...\"],[\"```\\n{\\u002fif}\\n\\nNow that we've saved some model and tokenizer artifacts, let's take another look at the *...\"],[\"```\\n\\nWe can then have a look at the files that are currently staged:\\n\\n```bash\\ngit status\\n```\\n\\n{#if f...\"],[\"```\\n\\nWe can see that all files have `Git` as a handler, except *pytorch_model.bin* and *sentencepiec...\"],[\"```\\n{\\u002fif}\\n\\nPushing can take a bit of time, depending on the speed of your internet connection and th...\"],[\"```\\n\\n{#if fw === 'pt'}\\nIf we take a look at the model repository when this is finished, we can see a...\"],[\"n these few videos, we'll take a look at the tokenizers. In Natural Language Processing, most of the...\"],[\"one, so we recommend you look at the videos in the following order: Word-based, Character-based, and...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Fine-tuning a masked language model[[fine-tuning-a-masked-language-...\"],[\"However, there are a few cases where you'll want to first fine-tune the language models on your data...\"],[\"\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-distilbert-base-uncased-finetuned-imdb.hf.space\\\" frameBorder=\\\"0\\\" h...\"],[\"\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-course\\u002fdocum...\"],[\"```\\n\\nWe can see how many parameters this model has by calling the `num_parameters()` method:\\n\\n```pyt...\"],[\"```python out\\nModel: \\\"tf_distil_bert_for_masked_lm\\\"\\n________________________________________________...\"],[\"vocab_projector (TFDistilBer multiple                  23866170  \\n==================================...\"],[\"```\\n\\n{\\u002fif}\\n\\nWith around 67 million parameters, DistilBERT is approximately two times smaller than th...\"],[\"```\\n\\nWith a tokenizer and a model, we can now pass our text example to the model, extract the logits...\"],[\"```\\n\\n{\\u002fif}\\n\\n```python out\\n'\\u003e\\u003e\\u003e This is a great deal.'\\n'\\u003e\\u003e\\u003e This is a great success.'\\n'\\u003e\\u003e\\u003e This is a ...\"],[\"```\\n\\nWe can see that the `train` and `test` splits each consist of 25,000 reviews, while there is an...\"],[\"```\\n\\n```python out\\n\\n'\\u003e\\u003e\\u003e Review: This is your typical Priyadarshan movie--a bunch of loony character...\"],[\"'\\u003e\\u003e\\u003e Review: I saw this movie at the theaters when I was about 6 or 7 years old. I loved it then, an...\"],[\"```\\n\\nYep, these are certainly movie reviews, and if you're old enough you may even understand the co...\"],[\"So to get started, we'll first tokenize our corpus as usual, but _without_ setting the `truncation=T...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n512\\n```\\n\\nThis value is derived from the *tokenizer_config.json* file associated w...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e Review 0 length: 200'\\n'\\u003e\\u003e\\u003e Review 1 length: 559'\\n'\\u003e\\u003e\\u003e Review 2 length: 192'\\n...\"],[\"```\\n\\nAs you can see in this example, the last chunk will generally be smaller than the maximum chunk...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['attention_mask', 'input_id...\"],[\"```\\n\\n```python out\\n\\\".... at.......... high. a classic line : inspector : i'm here to sack one of you...\"],[\"```\\n\\nTo see how the random masking works, let's feed a few examples to the data collator. Since it e...\"],[\"```\\n\\nNice, it worked! We can see that the `[MASK]` token has been randomly inserted at various locat...\"],[\"```py\\nimport collections\\nimport numpy as np\\n\\nfrom transformers import default_data_collator\\n\\nwwm_pro...\"],[\"```\\n\\n{:else}\\n\\n```py\\nimport collections\\nimport numpy as np\\n\\nfrom transformers.data.data_collator impo...\"],[\"```\\n\\n```python out\\n'\\u003e\\u003e\\u003e [CLS] bromwell high is a cartoon comedy [MASK] it ran at the same time as so...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\n‚úèÔ∏è **Try it out!** Run the code snippet above several times to see the random masking ha...\"],[\"```\\n\\nThis has automatically created new `train` and `test` splits, with the training set size set to...\"],[\"```\\n\\nNext, we set up our training hyperparameters and compile our model. We use the `create_optimize...\"],[\"# Train in mixed-precision float16\\ntf.keras.mixed_precision.set_global_policy(\\\"mixed_float16\\\")\\n\\nmode...\"],[\"```\\n\\nWe're now ready to run `model.fit()` -- but before doing so let's briefly look at _perplexity_,...\"],[\"```\\n\\nHere we tweaked a few of the default options, including `logging_steps` to ensure we track the ...\"],[\"```\\n\\nWe're now ready to run `trainer.train()` -- but before doing so let's briefly look at _perplexi...\"],[\"```\\n\\n{:else}\\n\\nAssuming our test set consists mostly of sentences that are grammatically correct, the...\"],[\"```\\n\\n{:else}\\n\\n```python\\neval_loss = model.evaluate(tf_eval_dataset)\\nprint(f\\\"Perplexity: {math.exp(ev...\"],[\"```\\n\\n{\\u002fif}\\n\\n\\u003cTip\\u003e\\n\\n‚úèÔ∏è **Your turn!** Run the training above after changing the data collator to the ...\"],[\"```\\n\\nNext, we'll apply this function to our test set and drop the unmasked columns so we can replace...\"],[\"```\\n\\nWith these objects, we can now prepare everything for training with the `Accelerator` object:\\n\\n...\"],[\"```\\n\\nWith that done, it's just a simple matter of writing out the full training and evaluation loop:...\"],[\"```\\n\\nCool, we've been able to evaluate perplexity with each epoch and ensure that multiple training ...\"],[\"FrameworkSwitchCourse {fw} \\u002f\\u003e\\n\\n# Using pretrained models[[using-pretrained-models]]\\n\\n{#if fw === 'pt...\"],[\"Let's say we're looking for a French-based model that can perform mask filling.\\n\\n\\u003cdiv class=\\\"flex ju...\"],[\"```\\n\\n```python out\\n[\\n  {'sequence': 'Le camembert est d√©licieux :)', 'score': 0.49091005325317383, '...\"],[\"```\\n\\nAs you can see, loading a model within a pipeline is extremely simple. The only thing you need ...\"],[\"```\\n{:else}\\n```py\\nfrom transformers import CamembertTokenizer, TFCamembertForMaskedLM\\n\\ntokenizer = C...\"],[\"Understanding the Interface class[[understanding-the-interface-class]]\\n\\n\\u003cCourseFloatingBanner chapte...\"],[\"For a complete list of components, [see the Gradio docs ](https:\\u002f\\u002fgradio.app\\u002fdocs). Each pre-built c...\"],[\"```py\\nimport numpy as np\\nimport gradio as gr\\n\\n\\ndef reverse_audio(audio):\\n    sr, data = audio\\n    re...\"],[\"```\\n\\nThe code above will produce an interface like the one below (if your browser doesn't\\nask you fo...\"],[\"The code snippet below shows how three input components line up with the three arguments of the `gen...\"],[\"```\\n\\n\\u003ciframe src=\\\"https:\\u002f\\u002fcourse-demos-generate-tone.hf.space\\\" frameBorder=\\\"0\\\" height=\\\"450\\\" title=\\\"G...\"],[\"We'll cover the `share` parameter in a lot more detail in the next section!\\n\\n## ‚úèÔ∏è Let's apply it![[...\"],[\"```\\n\\nIf your browser doesn't ask you for microphone permissions, \\u003ca href=\\\"https:\\u002f\\u002fhuggingface.co\\u002fspa...\"],[\"ou are at the right place if you want to understand what the Byte pair Encoding subword tokenization...\"],[\"corpus we used to train it. How is a BPE tokenizer trained? First of all, we have to get a corpus of...\"],[\"of the following words: huggingface, hugging, hug, hugger, etc. BPE is an algorithm that starts with...\"],[\"how to increase it. We return to our split corpus, we will go through the words one by one and count...\"],[\"note our first merging rule and we add the new token to our vocabulary. We can then apply this mergi...\"],[\"the vocabulary and then we merge all the pairs of tokens composed of the token \\\"le\\\" and \\\"a\\\" into our...\"],[\"have learned our vocabulary and our merging rules, we can tokenize new texts. For example, if we wan...\"],[\"that's it, I hope that now the BPE algorithm has no more secret for you!...\"],[\"ome bugs in your code are very straightforward. You try running it, you get a syntax error somewhere...\"],[\"added bonus problem that your models are often compiled before execution, which is great for perform...\"],[\"we create our tokenizer and we tokenize the dataset. Next, we convert our datasets to TensorFlow dat...\"],[\"- how do we even begin to debug something like that? When the error you get doesn't immediately sugg...\"],[\"like so, by looping over the dataset for one iteration and then breaking. So what do we get when we ...\"],[\"need to be passed in the input dictionary, where the model can see them. This internal loss is the l...\"],[\"are, or we keep using Keras losses, but we move the labels to the place Keras expects them. For simp...\"],[\"all the outputs nan , all the weights are nan too. Once a single nan creeps into your computations, ...\"],[\"You can see this in more detail in the accompanying section of the course notes, but we find that if...\"],[\"loss of nan because we got an \\\"impossible\\\" label. To fix that, we need to go back and set the model ...\"],[\"at a fairly high value. What's going on? Well, when things are mostly working, but training is just ...\"],[\"but in the process we invisibly got the default learning rate, which is 1e-3, or ten to the power of...\"],[\"this to see this in more detail and to experiment with the code yourself. Good luck, and remember to...\"],[\"How to write a good issue[[how-to-write-a-good-issue]]\\n\\n\\u003cCourseFloatingBanner chapter={8}\\n  classNam...\"],[\"\\u003cTip\\u003e\\n\\nüö® Many issues in the ü§ó Transformers repository are unsolved because the data used to reproduc...\"],[\"```\\ntransformers-cli env\\n```\\n\\nand you should get something like this:\\n\\n```out\\nCopy-and-paste the tex...\"],[\"```\\n```python\\n```\\n\\nthen paste in your minimal reproducible example and type a new line with three ba...\"],[\"oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll...\"],[\"need to provide the name of the format to the load_dataset function, along with a data_files argumen...\"],[\"data_files argument. The CSV loading script also allows you to pass several keyword arguments, so he...\"],[\"filepath. Let's now take a look at loading raw text files. This format is quite common in NLP and yo...\"],[\"text are also represented as a row in the dataset. For JSON files, there are two main formats to kno...\"],[\"Summary[[summary]]\\n\\n\\u003cCourseFloatingBanner\\n    chapter={1}\\n    classNames=\\\"absolute z-10 right-0 top-...\"],[\"What if my dataset isn't on the Hub?[[what-if-my-dataset-isnt-on-the-hub]]\\n\\n\\u003cCourseFloatingBanner ch...\"],[\"ü§ó Datasets provides loading scripts to handle the loading of local and remote datasets. It supports ...\"],[\"The training and test splits are hosted on GitHub, so we can download them with a simple `wget` comm...\"],[\"```\\n\\nThis will download two compressed files called *SQuAD_it-train.json.gz* and *SQuAD_it-test.json...\"],[\"```\\n\\n```python out\\nDatasetDict({\\n    train: Dataset({\\n        features: ['title', 'paragraphs'],\\n   ...\"],[\"```\\n\\nThis is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up ...\"],[\"```\\n\\nThis can be useful if you don't want to manually decompress many GZIP files. The automatic deco...\"],[\"```\\n\\nThis returns the same `DatasetDict` object obtained above, but saves us the step of manually do...\"],[\"rite your own training loop in PyTorch. In this video, we will look at how we can do the same fine-t...\"],[\"it to the model. With the labels, we can then compute a loss. That number is not useful on its own, ...\"],[\"on your favorite deep learning course. We will use the GLUE MRPC dataset here again, and we have see...\"],[\"we try to grab a batch of data and inspect it. Like our dataset elements, it's a dictionary, but thi...\"],[\"here two. Again, to be sure everything is going well, we pass the batch we grabbed to our model and ...\"],[\"you like. Using the previous loss and computing the gradients with loss.backward(), we check that we...\"],[\"the Transformers library is just a convenience function to easily build such a scheduler, you can ag...\"],[\"for you! We can now put everything together! First we put our model in training mode (which will act...\"],[\"in our scheduler for the next iteration and zero the gradients of the optimizer. Once this is finish...\"],[\"send it those intermediate predictions. Once the evaluation loop is finished, we just have to call t...\"],[\"atasets and DataFrames equals love. Although the processing functions of Datasets will cover most th...\"],[\"an example, let's suppose we're analysing Supreme Court cases from Switzerland. As usual we download...\"],[\"regions. Answering these questions with the native Arrow format isn't easy, but we can easily switch...\"],[\"that the Datasets library changes the magic __getitem__() method of the dataset. The __getitem__() m...\"],[\"one go. And once you have a DataFrame, you can find answers to all sorts of complex questions or mak...\"],[\"n this video, we'll study the decoder architecture. An example of a popular decoder-only architectur...\"],[\"use a small example, using three words. We pass them through the decoder. We retrieve a numerical re...\"],[\"representation of the word in question. The dimension of that vector is defined by the architecture ...\"],[\"the words on the left and right, I.e., the bidirectional context, decoders only have access to the w...\"],[\"used in a wide variety of tasks. However, the strength of a decoder lies in the way a word has acces...\"],[\"We use this as input for the decoder. The model outputs a vectors of dimension 768. This vector cont...\"],[\"we are now at \\\"My name\\\". This is where the \\\"autoregressive\\\" aspect comes in. Auto-regressive models ...\"],[\"a while; GPT-2, for example, has a maximum context size of 1024. We could eventually generate up to ...\"],[\"It is based off of the masked self-attention layer, which allows to have word embeddings which have ...\"],[\"\\\"sequence-to-sequence\\\" transformer (which can generally be used interchangeably). We recommend you c...\"],[\"n our other videos, and as always, there'll be links below if you want to check those out, we showed...\"],[\"can use the standard Keras predict() method, as shown here. You simply pass in tokenized text to thi...\"],[\"into the model‚Äôs probability outputs, you just apply a softmax, like so. What if we want to turn tho...\"],[\"skip the softmax step entirely, because the largest logit will always be the largest probability too...\"],[\"data from the MRPC dataset, which is part of the GLUE benchmark. Each of the GLUE datasets, as well ...\"],[\"compute those metrics to benchmark our model, we just pass them the model‚Äôs predictions, and the gro...\"],[\"'metric' argument to compile(). As with things like loss and optimizer, you can specify the metrics ...\"],[\"a bit beyond the scope of this course, I'll link to the relevant TF docs below because it can be ver...\"],[\"Unigram tokenization[[unigram-tokenization]]\\n\\n\\u003cCourseFloatingBanner chapter={6}\\n  classNames=\\\"absolu...\"],[\"This is all a very costly operation, so we don't just remove the single symbol associated with the l...\"],[\"```\\n(\\\"hug\\\", 10), (\\\"pug\\\", 5), (\\\"pun\\\", 12), (\\\"bun\\\", 4), (\\\"hugs\\\", 5)\\n```\\n\\nand for this example, we will...\"],[\"```\\n\\nSo, the sum of all frequencies is 210, and the probability of the subword `\\\"ug\\\"` is thus 20\\u002f210...\"],[\"```\\n\\nSo, `\\\"pug\\\"` would be tokenized as `[\\\"p\\\", \\\"ug\\\"]` or `[\\\"pu\\\", \\\"g\\\"]`, depending on which of those s...\"],[\"```\\n\\nThus `\\\"unhug\\\"` would be tokenized as `[\\\"un\\\", \\\"hug\\\"]`.\\n\\n\\u003cTip\\u003e\\n\\n‚úèÔ∏è **Now your turn!** Determine t...\"],[\"```\\n\\nNow we need to compute how removing each token affects the loss. This is rather tedious, so we'...\"],[\"```\\n\\nLike for BPE and WordPiece, we begin by counting the number of occurrences of each word in the ...\"],[\"```\\n\\nWe group the characters with the best subwords to arrive at an initial vocabulary of size 300:\\n...\"],[\"```\\n\\nNow the main function is the one that tokenizes words using the Viterbi algorithm. As we saw be...\"],[\"Once the main loop is finished, we just start from the end and hop from one start position to the ne...\"],[\"```\\n\\nWe can already try our initial model on some words:\\n\\n```python\\nprint(encode_word(\\\"Hopefully\\\", m...\"],[\"```\\n\\nWe can try it on a given token:\\n\\n```python\\nscores = compute_scores(model)\\nprint(scores[\\\"ll\\\"])\\np...\"],[\"```\\n\\nThen, to tokenize some text, we just need to apply the pre-tokenization and then use our `encod...\"]],\"hovertemplate\":\"source=course\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"course, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"course, circle\",\"showlegend\":true,\"x\":[-5.7095537,-5.781458,-5.736165,-5.5026674,-5.349195,-1.8015488,0.1976975,0.01643036,-0.23424071,0.2196451,-0.020383257,-0.0056188786,-0.5488725,0.76005304,0.28361937,-0.22743848,-4.988725,-5.599414,-2.8699849,-7.114456,-6.757958,-7.010097,-6.574021,-6.9647217,-7.316119,-7.9318748,-7.9854436,-8.003584,-7.7134523,-7.549757,-7.0277567,-6.040097,-6.2206984,-6.0823946,-6.252129,-6.6714687,-6.110241,-4.7700076,-7.383801,5.5072966,5.6916056,5.6655316,5.5320835,2.2993512,2.4120705,2.3560715,2.3458183,2.1978035,2.3402784,-5.0498466,2.3445003,2.3159077,2.2989025,2.2942045,1.0005809,2.4148047,2.2778664,1.2356409,1.4838914,1.080974,1.6456703,1.4928632,2.0492692,-3.9553463,0.85460776,-3.8895292,-3.3051481,-3.1092007,-3.1977494,-3.298082,-3.2971456,-3.4028146,-4.231115,-6.003803,-6.040962,-5.774686,-5.4811974,-5.85796,-5.9491653,-6.898855,-5.575419,-5.8123026,-5.978944,-6.2369065,-6.066791,-2.0413547,-1.810653,-1.7579571,-1.8015704,-1.6766603,-0.72243226,12.582566,12.719839,13.04982,12.695442,12.680336,-6.67966,-7.174846,-4.911524,0.80100405,1.3024426,2.0096672,-0.7989973,0.53396684,1.6879106,1.4295117,1.5656568,2.186907,1.7018385,1.5455916,2.5431788,2.8456428,2.3776765,3.414661,2.6125553,3.3832576,3.4580433,3.5007348,3.471199,1.6349599,-7.0361524,-6.689255,-4.6261425,-5.560152,-4.981568,-4.654867,-3.3885162,-3.5973501,0.14996296,-6.0282626,-5.168326,-5.291391,-5.074404,-7.498722,-7.939687,-6.061848,-5.491151,-5.3455653,-5.29251,-5.4999113,-5.629471,-5.5582094,-5.6676717,-5.445224,-5.3163276,-4.5189943,-4.8101273,-7.5170155,-5.295854,-1.6031054,-1.909904,-2.3321142,-2.6901891,-2.3317235,-5.669443,-5.6660557,-3.6271105,-4.7101865,-3.1964397,-1.3491827,-2.1144059,-5.586931,0.31776163,-3.531671,-5.1798067,-3.2510812,-3.3746047,14.005475,12.562918,12.166787,11.924587,12.3798485,12.340889,11.724464,12.155971,-7.0582767,-7.0844955,-6.8695784,-6.7676687,-6.9729085,-7.023837,-6.929879,-7.2181783,-2.8412402,-3.2929637,-3.275266,-3.0259354,-2.9248936,-3.263055,-3.1525574,-3.1783495,-3.7165701,-3.5254421,-2.525274,-2.263538,2.707743,1.5004537,1.8906142,1.4659857,1.2331132,1.1002238,0.6154428,0.8263071,0.74079216,0.42065173,-5.9588957,1.370302,0.7494245,-0.20675634,0.86518013,-0.1810238,0.7906931,1.0205426,0.99956787,-2.4268494,-2.8931754,-3.456343,-3.0898335,-2.480003,-2.1659517,-1.9108257,-1.9812026,-3.0222547,-3.1138878,-0.31607446,-0.3163547,-0.040044006,0.069216676,-0.21578097,-9.224552,-9.524411,-10.106354,-10.268485,-8.340269,-9.323845,-8.788287,-9.359599,-10.378365,-9.575304,-3.3417382,-2.8935544,-2.8808877,-2.9582307,-3.9060488,-3.3667693,-3.5070267,-3.5746193,-7.185749,-7.0493774,-7.0872383,-6.955972,-6.908981,-7.091431,-7.0304437,-6.265986,-6.4968934,-6.6637816,-6.319963,-5.500475,-6.6197457,-7.028131],\"xaxis\":\"x\",\"y\":[-7.0886917,-7.1517925,-7.094271,-7.170077,-7.0535717,-3.0529654,-7.236887,-7.2926526,-7.1660247,-7.2943573,-7.126036,-7.0786953,-7.036462,-5.559725,-6.23124,-7.12095,-6.8552313,-7.174351,-7.009773,-6.4995546,-6.696783,-6.7666845,-6.579768,-6.7785945,-7.369562,-8.205698,-8.294613,-8.268947,-7.88833,-7.6221085,-6.925465,-6.944837,-4.263102,-4.46247,-7.082377,-6.9918346,-6.951564,-1.3186942,-2.1006713,0.49535665,0.5564701,0.5369561,0.49246106,-0.59656507,-0.6903996,-0.7142497,-0.6131302,-0.33917713,-0.59909934,-1.692353,-2.0078125,-3.1989722,-3.0393815,-3.391607,-4.210514,-4.358914,-4.723197,-4.1654325,-4.682579,-4.0853033,-4.8107343,-4.575639,-4.795516,-4.93717,-4.354109,-6.434281,-6.7700925,-6.578814,-6.7622766,-6.56236,-6.567582,-6.702147,-5.269487,-1.5051512,-1.8051969,-1.1031723,-1.3035371,-1.8775225,-1.4897017,-1.8001261,-6.9837117,-6.898106,-6.982496,-6.8222194,-6.867197,-3.1894357,-4.0350323,-4.416886,-4.6098228,-4.901892,-4.520388,1.597133,1.3400545,1.3872446,1.4665753,1.2774004,-3.5832973,-5.2696285,-1.0417826,-4.7022934,-4.6501436,-4.260717,-5.66189,-4.985427,-4.724228,-4.402509,-4.8998413,-4.546144,-4.409195,-3.9688582,-4.6335816,-4.732761,-4.7650113,-4.5358357,-4.6320143,-4.704555,-4.6725025,-4.7551193,-4.7578263,-3.5609643,-6.61681,-4.0027947,-3.4389272,-3.0287669,-1.2421004,-3.024077,-5.2548895,-4.8204913,-1.4818437,-4.317755,-6.3156853,-4.0893316,-5.915649,-1.5018222,-1.9849843,-3.9709148,-6.8125234,-5.996321,-6.741408,-7.222156,-6.622757,-5.7424617,-5.448422,-5.716136,-5.7431273,-6.6819167,-6.4284835,-1.5862423,-5.8101277,-6.4456334,-5.7186904,-5.924346,-6.348132,-5.9572206,-4.433908,-5.66553,-6.829508,-5.920166,-6.4939976,-5.0327015,-5.682927,-3.7794807,-3.8342314,-4.7884126,-8.253367,-4.66817,-5.056606,1.063325,0.3161256,0.04493055,0.16510963,0.45144898,1.1204642,-0.27815154,0.37397733,-6.658615,-7.0241613,-6.721249,-6.8863606,-6.958712,-6.941225,-6.8739133,-1.1410747,-5.621855,-5.3191013,-5.5515513,-5.762834,-6.084248,-6.1447,-6.063094,-6.15241,-6.3691373,-6.5200467,-4.2300763,-4.292645,-1.9748645,-3.956428,-3.4105403,-3.44795,-3.6206234,-5.3929057,-6.9415436,-6.888895,-6.919284,-6.677946,-2.6629815,-5.1571407,-6.7535467,-4.6305623,-6.4814353,-6.7954793,-6.577197,-6.6731687,-5.8620024,-6.119257,-5.997644,-6.73437,-6.4308105,-5.9212313,-5.050255,-4.35685,-4.8897243,-7.4280887,-7.539539,-6.875279,-6.7923436,-7.091146,-7.1187882,-7.0599113,-5.137023,-5.174088,-5.1351724,-5.060634,-5.401438,-5.1003175,-4.8457317,-5.0458083,-5.0597324,-5.1098876,-5.3853035,-7.657412,-7.701433,-7.6361666,-8.726045,-7.9601564,-7.996615,-8.3225355,-6.9535084,-6.71992,-6.9768643,-6.9099464,-6.88979,-6.94113,-7.074876,-6.955233,-6.989878,-6.959918,-6.8316603,-6.1493773,-6.6245213,-7.4318876],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"Its architecture includes 3 main components:\\n1. [FLAN-UL2](https:\\u002f\\u002fhuggingface.co\\u002fgoogle\\u002fflan-ul2), ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cTip\\u003e\\n\\nLoRA is very versatile and supported for [DreamBooth](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffuser...\"],[\"```\\n\\nNavigate to the example folder with the training script and install the required dependencies f...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nMany of the basic and important parameters are described in the [Text-to-image](text2image#scri...\"],[\"```py\\nlora_attn_procs = {}\\nfor name in unet.attn_processors.keys():\\n    cross_attention_dim = None i...\"],[\"```\\n\\nThe [optimizer](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002fdd9a5caf61f04d11c0fa9f3947b69ab00...\"],[\"```\\n\\nAside from setting up the LoRA layers, the training script is more or less the same as train_te...\"],[\"accelerate launch --mixed_precision=\\\"fp16\\\"  train_text_to_image_lora.py \\\\\\n  --pretrained_model_name_...\"],[\"```\\n\\nOnce training has been completed, you can use your model for inference:\\n\\n```py\\nfrom diffusers i...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Text-to-audio (TTA) system has recently gained attention for its a...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"For a more technical overview of LCMs, refer to [the paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2310.04378...\"],[\"Before going through this guide, we'll take a look at the general workflow for performing inference ...\"],[\"```\\n\\n## Text-to-image\\n\\nYou'll use the [`StableDiffusionXLPipeline`] with the scheduler: [`LCMSchedul...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npi...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# Combine LoRAs\\npipe.set_adapters([\\\"lcm\\\", \\\"papercut\\\"], adapter_weights=[1.0, 0.8])\\n\\nprompt = \\\"paperc...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# set scheduler\\npipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\\n\\n# load LCM-LoRA\\npi...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"# load adapter\\nadapter = T2IAdapter.from_pretrained(\\\"TencentARC\\u002ft2i-adapter-canny-sdxl-1.0\\\", torch_d...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"pipe.set_adapters([\\\"lcm\\\", \\\"motion-lora\\\"], adapter_weights=[0.55, 1.2])\\n\\nprompt = \\\"best quality, mast...\"],[\"```\\n\\n![](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffusers\\u002flcm...\"],[\"Latent Consistency Distillation Example:\\n\\n[Latent Consistency Models (LCMs)](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2...\"],[\"```\\n\\nWhen running `accelerate config`, if we specify torch compile mode to True there can be dramati...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"```\\n\\n## LCM-LoRA\\n\\nInstead of fine-tuning the full model, we can also just train a LoRA that can be i...\"],[\"```bash\\nexport MODEL_NAME=\\\"stabilityai\\u002fstable-diffusion-xl-base-1.0\\\"\\nexport OUTPUT_DIR=\\\"path\\u002fto\\u002fsave...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*There is large consent that successful training of deep networks r...\"],[\"Stable Diffusion\\n\\n## Overview\\n\\nStable Diffusion was proposed in [Stable Diffusion Announcement](http...\"],[\"## Available Pipelines:\\n\\n| Pipeline | Tasks | Colab\\n|---|---|:---:|\\n| [pipeline_stable_diffusion.py]...\"],[\"## Examples:\\n\\n### Using Stable Diffusion without being logged into the Hub.\\n\\nIf you want to download...\"],[\"```\\n\\nThis however can make it difficult to build applications on top of `diffusers` as you will alwa...\"],[\"```\\n\\n### Text-to-Image with K-LMS scheduler\\n\\n```python\\n# make sure you're logged in with `huggingfac...\"],[\"```\\n\\n### CycleDiffusion using Stable Diffusion and DDIM scheduler\\n\\n```python\\nimport requests\\nimport ...\"],[\"image.save(\\\"horse_to_elephant.png\\\")\\n\\n# let's try another example\\n# See more samples at the original ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [ai-forever\\u002fKandinsky-2](https:\\u002f\\u002fgithub.com\\u002fai-forever\\u002fKandins...\"],[\"## KandinskyV22Img2ImgPipeline\\n\\n[[autodoc]] KandinskyV22Img2ImgPipeline\\n\\t- all\\n\\t- __call__\\n\\n## Kandi...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about MultiDiffusion on the [project page](https:\\u002f\\u002fmultidiffusio...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"The original codebase can be found at [openai\\u002fshap-e](https:\\u002f\\u002fgithub.com\\u002fopenai\\u002fshap-e).\\n\\n\\u003cTip\\u003e\\n\\nSee...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"- **Self-contained**: An example script shall only depend on \\\"pip-install-able\\\" Python packages that...\"],[\"We provide **official** examples that cover the most popular tasks of diffusion models.\\n*Official* e...\"],[\"Training examples show how to pretrain or fine-tune diffusion models for a variety of tasks. Current...\"],[\"## Community\\n\\nIn addition, we provide **community** examples, which are examples added and maintaine...\"],[\"```\\nThen cd in the example folder of your choice and run\\n```bash\\npip install -r requirements.txt\\n```...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Creating noise from data is easy; creating data from noise is gene...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThe `apply_patch` function exposes a number of [arguments](https:\\u002f\\u002fgithub.com\\u002fdbolya\\u002ftomesd#usa...\"],[\"```bash\\n- `diffusers` version: 0.15.1\\n- Python version: 3.8.16\\n- PyTorch version (GPU?): 1.13.1+cu11...\"],[\"```\\n\\nTo reproduce this benchmark, feel free to use this [script](https:\\u002f\\u002fgist.github.com\\u002fsayakpaul\\u002f2...\"],[\"| **GPU**  | **Resolution** | **Batch size** | **Vanilla** | **ToMe**       | **ToMe + xFormers** |\\n...\"],[\"| **V100** |            512 |             10 |         OOM |          10.03 |                9.29 |\\n...\"],[\"As seen in the tables above, the speed-up from `tomesd` becomes more pronounced for larger image res...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about InstructPix2Pix on the [project page](https:\\u002f\\u002fwww.timothyb...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nThis tutorial shows you how to use an `AutoPipeline` to automatically infer the pipeline cla...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"response = requests.get(url)\\nimage = Image.open(BytesIO(response.content)).convert(\\\"RGB\\\")\\nimage.thum...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n## Use multiple pipelines\\n\\nFor some workflows or if you're loading many pipelines, it is more m...\"],[\"```\\n\\nIf you passed an optional argument - like disabling the safety checker - to the original pipeli...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract from the paper is:\\n\\n*Free-form inpainting is the task of adding new content to an image...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nThis guide will show you how to use the Stable Diffusion and Stable Diffusion XL (SDXL) pipelin...\"],[\"```\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002foptimum\\u002fdocument...\"],[\"--\\n{{ card_data }}\\n---\\n\\n\\u003c!-- This model card has been generated automatically according to the infor...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Attend-and-Excite on the [project page](https:\\u002f\\u002fattendande...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Vers...\"],[\"\\u003cTip\\u003e\\n\\nMake sure to check out the Schedulers [guide](..\\u002f..\\u002fusing-diffusers\\u002fschedulers) to learn how ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*The incredible generative ability of large-scale text-...\"],[\"## Usage example with the base model of StableDiffusion-1.4\\u002f1.5\\n\\nIn the following we give a simple e...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_ref...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fcolor_out...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch.png)\\n\\nThen, create the ada...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fAdapter\\u002ft2iadapter\\u002fresolve\\u002fmain\\u002fsketch_output.png)\\n\\n## Available ...\"],[\"| Model Name | Control Image Overview| Control Image Example | Generated Image Example |\\n|---|---|--...\"],[\"|[TencentARC\\u002ft2iadapter_canny_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_canny_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_sketch_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_sketch_sd14v1)\\u003cb...\"],[\"|[TencentARC\\u002ft2iadapter_depth_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_depth_sd14v1)\\u003cbr\\u002f...\"],[\"|[TencentARC\\u002ft2iadapter_openpose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_openpose_sd14v...\"],[\"|[TencentARC\\u002ft2iadapter_keypose_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_keypose_sd14v1)...\"],[\"|[TencentARC\\u002ft2iadapter_seg_sd14v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_seg_sd14v1)\\u003cbr\\u002f\\u003e*Tr...\"],[\"|[TencentARC\\u002ft2iadapter_zoedepth_sd15v1](https:\\u002f\\u002fhuggingface.co\\u002fTencentARC\\u002ft2iadapter_zoedepth_sd15v...\"],[\"## Combining multiple adapters\\n\\n[`MultiAdapter`] can be used for applying multiple conditionings at ...\"],[\"```\\n\\nThe two control images look as such:\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n![img](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-images\\u002fresolve\\u002fmain\\u002ft2i-adapter\\u002fkeypose_d...\"],[\"Adapt a model to a new task\\n\\nMany diffusion systems share the same components, allowing you to adapt...\"],[\"```\\n\\nTo adapt your text-to-image model for inpainting, you'll need to change the number of `in_chann...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003c\\u002fTip\\u003e\\n\\n## StableDiffusionLatentUpscalePipeline\\n\\n[[autodoc]] StableDiffusionLatentUpscalePipeline\\n\\t-...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n## Text-to-3D\\n\\nTo generate a gif of a 3D object, pass a text prompt to the [`ShapEPipeline`]. T...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"prompt = \\\"A cheeseburger, white background\\\"\\n\\nimage_embeds, negative_image_embeds = prior_pipeline(pr...\"],[\"```\\n\\nPass the cheeseburger to the [`ShapEImg2ImgPipeline`] to generate a 3D representation of it.\\n\\n`...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex gap-4\\\"\\u003e\\n  \\u003cdiv\\u003e\\n    \\u003cimg class=\\\"rounded-xl\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatase...\"],[\"```\\n\\nUse the [`~utils.export_to_ply`] function to save the mesh output as a `ply` file:\\n\\n\\u003cTip\\u003e\\n\\nYou ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The abstract of the paper is the following:\\n\\n*With the advance of text-to-image models (e.g., Stable...\"],[\"## Usage example\\n\\nAnimateDiff works with a MotionAdapter checkpoint and a Stable Diffusion model che...\"],[\"```\\n\\nHere are some sample outputs:\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestq...\"],[\"scheduler = DDIMScheduler.from_pretrained(\\n    model_id, subfolder=\\\"scheduler\\\", clip_sample=False, t...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"```\\n\\nThen you can use the following code to combine Motion LoRAs.\\n\\n```python\\nimport torch\\nfrom diffu...\"],[\"```\\n\\n\\u003ctable\\u003e\\n    \\u003ctr\\u003e\\n        \\u003ctd\\u003e\\u003ccenter\\u003e\\n        masterpiece, bestquality, sunset.\\n        \\u003cbr\\u003e\\n  ...\"],[\"!--Copyright 2023 The GLIGEN Authors and The HuggingFace Team. All rights reserved.\\n\\nLicensed under ...\"],[\"The abstract from the [paper](https:\\u002f\\u002fhuggingface.co\\u002fpapers\\u002f2301.07093) is:\\n\\n*Large-scale text-to-im...\"],[\"## StableDiffusionGLIGENPipeline\\n\\n[[autodoc]] StableDiffusionGLIGENPipeline\\n\\t- all\\n\\t- __call__\\n\\t- en...\"],[\"W√ºrstchen text-to-image fine-tuning\\n\\n## Running locally with PyTorch\\n\\nBefore running the scripts, ma...\"],[\"```\\n\\n## Prior training\\n\\nYou can fine-tune the W√ºrstchen prior model with the `train_text_to_image_pr...\"],[\"```\\n\\u003c!-- accelerate_snippet_end --\\u003e\\n\\n## Training with LoRA\\n\\nLow-Rank Adaption of Large Language Mode...\"],[\"```bash\\nexport DATASET_NAME=\\\"lambdalabs\\u002fpokemon-blip-captions\\\"\\n\\naccelerate launch train_text_to_imag...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"You can find additional information about Self-Attention Guidance on the [project page](https:\\u002f\\u002fku-c...\"],[\"# [Deprecated] Multi Token Textual Inversion\\n\\n**IMPORTART: This research project is deprecated. Mult...\"],[\"Colab for inference\\n[![Open In Colab](https:\\u002f\\u002fcolab.research.google.com\\u002fassets\\u002fcolab-badge.svg)](htt...\"],[\"```\\n\\nThen cd in the example folder  and run\\n```bash\\npip install -r requirements.txt\\n```\\n\\nAnd initial...\"],[\"```\\n\\nIf you have already cloned the repo, then you won't need to go through these steps.\\n\\n\\u003cbr\\u003e\\n\\nNow ...\"],[\"```\\n\\nA full training run takes ~1 hour on one V100 GPU.\\n\\n### Inference\\n\\nOnce you have trained a mode...\"],[\"```\\nIt should be at least 70% faster than the PyTorch script with the same configuration.\\n\\n### Train...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"The original codebase can be found at [CompVis\\u002flatent-diffusion](https:\\u002f\\u002fgithub.com\\u002fCompVis\\u002flatent-d...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\nNext, load a LoRA checkpoint with the [`~diffusers.loaders.StableDiffusionXLLoraLoaderMixin.lo...\"],[\"```\\n\\nLet's now generate an image with the second adapter and check the result:\\n\\n```python\\nprompt = \\\"...\"],[\"```\\n\\nNow that we have set these two adapters, let's generate an image from the combined adapters!\\n\\n\\u003c...\"],[\"```\\n\\n![toy-face-pixel-art](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002f...\"],[\"```\\n\\n![no-lora](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fresolve\\u002fmain\\u002fdiffus...\"],[\"```\\n\\n## Fusing adapters into the model\\n\\nYou can use PEFT to easily fuse\\u002funfuse multiple adapters dir...\"],[\"Consistency Decoder\\n\\nConsistency decoder can be used to decode the latents from the denoising UNet i...\"],[\"# Amused training\\n\\nAmused can be finetuned on simple datasets relatively cheaply and quickly. Using ...\"],[\"#### Full finetuning\\n\\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 750-1000 steps\\n\\n| ...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nNote that this training config keeps the batch size low and ...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + lora\\n\\nBatch size: 16, Learning rate: 8e-4, Gives decent results in 1000-...\"],[\"```\\n\\n### Finetuning the 512 checkpoint\\n\\nThese examples finetune on this [minecraft](https:\\u002f\\u002fhuggingf...\"],[\"```sh\\naccelerate launch train_amused.py \\\\\\n    --output_dir \\u003coutput path\\u003e \\\\\\n    --train_batch_size \\u003cb...\"],[\"```\\n\\n#### Full finetuning + 8 bit adam\\n\\nBatch size: 8, Learning rate: 5e-6, Gives decent results in ...\"],[\"```\\n\\n#### Full finetuning + lora \\n\\nBatch size: 8, Learning rate: 1e-4, Gives decent results in 500-1...\"],[\"```\\n\\n### Styledrop\\n\\n[Styledrop](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2306.00983) is an efficient finetuning method ...\"],[\"```\\n\\n#### 256\\n\\nExample results:\\n\\n![glowing_256_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"```\\n\\n#### 512\\n\\nExample results:\\n\\n![glowing_512_1](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fdiffusers\\u002fdocs-ima...\"],[\"# Diffusers examples with Intel optimizations\\n\\n**This research project is not actively maintained by...\"],[\"üß® Diffusers Experimental\\n\\nWe are adding experimental code to support novel applications and usages o...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\ncd examples\\u002fdreambooth\\npip install -r requirements_fl...\"],[\"```\\n\\nOr if your environment doesn't support an interactive shell, like a notebook, you can use:\\n\\n```...\"],[\"```\\n\\nSome basic and important parameters to know and specify are:\\n\\n- `--pretrained_model_name_or_pat...\"],[\"```\\n\\n### Prior preservation loss\\n\\nPrior preservation loss is a method that uses a model's own genera...\"],[\"```\\n\\n## Training script\\n\\nDreamBooth comes with its own dataset classes:\\n\\n- [`DreamBoothDataset`](htt...\"],[\"```\\n\\nNext is the [`main()`](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347a63ec...\"],[\"if model_has_vae(args):\\n    vae = AutoencoderKL.from_pretrained(\\n        args.pretrained_model_name_...\"],[\"```\\n\\nThen, it's time to [create the training dataset](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f...\"],[\"```\\n\\nLastly, the [training loop](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fdiffusers\\u002fblob\\u002f072e00897a7cf4302c347...\"],[\"```\\n\\nOne more thing before you launch the script! Depending on the GPU you have, you may need to ena...\"],[\"```\\n\\nDuring configuration, confirm that you want to use DeepSpeed. Now it should be possible to trai...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003chfoption id=\\\"Flax\\\"\\u003e\\n\\n```bash\\nexport MODEL_NAME=\\\"duongna\\u002fstable-diffusion-v1-4-flax...\"],[\"```\\n\\n\\u003c\\u002fTip\\u003e\\n\\n\\u003chfoptions id=\\\"training-inference\\\"\\u003e\\n\\u003chfoption id=\\\"PyTorch\\\"\\u003e\\n\\n```py\\nfrom diffusers impor...\"],[\"```\\n\\n\\u003c\\u002fhfoption\\u003e\\n\\u003c\\u002fhfoptions\\u003e\\n\\n## LoRA\\n\\nLoRA is a training technique for significantly reducing the ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdoc...\"],[\"```\\n\\n2. Set the number of timesteps to run the denoising process for:\\n\\n```py\\n\\u003e\\u003e\\u003e scheduler.set_times...\"],[\"```\\n\\n5. Now write a loop to iterate over the timesteps. At each timestep, the model does a [`UNet2DM...\"],[\"```\\n\\nIn the next section, you'll put your skills to the test and breakdown the more complex Stable D...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nNow that you know what you need for the Stable Diffusion pipeline, load all these components...\"],[\"```\\n\\nInstead of the default [`PNDMScheduler`], exchange it for the [`UniPCMultistepScheduler`] to se...\"],[\"```\\n\\nTokenize the text and generate the embeddings from the prompt:\\n\\n```py\\n\\u003e\\u003e\\u003e text_input = tokenize...\"],[\"```\\n\\n### Create random noise\\n\\nNext, generate some initial random noise as a starting point for the d...\"],[\"```\\n\\nThe last step is to create the denoising loop that'll progressively transform the pure noise in...\"],[\"```\\n\\nLastly, convert the image to a `PIL.Image` to see your generated image!\\n\\n```py\\n\\u003e\\u003e\\u003e image = (ima...\"],[\"Overview\\n\\nThese examples show how to run [Diffuser](https:\\u002f\\u002farxiv.org\\u002fabs\\u002f2205.09991) in Diffusers. ...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"There are two options for converting a `.ckpt` file: use a Space to convert the checkpoint or conver...\"],[\"```\\n\\nTo use the script:\\n\\n1. Git clone the repository containing the `.ckpt` file you want to convert...\"],[\"```\\n\\n## Keras .pb or .h5\\n\\n\\u003cTip warning={true}\\u003e\\n\\nüß™ This is an experimental feature. Only Stable Diffu...\"],[\"The Convert KerasCV Space allows you to input the following:\\n\\n* Your Hugging Face token.\\n* Paths to ...\"],[\"```\\n\\nThen, you can generate an image like:\\n\\n```py\\nfrom diffusers import DiffusionPipeline\\n\\npipeline ...\"],[\"```\\n\\nLoad the LoRA checkpoint into the pipeline with the [`~loaders.LoraLoaderMixin.load_lora_weight...\"],[\"InstructPix2Pix SDXL training example\\n\\n***This is based on the original InstructPix2Pix training exa...\"],[\"### Toy example\\n\\nAs mentioned before, we'll use a [small toy dataset](https:\\u002f\\u002fhuggingface.co\\u002fdataset...\"],[\"```\\n\\nNow, we can launch training:\\n\\n```bash\\naccelerate launch train_instruct_pix2pix_sdxl.py \\\\\\n    --...\"],[\"```\\n\\nAdditionally, we support performing validation inference to monitor training progress\\nwith Weig...\"],[\"```\\n\\n We recommend this type of validation as it can be useful for model debugging. Note that you ne...\"],[\"```bash \\naccelerate launch --mixed_precision=\\\"fp16\\\" --multi_gpu train_instruct_pix2pix_sdxl.py \\\\\\n   ...\"],[\"```\\n\\n ## Inference\\n\\n Once training is complete, we can perform inference:\\n\\n ```python\\nimport PIL\\nimp...\"],[\"```\\n\\nWe encourage you to play with the following three parameters to control\\nspeed and quality durin...\"],[\"accelerate launch train_instruct_pix2pix.py \\\\\\n    --pretrained_model_name_or_path=$MODEL_NAME \\\\\\n    ...\"],[\"```\\n\\nWe discovered that compared to training with SD-1.5 as the pretrained model, SDXL-0.9 results i...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface\\u002fdocumentation-images\\u002fre...\"]],\"hovertemplate\":\"source=diffusers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"diffusers, circle\",\"marker\":{\"color\":\"#FECB52\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"diffusers, circle\",\"showlegend\":true,\"x\":[-3.1916444,-2.9136622,-3.016878,-3.592608,-2.353267,-0.7469679,-1.0999513,-3.3033404,-3.1731076,-2.8517694,-1.2312729,-1.0369076,-1.5118306,-8.177781,-8.083991,-0.63034636,-3.0090768,-2.9060333,-2.313079,-1.9685427,-2.543825,-1.4820065,-2.155377,-2.2541904,-1.5542125,-1.9205931,-1.7712635,-2.2871492,-1.5902967,-1.8227725,-2.2006116,-1.3387574,-1.9347727,-2.3607285,-0.9329965,-1.1929555,-3.0301347,-1.0095152,-3.1802647,-4.2819304,-3.139062,-2.1901717,-2.0388265,-1.6493795,-1.6556361,-1.5257568,-1.4523535,-3.5875556,-0.69508773,-0.5377669,-3.2071204,-2.7989714,-0.5518736,-3.0436325,-0.6530429,-2.4980845,-2.254567,-2.811043,-2.6474578,-2.5831048,1.1515763,-7.5269547,-2.9943,-2.0452895,-2.593787,-2.0155492,-1.3824482,-1.5403718,-6.785893,-1.5021315,-3.1348917,-0.5907673,-1.8879216,-1.1413924,-1.8446648,-1.4192349,-1.5853968,-1.3921922,-1.314182,-1.2513554,-2.3882513,-3.0305078,-2.818756,-1.9230243,-2.4132063,-2.7989912,-3.3776445,-0.6813026,-3.0440865,-3.5751796,-0.6249112,-3.203998,-3.1660213,-1.9553839,-1.3402972,-1.9549512,-1.6209677,-2.2102363,-3.050216,-3.149138,-3.2283952,-3.1357148,-3.0865474,-3.1524808,-3.4283633,2.0770776,-2.364095,-1.6141393,-1.8272055,-2.438905,-2.7268312,-3.0328083,-2.9638984,-2.8981278,-1.490192,-2.8369894,-1.2470685,-1.4105858,-1.2921365,-1.2843304,-1.2241865,-1.0093232,-3.1722696,-2.9779403,-1.575999,-2.0002613,-1.4787923,-2.0574026,-1.5407596,-1.043342,-3.195276,-3.2309785,-1.5788201,-2.1118443,-1.4920802,-3.7342882,-1.1505806,-1.9414483,-2.8601105,-3.1111507,-0.6426683,-2.9885683,-2.4241204,1.4577374,-0.9262392,-1.6374866,-1.371139,-3.2970335,-0.55235547,-2.1240165,-1.7260216,-1.7564925,-2.011667,-1.8454225,-1.9109082,-2.0787027,-2.2671804,-2.1046388,-1.9487534,-2.2440097,-1.1717526,-2.0164135,-2.1150148,-1.1597375,-2.0511806,-2.0601327,-2.8878002,-1.8157518,-1.9792883,-2.9093752,-2.5749807,-2.5072703,-0.8596715,-1.1984657,-1.3278799,-1.693255,-1.6066446,-1.5825568,-3.261406,-1.8264643,-1.8415172,-1.2799387,-1.866552,-1.2919837,-1.3770201,-2.9563518,-3.0587487,-2.3587458,-2.222991,-2.2462485,-2.2210002,-3.024077,-1.9151685,-1.9141761,-5.3256316,-2.176533,-2.2766035,-1.9068651,-1.9168055,-2.4105964,-2.1747031,3.4431074,-2.5072129,-2.5817828,-1.9393369,-1.7456002,-2.7527626,-2.4251566,-1.1042944,-1.3743643,-1.9019034,-1.0647163,-1.2662022,-2.5902512,-1.240126,-2.9706938,-2.8018355],\"xaxis\":\"x\",\"y\":[4.0274706,3.3317757,4.241381,-2.156779,4.7602577,-4.3913574,-4.9284625,-2.4508624,-6.518288,-3.7443817,-4.866794,-4.467422,5.1020603,-3.7214181,-3.717932,4.5898952,4.372087,4.505999,5.1931043,4.9614463,4.6379824,5.218838,4.861218,4.93429,4.9679065,5.012564,5.052708,4.7240796,5.0319695,4.9723625,4.3952303,4.867374,3.1332781,5.129587,-3.6800838,-5.086068,4.290296,-5.4683065,4.473193,0.5787593,4.2774887,5.005017,5.233854,5.2644887,5.072595,5.097382,4.5621233,2.9050744,4.5741067,1.4708874,3.8420844,3.8456194,4.5144506,2.7469573,4.566358,5.1142936,5.335122,5.0387425,4.8628263,5.3480244,-2.758533,-1.4264985,4.201908,5.0824547,4.437477,5.468451,-2.2987823,-2.1432235,-3.0976615,-2.7408316,3.7331111,4.575783,4.915129,5.054144,4.8939686,4.315622,4.945,4.8905706,4.8534174,4.936081,4.754521,3.845517,-1.7694618,4.986624,4.575398,5.0896764,3.8292038,4.505943,4.491286,3.2522047,4.544849,3.9144251,3.8251066,4.905837,4.703525,4.957544,5.0016003,4.7809553,3.6177425,3.56365,3.2417698,3.6249075,3.817233,3.47142,3.0121214,-0.39886168,4.3132663,4.947797,4.417632,5.0269766,4.786683,4.080039,5.061572,4.5085435,4.5549755,2.558848,4.315301,4.368568,4.4447784,4.0625477,4.01881,3.792758,3.1328938,3.5751224,4.951213,4.109069,4.9685574,4.0160265,4.616361,4.456514,3.489949,3.8075206,4.6970534,5.1795244,-4.5566173,-2.741002,-4.8762097,5.098894,4.534366,4.609841,4.4627724,4.5452523,5.426097,-4.2922626,-5.3052435,5.0056915,-2.928031,4.121415,4.5600553,5.0418367,4.988184,4.8643074,4.892002,4.8438034,4.9586296,4.8920217,5.1387177,-2.7591426,-2.7435985,-2.8301158,-4.267159,-2.830624,-2.7818825,-4.361733,-2.9882734,-2.7987077,3.2479773,-2.9567327,-2.8787072,5.3159175,5.2531605,4.750778,-4.0863533,-4.939107,-4.9867773,-5.6327124,-5.2969327,-5.55321,-4.8730135,-5.8759856,5.116475,-3.0887659,-2.7911348,4.949594,5.0145354,4.3832364,3.6867094,5.2176785,4.860183,5.490158,5.312639,4.5880904,5.017007,5.1413274,-6.4385448,5.1992855,5.2271786,5.1547694,5.3220787,5.1940475,5.3532276,-4.5742645,4.953766,4.9914594,5.0101666,4.9739485,4.318271,4.5085974,-4.6068635,-4.90279,-3.604109,-4.5642123,5.020537,4.5193806,-4.512329,3.8127327,3.409701],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Datasets server - worker\\n\\n\\u003e Workers that pre-compute and cache the response to \\u002fsplits, \\u002ffirst-rows,...\"],[\"- `WORKER_CONTENT_MAX_BYTES`: the maximum size in bytes of the response content computed by a worker...\"],[\"- `WORKER_MAX_LOAD_PCT`: maximum load of the machine (in percentage: the max between the 1m load and...\"],[\"Also, it's possible to force the parent directory in which the temporary files (as the current job s...\"],[\"- `NUMBA_CACHE_DIR`: directory where the `numba` decorators (used by `librosa`) can write cache.\\n\\nNo...\"],[\"### First rows worker\\n\\nSet environment variables to configure the `first-rows` worker (`FIRST_ROWS_`...\"],[\"- `PARQUET_AND_INFO_COMMIT_MESSAGE`: the git commit message when the worker uploads the parquet file...\"],[\"- `PARQUET_AND_INFO_SOURCE_REVISION`: the git revision of the dataset to use to prepare the parquet ...\"],[\"### Duckdb Index worker\\n\\nSet environment variables to configure the `duckdb-index` worker (`DUCKDB_I...\"],[\"### Descriptive statistics worker\\n\\nSet environment variables to configure the `descriptive-statistic...\"],[\"`column_statistics` content depends on the feature type, see examples below.\\n##### class_label\\n\\n\\u003cdet...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### float\\n\\nBin size for histogram is counted as `(max_value - min_value) \\u002f D...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e \\n\\n##### int\\n\\nAs bin edges for integer values also must be integers, bin size is ...\"],[\"```python\\n{\\n    \\\"column_name\\\": \\\"direction\\\",\\n    \\\"column_type\\\": \\\"int\\\",\\n    \\\"column_statistics\\\": {\\n   ...\"],[\"],\\n            \\\"bin_edges\\\": [\\n                0,\\n                3,\\n                6,\\n             ...\"],[\"\\\"nan_count\\\": 0,\\n        \\\"nan_proportion\\\": 0.0,\\n        \\\"min\\\": 0,\\n        \\\"max\\\": 6,\\n        \\\"mean\\\": 3...\"],[\"```\\n\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### string_label\\n\\nIf the number of unique values in a column (within request...\"],[\"```\\n\\u003c\\u002fp\\u003e\\n\\u003c\\u002fdetails\\u003e\\n\\n##### bool\\n\\n\\u003cdetails\\u003e\\u003csummary\\u003eexample: \\u003c\\u002fsummary\\u003e\\n\\u003cp\\u003e\\n\\n```python\\n{\\n    'column_...\"],[\"--\\ntitle: Datasets Server Admin UI\\nemoji: üìä\\ncolorFrom: gray\\ncolorTo: purple\\nsdk: gradio\\nsdk_version:...\"],[\"Filter rows in a dataset\\n\\nDatasets Server provides a `\\u002ffilter` endpoint for filtering rows in a data...\"],[\"```\\nwhere=age\\u003e30 AND (name='Simone' OR children=0)\\n```\\nwill filter the data to select only those row...\"],[\"List Parquet files\\n\\nDatasets can be published in any format (CSV, JSONL, directories of images, etc....\"],[\"The `\\u002fparquet` endpoint accepts the dataset name as its query parameter:\\n\\n\\u003cinferencesnippet\\u003e\\n\\u003cpython...\"],[\"```\\n\\u003c\\u002fpython\\u003e\\n\\u003cjs\\u003e\\n```js\\nimport fetch from \\\"node-fetch\\\";\\nasync function query(data) {\\n    const resp...\"],[\"```\\n\\u003c\\u002fcurl\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nThe endpoint response is a JSON containing a list of the dataset's ...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"ParaphraseRC\\\",\\n     ...\"],[\"},\\n    {\\n      \\\"dataset\\\": \\\"duorc\\\",\\n      \\\"config\\\": \\\"SelfRC\\\",\\n      \\\"split\\\": \\\"test\\\",\\n      \\\"url\\\": \\\"ht...\"],[\"```\\n\\n## Sharded Parquet files\\n\\nBig datasets are partitioned into Parquet files (shards) of about 500...\"],[\"```json\\n{\\n  \\\"parquet_files\\\": [\\n    {\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_pol...\"],[\"{\\n      \\\"dataset\\\": \\\"amazon_polarity\\\",\\n      \\\"config\\\": \\\"amazon_polarity\\\",\\n      \\\"split\\\": \\\"train\\\",\\n   ...\"],[\"```\\n\\nTo read and query the Parquet files, take a look at the [Query datasets from Datasets Server](p...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003ccurl\\u003e\\n```curl\\ncurl https:\\u002f\\u002fhuggingface.co\\u002fapi\\u002fdatasets\\u002fduorc\\u002fparquet \\\\\\n        -X GET \\\\\\n ...\"],[\"```\\n\\nOptionally you can specify which configuration name to return, as well as which split:\\n\\n\\u003cinfere...\"],[\"datasets-server Helm chart\\n\\nThe `datasets-server` Helm [chart](https:\\u002f\\u002fhelm.sh\\u002fdocs\\u002ftopics\\u002fcharts\\u002f) ...\"],[\"How to contribute to the Datasets Server?\\n\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fCon...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n   ```bash\\n   git checkout -b a-descr...\"],[\"DuckDB\\n\\n[DuckDB](https:\\u002f\\u002fduckdb.org\\u002fdocs\\u002f) is a database that supports reading and querying Parquet ...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nNow you can write and execute your SQL query on the Parquet file:\\n\\n\\u003ci...\"],[\"```\\n\\u003c\\u002fjs\\u003e\\n\\u003c\\u002finferencesnippet\\u003e\\n\\nTo query multiple files - for example, if the dataset is sharded:\\n\\n\\u003ci...\"],[\"Overview\\n\\nDatasets Server automatically converts and publishes public datasets less than 5GB on the ...\"],[\"Pandas\\n\\n[Pandas](https:\\u002f\\u002fpandas.pydata.org\\u002fdocs\\u002findex.html) is a popular DataFrame library for data ...\"]],\"hovertemplate\":\"source=datasets-server\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets-server, circle\",\"marker\":{\"color\":\"#636efa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets-server, circle\",\"showlegend\":true,\"x\":[0.9293697,0.6982015,0.9359866,1.5400056,2.216878,1.184522,2.889543,2.9499502,1.9287657,-4.066555,-3.7669704,-4.2331185,-4.4791,-4.2514286,-4.5706253,-4.33223,-4.1139917,-3.802346,14.098014,0.75313073,-0.11308172,1.8610027,1.9649938,1.874675,2.0073054,2.0574627,3.6494656,0.64835507,2.2784162,2.0436084,2.0258224,1.993008,2.0440004,1.1529623,2.3158572,3.4226909,1.9042792,1.7881558,1.9908979,1.8640976,1.745385],\"xaxis\":\"x\",\"y\":[-4.3743753,-3.737139,-3.6513736,-4.845158,-4.59277,-4.105936,-4.9643397,-4.9085574,-4.7043533,-8.142509,-8.342823,-8.212337,-8.066498,-8.441129,-8.058086,-8.211051,-8.00371,-8.326813,2.0393705,-6.5320296,-7.093811,-5.3018923,-5.159466,-3.3880048,-4.2663226,-4.3057375,0.07315231,-5.5964675,-3.7108824,-3.7182016,-5.031201,-4.297112,-4.283377,-4.920487,-4.454252,-4.250939,-5.4922123,-5.480406,-5.291751,-5.4923844,-5.7864413],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Differences between Dataset and IterableDataset\\n\\nThere are two types of dataset objects, a [`Dataset...\"],[\"```\\n\\nStreaming can read online data without writing any file to disk.\\nFor example, you can stream da...\"],[\"```\\n\\n## Loading local files entirely and progressively\\n\\nIt is possible to convert local or remote da...\"],[\"```\\n\\nOn the other hand, due to the \\\"lazy\\\" nature of an `IterableDataset`, calling [`IterableDataset....\"],[\"```\\n\\nSince we don't have random access to the rows in the case of an `IterableDataset`, we can't use...\"],[\"```\\n\\nBut using a shuffle buffer is not enough to provide a satisfactory shuffling for machine learni...\"],[\"```\\n\\n## Speed differences\\n\\nRegular [`Dataset`] objects are based on Arrow which provides fast random...\"],[\"```\\n\\n\\nIn this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approxi...\"],[\"```\\n\\nIf you want to shuffle your dataset or [use it with a PyTorch DataLoader](.\\u002fuse_with_pytorch#st...\"],[\"Metric Card for F1\\n\\n\\n## Metric Description\\n\\nThe F1 score is the harmonic mean of the precision and r...\"],[\"```\\n\\n\\n### Inputs\\n- **predictions** (`list` of `int`): Predicted labels.\\n- **references** (`list` of ...\"],[\"### Output Values\\n- **f1**(`float` or `array` of `float`): F1 score or list of f1 scores, depending ...\"],[\"```\\n```python\\n{'f1': array([0.8, 0.0, 0.0])}\\n```\\n\\nThis metric outputs a dictionary, with either a si...\"],[\"```\\n\\nExample 4-A multiclass example, with different values for the `average` input.\\n```python\\n\\u003e\\u003e\\u003e pr...\"],[\"Cache management\\n\\nWhen you download a dataset, the processing scripts and data are stored locally on...\"],[\"```\\n\\nRefer to [`DownloadMode`] for a full list of download modes.\\n\\n## Cache files\\n\\nClean up the cach...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nKeeping the predictions in-memory is not possible in a distributed settin...\"],[\"Metric Card for MSE\\n\\n\\n## Metric Description\\n\\nMean Squared Error(MSE) represents the average of the s...\"],[\"```\\n\\n### Inputs\\n\\nMandatory inputs: \\n- `predictions`: numeric array-like of shape (`n_samples,`) or (...\"],[\"```\\n\\nIf `multioutput=\\\"raw_values\\\"`:\\n```python\\n{'mse': array([0.41666667, 1. ])}\\n```\\n\\n#### Values fro...\"],[\"```\\n\\n## Limitations and Bias\\nMSE has the disadvantage of heavily weighting outliers -- given that it...\"],[\"All about metrics\\n\\n\\u003cTip warning={true}\\u003e\\n\\nMetrics is deprecated in ü§ó Datasets. To learn more about ho...\"],[\"## Distributed evaluation\\n\\nComputing metrics in a distributed environment can be tricky. Metric eval...\"],[\"Metric Card for METEOR\\n\\n## Metric description\\n\\nMETEOR (Metric for Evaluation of Translation with Exp...\"],[\"```\\n\\n## Output values\\n\\nThe metric outputs a dictionary containing the METEOR score. Its values range...\"],[\"```\\n\\n## Limitations and bias\\n\\nWhile the correlation between METEOR and human judgments was measured ...\"],[\"Preprocess\\n\\nIn addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of...\"],[\"```\\n\\nGrab a dataset of your choice and follow along!\\n\\n## Tokenize text\\n\\nModels cannot process raw te...\"],[\"```\\n\\n**2**. Call your tokenizer on the first row of `text` in the dataset:\\n\\n```py\\n\\u003e\\u003e\\u003e tokenizer(data...\"],[\"```\\n\\nThe tokenizer returns a dictionary with three items:\\n\\n- `input_ids`: the numbers representing t...\"],[\"```\\n\\u003c\\u002fpt\\u003e\\n\\u003ctf\\u003e\\nUse the [`~Dataset.to_tf_dataset`] function to set the dataset format to be compatibl...\"],[\"```\\n\\u003c\\u002ftf\\u003e\\n\\u003c\\u002fframeworkcontent\\u003e\\n\\n**5**. The dataset is now ready for training with your machine learni...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `audio` column of the dataset...\"],[\"```\\n\\n**3**. Reading a dataset card is incredibly useful and can give you a lot of information about ...\"],[\"```\\n\\n**4**. Use the [`~Dataset.map`] function to resample the entire dataset to 16kHz. This function...\"],[\"```\\n\\n**2**. Index into the first row of the dataset. When you call the `image` column of the dataset...\"],[\"Image classification\\n\\nImage classification datasets are used to train a model to classify an entire ...\"],[\"```\\n\\nThe dataset has three fields:\\n\\n* `image`: a PIL image object.\\n* `image_file_path`: the path to ...\"],[\"```\\n\\n\\u003cdiv class=\\\"flex justify-center\\\"\\u003e\\n    \\u003cimg class=\\\"block dark:hidden\\\" src=\\\"https:\\u002f\\u002fhuggingface.c...\"],[\"Beam Datasets\\n\\nSome datasets are too large to be processed on a single machine. Instead, you can pro...\"],[\"```\\n\\n4. Run the pipeline:\\n\\n```\\ndatasets-cli run_beam datasets\\u002f$DATASET_NAME \\\\\\n--name $CONFIG_NAME \\\\\\n...\"],[\"Load image data\\n\\nImage datasets have [`Image`] type columns, which contain PIL objects. \\n\\n\\u003cTip\\u003e\\n\\nTo ...\"],[\"```\\n\\nIf you only want to load the underlying path to the image dataset without decoding the image ob...\"],[\"```\\n\\nLoad your dataset by specifying `imagefolder` and the directory of your dataset in `data_dir`:\\n...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nFor more information about creating your own `ImageFolder` dataset, take a look at the [...\"],[\"Metric Card for Recall\\n\\n\\n## Metric Description\\n\\nRecall is the fraction of the positive examples that...\"],[\"### Inputs\\n- **predictions** (`list` of `int`): The predicted labels.\\n- **references** (`list` of `i...\"],[\"- `'samples'`: Calculate metrics for each instance, and find their average (only meaningful for mult...\"],[\"### Output Values\\n- **recall**(`float`, or `array` of `float`, for multiclass targets): Either the g...\"],[\"```\\n```python\\n{'recall': array([1., 0., 0.])}\\n```\\n\\nThis metric outputs a dictionary with one entry, ...\"],[\"```\\n\\nExample 4-A multiclass example, using different averages.\\n```python\\n\\u003e\\u003e\\u003e recall_metric = dataset...\"],[\"Dataset features\\n\\n[`Features`] defines the internal structure of a dataset. It is used to specify th...\"],[\"```\\n\\nThe [`Value`] feature tells ü§ó Datasets:\\n\\n- The `idx` data type is `int32`.\\n- The `sentence1` an...\"],[\"```\\n\\nThe `answers` field is constructed using the [`Sequence`] feature because it contains two subfi...\"],[\"```\\n\\n## Audio feature\\n\\nAudio datasets have a column with type [`Audio`], which contains three import...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nIndex into an audio dataset using the row index first and then the `audio...\"],[\"```\\n\\n## Image feature\\n\\nImage datasets have a column with type [`Image`], which loads `PIL.Image` obj...\"],[\"```\\n\\nDepending on the dataset, you may get the path to the local downloaded image, or the content of...\"],[\"Metric Card for GLUE\\n\\n## Metric description\\nThis metric is used to compute the GLUE evaluation metri...\"],[\"```\\n## Output values\\n\\nThe output of the metric depends on the GLUE subset chosen, consisting of a di...\"],[\"### Values from popular papers\\nThe [original GLUE paper](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fglue) repor...\"],[\"```\\n\\nMinimal values for the STSB subset (which outputs `pearson` and `spearmanr`):\\n\\n```python\\nfrom d...\"],[\"```\\n    \\n## Further References \\n\\n- [GLUE benchmark homepage](https:\\u002f\\u002fgluebenchmark.com\\u002f)\\n- [Fine-tun...\"],[\"Metric Card for Matthews Correlation Coefficient\\n\\n## Metric Description\\nThe Matthews correlation coe...\"],[\"```\\n\\nThe same example as above, but also including sample weights:\\n```python\\n\\u003e\\u003e\\u003e matthews_metric = d...\"],[\"```\\n\\n## Further References\\n\\n- This Hugging Face implementation uses [this scikit-learn implementatio...\"],[\"Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"]],\"hovertemplate\":\"source=datasets\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"datasets, circle\",\"marker\":{\"color\":\"#EF553B\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"datasets, circle\",\"showlegend\":true,\"x\":[0.29055056,0.39552936,0.47078153,0.42699164,0.4432805,0.22538404,0.4363113,0.4145549,0.3494464,-4.470768,-3.9682853,-4.408667,-4.521629,-4.613169,1.2421316,1.350072,0.4574759,-4.388453,-4.302121,-4.4545145,-7.203997,-3.9821923,-3.9485023,-4.2149005,-4.323827,-7.1580553,-2.561119,-6.1758556,-5.3195777,-5.358363,-2.8805964,10.958637,11.111729,10.967778,10.91447,-1.1267855,-0.54986817,-0.81992304,-3.0855856,0.58481246,0.7325145,-0.29740703,0.12639016,0.23995164,0.12688385,-4.52297,-4.037996,-4.1959095,-4.400793,-4.7022586,-4.623341,-1.0265586,-0.9613365,-3.0485802,11.017954,11.00636,-0.1965918,-0.29021066,-4.233557,-4.3364415,-4.2595725,-4.456741,-2.7853992,-4.390631,-4.4770784,-2.1756277,-7.4722323,-7.569971],\"xaxis\":\"x\",\"y\":[-7.2263436,-7.1301293,-6.9537234,-7.2854776,-7.3395357,-7.2520604,-7.228653,-7.274037,-7.3192058,-9.579512,-9.346273,-9.439393,-9.499263,-9.650803,-6.331595,-6.2644687,-6.4489117,-9.474505,-9.548859,-9.604524,-1.8391551,-8.6250305,-8.784755,-9.10821,-9.352324,-2.129602,-6.648368,-6.150489,-6.846843,-6.735419,-6.5511165,-1.0967451,-0.92705417,-1.1180639,-1.079865,-7.0271263,-6.768062,-7.0123,1.3216827,-5.199704,-4.4398537,-7.121656,-7.017081,-7.0484076,-7.0882387,-9.628387,-9.331484,-9.299089,-9.594907,-9.554211,-9.675046,-7.586225,-7.3909087,-7.268348,-1.0088494,-0.9912876,-7.1515813,-7.192415,-8.921682,-9.425954,-9.299562,-9.346354,-2.7025237,-9.512413,-9.578798,-2.5169058,1.2033911,1.1758188],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"Q-Learning Recap [[q-learning-recap]]\\n\\n\\n*Q-Learning* **is the RL algorithm that** :\\n\\n- Trains a *Q-f...\"],[\"This is the Q-Learning pseudocode:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-co...\"],[\"Quiz\\n\\nThe best way to learn and [to avoid the illusion of competence](https:\\u002f\\u002fwww.coursera.org\\u002flectu...\"],[\"### Q2: Which of the following statements are true, when talking about models with bias and\\u002for varia...\"],[\"### Q4: How would you describe, with your own words, the Actor-Critic Method (A2C)?\\n\\n\\u003cdetails\\u003e\\n\\u003csumm...\"],[\"\\u003c\\u002fdetails\\u003e\\n\\nCongrats on finishing this Quiz ü•≥, if you missed some elements, take time to read the ch...\"],[\"Additional Readings\\n\\nThese are **optional readings** if you want to go deeper.\\n\\n\\n## Introduction to ...\"],[\"Hands-on\\n\\nNow that you learned the basics of multi-agents, you're ready to train your first agents i...\"],[\"More precisely, AI vs. AI is three tools:\\n\\n- A *matchmaking process* defining the matches (which mod...\"],[\"### Competition Rules\\n\\nThis first AI vs. AI competition **is an experiment**: the goal is to improve...\"],[\"```\\n\\nTo be able to train our agents correctly and push to the Hub, we need to install ML-Agents\\n\\n```...\"],[\"```\\n\\nFinally, you need to install git-lfs: https:\\u002f\\u002fgit-lfs.com\\u002f\\n\\nNow that it‚Äôs installed, we need to...\"],[\"The goal in this environment **is to get the ball into the opponent's goal while preventing the ball...\"],[\"But in our case we‚Äôre 2vs2, and each team has 2 agents. How then can we **train cooperative behavior...\"],[\"The solution then is to use Self-Play with an MA-POCA trainer (called poca). The poca trainer will h...\"],[\"```\\n\\nCompared to Pyramids or SnowballTarget, we have new hyperparameters with a self-play part. How ...\"],[\"```\\n\\nThe executable contains 8 copies of SoccerTwos.\\n\\n‚ö†Ô∏è It‚Äôs normal if you don‚Äôt see a big increase...\"],[\"```\\n\\nThen, we need to run `mlagents-push-to-hf`.\\n\\nAnd we define four parameters:\\n\\n1. `-run-id`: the ...\"],[\"```\\n\\nIf everything worked you should see this at the end of the process (but with a different url üòÜ)...\"],[\"2. That you have a `SoccerTwos.onnx` file\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-dee...\"],[\"Two types of value-based methods [[two-types-value-based-methods]]\\n\\nIn value-based methods,¬†**we lea...\"],[\"And consequently,¬†**we don't define by hand the behavior of our policy; it's the training that will ...\"],[\"In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that hand...\"],[\"The value of taking action \\\\\\\\(a\\\\\\\\) in state \\\\\\\\(s\\\\\\\\) under a policy \\\\\\\\(œÄ\\\\\\\\) is:\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhu...\"],[\"The advantages and disadvantages of policy-gradient methods\\n\\nAt this point, you might ask, \\\"but Deep...\"],[\"Our vacuum cleaner can only perceive where the walls are.\\n\\nThe problem is that the **two red (colore...\"],[\"But what if we have an infinite possibility of actions?\\n\\nFor instance, with a self-driving car, at e...\"],[\"Glossary \\n\\nThis is a community-created glossary. Contributions are welcome!\\n\\n- **Deep Q-Learning:** ...\"],[\"Conclusion [[conclusion]]\\n\\nCongrats on finishing this unit! **That was the biggest one**, and there ...\"],[\"Introduction\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fres...\"],[\"Introduction to Q-Learning [[introduction-q-learning]]\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhu...\"],[\"Conclusion\\n\\nCongrats on finishing this unit! You‚Äôve just trained your first ML-Agents and shared it ...\"],[\"Language models in RL\\n## LMs encode useful knowledge for agents\\n\\n**Language models** (LMs) can exhib...\"],[\"1) Sample inefficiency\\n\\n2) Unexpected behaviors from humans‚Äô eyes\\n\\nAs a first attempt, the paper [‚ÄúG...\"],[\"## Further reading\\n\\nFor more information we recommend you check out the following resources:\\n\\n- [Goo...\"],[\"The Deep Q-Network (DQN)  [[deep-q-network]]\\nThis is the architecture of our Deep Q-Learning network...\"],[\"**Why do we stack four frames together?**\\nWe stack frames together because it helps us **handle the ...\"],[\"So, we see that Deep Q-Learning uses a neural network to approximate, given a state, the different Q...\"],[\"The certification process\\n\\n\\nThe certification process is **completely free**:\\n\\n- To get a *certifica...\"],[\"Summary [[summary]]\\n\\nThat was a lot of information! Let's summarize:\\n\\n- Reinforcement Learning is a ...\"],[\"Glossary [[glossary]]\\n\\nThis is a community-created glossary. Contributions are welcomed!\\n\\n\\n### Strat...\"],[\"### Greedy strategy:\\n\\n- Involves always choosing the action that is expected to lead to the highest ...\"],[\"The Reinforcement Learning Framework [[the-reinforcement-learning-framework]]\\n\\n## The RL Process [[t...\"],[\"This RL loop outputs a sequence of¬†**state, action, reward and next state.**\\n\\n\\u003cimg src=\\\"https:\\u002f\\u002fhugg...\"],[\"- *State s*: is **a complete description of the state of the world** (there is no hidden information...\"],[\"## Action Space [[action-space]]\\n\\nThe Action space is the set of¬†**all possible actions in an enviro...\"],[\"Taking this information into consideration is crucial because it will¬†**have importance when choosin...\"],[\"\\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fhuggingface-deep-rl-course\\u002fcourse-images\\u002fresolve\\u002fmain\\u002fen\\u002fu...\"]],\"hovertemplate\":\"source=deep-rl-class\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"deep-rl-class, circle\",\"marker\":{\"color\":\"#00cc96\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"deep-rl-class, circle\",\"showlegend\":true,\"x\":[-9.132021,-7.5484033,-8.841248,-8.892261,-9.302159,-7.9310718,-9.24314,-8.472808,-8.294321,-8.143943,0.5704075,1.0065352,-8.706671,-8.329025,-8.432161,-8.380518,-8.283743,2.9716709,0.22765829,-8.473006,-9.52178,-9.459398,-9.400334,-9.384653,-9.376422,-9.319181,-9.475648,-9.2769165,-8.357331,-8.6842985,-8.998373,-8.438196,-5.774557,-5.6938763,-6.2148767,-8.964854,-8.951579,-9.054319,-8.019743,-9.36033,-9.4230385,-9.3804245,-9.2532625,-9.388133,-8.790524,-9.040718,-9.301814,-9.28232],\"xaxis\":\"x\",\"y\":[0.26342267,0.38911417,-0.3091458,-0.5015769,-0.024036484,-2.0852973,-0.15813018,0.14832142,0.10537725,0.00032473193,-3.5332417,-3.6703255,0.051797777,0.05397482,0.084365815,0.029529642,0.09441718,-4.3810096,-3.8723547,0.1250173,0.23692076,0.2551168,0.27094898,0.12508452,0.23281753,0.28129017,0.26178393,0.25594065,-0.051786665,0.13313524,0.22922069,0.03350293,-2.3138902,-2.168363,-1.4925205,0.10552603,-2.5292192,0.24885388,0.13278171,0.35448077,0.2464734,0.14826159,0.21609786,0.1911677,-0.23994298,0.017822102,0.04875655,0.0013968459],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"### Onnxruntime Training\\n\\nThe following example fine-tunes a T5 large model on the wmt16 dataset.\\n\\n`...\"],[\"```\\n\\n### Performance\\n\\nWe get the following results for [t5-large](https:\\u002f\\u002fhuggingface.co\\u002ft5-large) m...\"],[\"Inference pipelines with the ONNX Runtime accelerator\\n\\nThe [`~pipelines.pipeline`] function makes it...\"],[\"```\\n\\n_Note: The default models used in the [`~pipelines.pipeline`] function are not optimized for in...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path, export=True)`\\nmeth...\"],[\"```\\n\\nIt is also possible to load it with the `from_pretrained(model_name_or_path)`\\nmethod associated...\"],[\"```\\n\\n\\n## Optimizing and quantizing in pipelines\\n\\nThe [`~pipelines.pipeline`] function can not only r...\"],[\"\\u003e\\u003e\\u003e # Load the quantized model from a local repository\\n\\u003e\\u003e\\u003e model = ORTModelForSequenceClassification...\"],[\"```\\n\\n### Optimizing with `ORTOptimizer`\\n\\n```python\\n\\u003e\\u003e\\u003e from transformers import AutoTokenizer\\n\\u003e\\u003e\\u003e fr...\"],[\"# Save and push the model to the hub\\n\\u003e\\u003e\\u003e tokenizer.save_pretrained(\\\"new_path_for_directory\\\")  # doct...\"],[\"Quantization\\n\\n## AutoGPTQ Integration\\n\\nü§ó Optimum collaborated with [AutoGPTQ library](https:\\u002f\\u002fgithub...\"],[\"- Install latest `accelerate` library:\\n`pip install --upgrade accelerate`\\n\\n### Load and quantize a m...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\nGPTQ quantization only works for text model for now. Futhermore, the quant...\"],[\"```\\n\\n### Exllama kernels for faster inference\\n\\nWith the release of exllamav2 kernels, you can get fa...\"],[\"```\\n\\nNote that only 4-bit models are supported with exllama\\u002fexllamav2 kernels for now. Furthermore, ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"### ORTModelForMultipleChoice\\n\\n[[autodoc]] onnxruntime.ORTModelForMultipleChoice\\n\\n### ORTModelForQue...\"],[\"### ORTModelForVision2Seq\\n\\n[[autodoc]] onnxruntime.ORTModelForVision2Seq\\n    - forward\\n\\n### ORTModel...\"],[\"#### ORTLatentConsistencyModelPipeline\\n\\n[[autodoc]] onnxruntime.ORTLatentConsistencyModelPipeline\\n  ...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"| Notebook                                                                                          ...\"],[\"|:--------------------------------------------------------------------------------------------------...\"],[\"---------------------------------------------------------------------------------------------|:-----...\"],[\"-|:-------------------------------------------------------------------------------------------------...\"],[\"-----------------------------------------------------------------------------------------|:---------...\"],[\"------------------|---------------------------------------------------------------------------------...\"],[\"-------------------------------------:|...\"],[\"| [How to use DeepSpeed to train models with billions of parameters on Habana Gaudi](https:\\u002f\\u002fgithub....\"],[\"## Optimum Intel\\n\\n### OpenVINO...\"],[\"| Notebook                                                                                          ...\"],[\"| [How to run inference with OpenVINO](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-intel\\u002fblob\\u002fmain\\u002fnotebo...\"],[\"| [How to quantize a question answering model with NNCF](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002foptimum-inte...\"],[\"| [Compare outputs of a quantized Stable Diffusion model with its full-precision counterpart](https:...\"],[\"### Neural Compressor...\"],[\"| [How to quantize a model with Intel Neural Compressor for text classification](https:\\u002f\\u002fgithub.com\\u002f...\"],[\"## Optimum ONNX Runtime...\"],[\"| Notebook                                                                                          ...\"],[\"----------------------------------------------------------------------------------|:----------------...\"],[\"-------------------------------------------|:-------------------------------------------------------...\"],[\"----------------------------------------------------------------|-----------------------------------...\"],[\"-----------------------------------------------------------------------------------:|...\"],[\"| [How to quantize a model with ONNX Runtime for text classification](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"| [How to fine-tune a model for text classification with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingfac...\"],[\"| [How to fine-tune DeBERTa for question-answering with ONNX Runtime](https:\\u002f\\u002fgithub.com\\u002fhuggingface...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Install...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nStatic quantization relies on feeding batches of data through the model to estimate the activat...\"],[\"# Define the processing function to apply to each example after loading the dataset\\n\\u003e\\u003e\\u003e def preproce...\"],[\"```\\n\\nAs a final example, let's take a look at applying _graph optimizations_ techniques such as oper...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fintel\\u002finf...\"],[\"```\\n\\nLet's see now how we can apply dynamic quantization with ONNX Runtime:\\n\\n```python\\n\\u003e\\u003e\\u003e from opti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fonnxrunti...\"],[\"```\\n\\nYou can find more examples in the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fhabana\\u002fqu...\"],[\"```\\n\\nCheck out the help for more options:\\n\\n```bash\\noptimum-cli export onnx --help\\n```\\n\\nCheck out the...\"],[\"```\\n\\nCheck out the [documentation](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002foptimum\\u002fbettertransformer\\u002foverview) f...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!---\\nCopyright 2022 The HuggingFace Team. All rights reserved.\\nLicensed under the Apache License, Ve...\"],[\"```\\n\\n\\n__Note__\\n\\u003e *To enable ONNX Runtime training, your devices need to be equipped with GPU. Instal...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-3...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eOptimize your model to speedup inference with \\u003cspan class=\\\"underline\\\" oncli...\"],[\"\\u003c\\u002fa\\u003e\\n    \\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" ...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eEnable performance optimizations for \\u003cspan class=\\\"underline\\\" onclick=\\\"event...\"],[\"\\u003cp class=\\\"text-gray-700\\\"\\u003eApply quantization and graph optimization to accelerate Transformers models...\"]],\"hovertemplate\":\"source=optimum\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"optimum, circle\",\"marker\":{\"color\":\"#ab63fa\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"optimum, circle\",\"showlegend\":true,\"x\":[0.79926103,0.77922285,-1.4785657,-3.648396,-2.2745721,-2.2907414,-4.7872453,-4.1593666,-3.4778442,-3.7897727,-3.9309444,-3.8252506,-3.859671,1.5482947,-3.5998898,-3.3001268,-3.407346,-3.2005603,-3.0368204,-2.7643523,-1.2613053,-1.1350297,-0.55890924,0.8308396,-9.096915,5.504571,5.6924195,5.5931907,5.691281,5.6721997,5.5112176,-2.943129,-1.1496477,-9.096264,-2.441328,-4.365637,-2.7907445,-2.5441844,-3.9017553,-1.8138618,-9.1460705,5.7122335,5.6508656,5.6670146,5.5729966,-8.941083,-8.949445,-9.052211,-4.8394713,-1.9120628,-3.6498303,-3.8326743,-3.5934768,-3.0232959,-3.7283893,-2.8966606,-3.4266443,-3.6356332,-3.0576527,-3.3335197,-3.6360967,-3.1045985,-1.3792956,-3.3088243,-1.4842887,-3.2169719,-2.701366,-2.740387,-2.5509958,-2.1688747,-3.788046],\"xaxis\":\"x\",\"y\":[-0.28772184,-0.14311333,-0.93352276,-2.3203778,-3.108455,-2.6664457,-3.5064778,-4.071146,-4.539373,-4.759453,-3.4526305,-4.1873827,-3.1912763,-4.8088493,-3.2059023,-3.873113,-3.703957,-3.7320955,-3.0392144,-1.0388994,1.5642834,2.6495125,1.7654359,-0.16187568,5.1344695,0.4853554,0.5561868,0.5290252,0.5534336,0.5420298,0.48640513,-2.4730144,-1.9333812,5.2501674,-2.0777476,-2.9986253,5.2629447,-1.4458237,-2.7109578,-2.21291,5.066758,0.5569568,0.5332125,0.54405487,0.50409746,5.9118676,6.0144696,5.862964,-3.1093938,-2.702991,-2.9184136,-3.2555325,-3.3584561,-6.399768,-3.2836826,-2.0411162,-2.9937384,-3.4417846,-3.1877623,-2.6047761,-3.0099077,-2.4810731,-0.9500095,-1.834668,-3.1218321,-1.5753419,-2.0079439,-1.8098533,-1.6752408,-1.8271362,-1.9398363],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Fine-tuning for image classification using LoRA and ü§ó PEFT\\n\\n## Vision Transformer model from transfo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"[[autodoc]] auto.AutoPeftModelForTokenClassification\\n\\n## AutoPeftModelForQuestionAnswering\\n\\n[[autodo...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"## LoHaConfig\\n\\n[[autodoc]] tuners.loha.config.LoHaConfig\\n\\n## LoHaModel\\n\\n[[autodoc]] tuners.loha.mode...\"],[\"!---\\nCopyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"```\\n\\nFor example:\\n\\n```bash\\ndoc-builder preview peft docs\\u002fsource\\n```\\n\\nThe docs will be viewable at [h...\"],[\"```\\nand of course, if you moved it to another file, then:\\n\\n```\\nSections that were moved:\\n\\n[ \\u003ca href=...\"],[\"```\\n\\nUse the relative style to link to the new file so that the versioned docs continue to work.\\n\\n\\n#...\"],[\"The same works for methods so you can either use \\\\[\\\\`XXXClass.method\\\\`\\\\] or \\\\[~\\\\`XXXClass.method\\\\`\\\\]...\"],[\"```\\n    Args:\\n        n_layers (`int`): The number of layers of the model.\\n```\\n\\nIf the description i...\"],[\"```\\n```python\\n# first line of code\\n# second line\\n# etc\\n```\\n````\\n\\n#### Writing a return block\\n\\nThe re...\"],[\"```\\n    Example:\\n\\n    ```python\\n    \\u003e\\u003e\\u003e import time\\n    \\u003e\\u003e\\u003e from accelerate import Accelerator\\n    \\u003e...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nStart by defining the model and tokenizer, text and label columns, and some hyperpara...\"],[\"```\\n\\n## Load dataset\\n\\nFor this guide, you'll train on the `sentences_allagree` subset of the [`finan...\"],[\"```\\n\\n## Preprocess dataset\\n\\nInitialize a tokenizer, and create a function to pad and truncate the `m...\"],[\"```\\n\\nCreate a [`DataLoader`](https:\\u002f\\u002fpytorch.org\\u002fdocs\\u002fstable\\u002fdata.html#torch.utils.data.DataLoader) ...\"],[\"```\\n\\nSetup the optimizer and learning rate scheduler:\\n\\n```py\\noptimizer = torch.optim.AdamW(model.par...\"],[\"```\\n\\nMove the model to the GPU, and then write a training loop to begin!\\n\\n```py\\nmodel = model.to(dev...\"],[\"```\\n\\nLet's see how well the model performs on the validation set:\\n\\n```py\\ncorrect = 0\\ntotal = 0\\nfor p...\"],[\"```\\n\\nIf you check the model file size in the repository, you'll see that it is only 3.93MB! ü§è\\n\\n## In...\"],[\"Fine-tuning a multilayer perceptron using LoRA and ü§ó PEFT\\n\\n[![Open In Colab](https:\\u002f\\u002fcolab.research....\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"However, after a model is quantized it isn't typically further trained for downstream tasks because ...\"],[\"```py\\nimport torch\\nfrom transformers import BitsAndBytesConfig\\n\\nconfig = BitsAndBytesConfig(\\n    loa...\"],[\"```\\n\\nPass the `config` to the [`~transformers.AutoModelForCausalLM.from_pretrained`] method.\\n\\n```py\\n...\"],[\"```\\n\\nYou're all set for training with whichever training method you prefer!\\n\\n### LoftQ initializatio...\"],[\"```\\n\\n## Next steps\\n\\nIf you're interested in learning more about quantization, the following may be h...\"],[\"Using PEFT with timm\\n\\n`peft` allows us to train any model with LoRA as long as the layer type is sup...\"],[\"```\\n\\nThese are the transformations steps necessary to process the image.\\n\\n\\n```python\\ntransform = cre...\"],[\"```\\n\\n## Training\\n\\nThis is just a function that performs the train loop, nothing fancy happening.\\n\\n\\n`...\"],[\"```\\n\\n### Selecting which layers to fine-tune with LoRA\\n\\nLet's take a look at the layers of our model...\"],[\"```\\n\\nFinally, let's create the `peft` model, the optimizer and criterion, and we can get started. As...\"],[\"```\\n\\n\\n```python\\npeft_model.push_to_hub(model_id);\\n```\\n\\nAs we can see, the adapter size is only 4.3 M...\"],[\"!--Copyright 2023 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"Below is a basic example usage of how to inject LoRA adapters into the submodule `linear` of the mod...\"],[\"```\\n\\nIf you print the model, you will notice that the adapters have been correctly injected into the...\"],[\"```\\n\\n## Pros and cons \\n\\nWhen to use this API and when to not use it? Let's discuss in this section t...\"],[\"``python\\nimport os\\n\\nimport torch\\nfrom transformers import (\\n    AutoTokenizer,\\n    default_data_coll...\"],[\"```\\n\\n\\n```python\\n# loading dataset\\ndataset = load_dataset(\\\"financial_phrasebank\\\", \\\"sentences_allagree...\"],[\"```\\n\\n\\n```python\\n# training and evaluation\\n\\n\\ndef compute_metrics(eval_preds):\\n    preds, labels = eva...\"],[\"```\\n\\n\\n```python\\nckpt = f\\\"{peft_model_id}\\u002fadapter_model.bin\\\"\\n!du -h $ckpt\\n```\\n\\n\\n```python\\nfrom peft i...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Setup\\n\\nLet's take care of some of the setup first so you can start training faster later. Se...\"],[\"```\\n\\n## Load dataset and metric\\n\\nThe [Common Voice 11.0](https:\\u002f\\u002fhuggingface.co\\u002fdatasets\\u002fmozilla-fou...\"],[\"```\\n\\nYou'll only be training on the `sentence` and `audio` columns, so you can remove the rest of th...\"],[\"```\\n\\nIf you look at the `sampling_rate`, you'll see the audio was sampled at 48kHz. The Whisper mode...\"],[\"```\\n\\nOnce you've cleaned up the dataset, you can write a function to generate the correct model inpu...\"],[\"```\\n\\nFinally, create a `DataCollator` class to pad the labels in each batch to the maximum length, a...\"],[\"```\\n\\n## Train\\n\\nNow that the dataset is ready, you can turn your attention to the model. Start by loa...\"],[\"```\\n\\nLet's also apply LoRA to the training to make it even more efficient. Load a [`~peft.LoraConfig...\"],[\"```\\n\\nNow you're ready to define some training hyperparameters in the [`~transformers.Seq2SeqTraining...\"],[\"```\\n\\nIt is also a good idea to write a custom [`~transformers.TrainerCallback`] to save model checkp...\"],[\"```\\n\\n## Evaluate\\n\\n[Word error rate](https:\\u002f\\u002fhuggingface.co\\u002fspaces\\u002fevaluate-metric\\u002fwer) (WER) is a co...\"],[\"```\\n\\nWrite a loop to evaluate the model performance. Set the model to evaluation mode first, and wri...\"],[\"```\\n\\n## Share model\\n\\nOnce you're happy with your results, you can upload your model to the Hub with ...\"],[\"```\\n\\nLoad an audio sample (you can listen to it in the [Dataset Preview](https:\\u002f\\u002fhuggingface.co\\u002fdata...\"]],\"hovertemplate\":\"source=peft\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"peft, circle\",\"marker\":{\"color\":\"#FFA15A\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"peft, circle\",\"showlegend\":true,\"x\":[-1.7599926,-3.2300112,-2.1035552,-0.49306464,-3.4710064,-0.5368423,2.2735941,2.3328688,2.8175695,2.5261629,2.2899656,-2.6876278,-4.6166096,-4.3032923,-5.0491805,-2.435967,-5.2401695,-3.0997405,-2.1037078,-2.1983507,-2.3422441,-2.8156047,-2.7035832,-3.1918607,-3.7400231,-2.974202,0.072597496,-3.2129717,-3.1102338,-3.8141942,-2.9811282,-1.8318309,-2.332748,-3.1159222,-2.5486171,-2.2223306,-1.6940324,-2.7590923,-2.8015041,-2.6406138,-4.324746,-3.2382033,-3.3353653,-3.306924,-3.9201999,-1.0341938,10.980399,10.927418,10.988054,10.943691,-3.9777489,-3.3315318,-3.0357008,-3.0067554,-2.9785469,-4.0587726,-2.9591057,-2.1551447,11.014463],\"xaxis\":\"x\",\"y\":[-1.0454824,-1.1780106,-1.1208328,1.6662948,-1.7352636,1.5598136,-3.4641118,-3.5181916,-3.1713896,-3.3467822,-1.1780276,-6.5084267,-7.063219,-6.891863,-3.332856,-5.0695677,-3.7030184,-6.614934,-4.1414547,-4.8408933,-5.8577223,-6.576765,-4.7277746,-2.7634003,-3.0215523,-3.5497882,-2.6116831,-4.429676,-3.515112,-2.966154,-0.9405342,-6.9227753,-6.34615,-3.5029614,-4.038496,-4.7185273,-1.3482405,-4.3699584,-4.532596,-3.068232,-5.392307,-6.534295,-7.8572984,-5.164167,-2.9514883,-3.5152433,-1.0508747,-1.2078444,-1.0366735,-1.1318561,-6.7338624,-4.2429495,-2.950718,-6.198426,-5.9072647,-8.439808,-6.7335143,-4.311282,-0.9919102],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"p align=\\\"center\\\"\\u003e\\n  \\u003cbr\\u002f\\u003e\\n    \\u003cimg alt=\\\"huggingface_hub library logo\\\" src=\\\"https:\\u002f\\u002fhuggingface.co\\u002fda...\"],[\"\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003ci\\u003eHuggingface Hub ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§ø‡§ï‡§æ‡§∞‡§ø‡§ï ‡§™‡§æ‡§Ø‡§•‡§® ‡§ï‡•ç‡§≤‡§æ‡§á‡§Ç‡§ü‡•§\\u003c\\u002fi\\u003e\\n\\u003c\\u002fp\\u003e\\n\\n\\u003cp align=\\\"center\\\"...\"],[\"\\u003ch4 align=\\\"center\\\"\\u003e\\n    \\u003cp\\u003e\\n        \\u003ca href=\\\"https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingface_hub\\u002fblob\\u002fmai...\"],[\"---\\n\\n## huggingface_hub ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à\\n\\n`huggingface_hub` ‡§≤‡§æ‡§á‡§¨‡•ç‡§∞‡•á‡§∞‡•Ä ‡§Ü‡§™‡§ï‡•ã [‡§π‡§ó‡§ø‡§Ç‡§ó ‡§´‡•á‡§∏ ‡§π‡§¨...\"],[\"## ‡§™‡•ç‡§∞‡§Æ‡•Å‡§ñ ‡§µ‡§ø‡§∂‡•á‡§∑‡§§‡§æ‡§ê‡§Ç\\n\\n- [‡§´‡§º‡§æ‡§á‡§≤‡•á‡§Ç ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ï‡§∞‡•á‡§Ç](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002f...\"],[\"```\\n\\n‡§Ø‡§¶‡§ø ‡§Ü‡§™ ‡§ö‡§æ‡§π‡•á‡§Ç, ‡§§‡•ã ‡§Ü‡§™ ‡§á‡§∏‡•á [conda](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002finstallation#ins...\"],[\"```\\n\\n‡§Ø‡§æ ‡§è‡§ï ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§≠‡§Ç‡§°‡§æ‡§∞\\n\\n```py\\nfrom huggingface_hub import snapshot_download\\n\\nsnapshot_download(\\\"st...\"],[\"```\\n\\n‡§Ø‡§æ ‡§è‡§ï ‡§∏‡§Ç‡§™‡•Ç‡§∞‡•ç‡§£ ‡§´‡§º‡•ã‡§≤‡•ç‡§°‡§∞\\n\\n```py\\nfrom huggingface_hub import upload_folder\\n\\nupload_folder(\\n    fold...\"],[\"```\\n\\n[‡§Ö‡§™‡§≤‡•ã‡§° ‡§ó‡§æ‡§á‡§°](https:\\u002f\\u002fhuggingface.co\\u002fdocs\\u002fhuggingface_hub\\u002fen\\u002fguides\\u002fupload) ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§µ‡§∞‡§£ ‡§ï‡•á ‡§≤‡§ø‡§è‡•§\\n\\n#...\"],[\"‡§´‡§æ‡§Ø‡§¶‡•á ‡§Ø‡•á ‡§π‡•à‡§Ç:\\n\\n- ‡§™‡•Å‡§∏‡•ç‡§§‡§ï‡§æ‡§≤‡§Ø‡•ã‡§Ç ‡§î‡§∞ ‡§â‡§®‡§ï‡•á ‡§â‡§™‡§Ø‡•ã‡§ó‡§ï‡§∞‡•ç‡§§‡§æ‡§ì‡§Ç ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡§ø‡§É‡§∂‡•Å‡§≤‡•ç‡§ï ‡§Æ‡•â‡§°‡§≤ ‡§Ø‡§æ ‡§°‡•á‡§ü‡§æ‡§∏‡•á‡§ü ‡§π‡•ã‡§∏‡•ç‡§ü‡§ø‡§Ç‡§ó‡•§\\n- ‡§ó‡§ø‡§ü-‡§Ü‡§ß...\"],[\"## ‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® (‡§∏‡•Å‡§µ‡§ø‡§ß‡§æ ‡§Ö‡§®‡•Å‡§∞‡•ã‡§ß, ‡§¨‡§ó, ‡§Ü‡§¶‡§ø) ‡§ï‡§æ ‡§Ö‡§§‡§ø ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à üíôüíöüíõüíúüß°‚ù§Ô∏è\\n\\n‡§Ø‡•ã‡§ó‡§¶‡§æ‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§∞ ‡§ï‡§ø‡§∏‡•Ä ‡§ï‡§æ ‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à ‡§î‡§∞ ‡§π...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"# Download from a dataset\\n\\u003e\\u003e\\u003e hf_hub_download(repo_id=\\\"google\\u002ffleurs\\\", filename=\\\"fleurs.py\\\", repo_ty...\"],[\"```\\n\\n### From specific version\\n\\nBy default, the latest version from the `main` branch is downloaded....\"],[\"```\\n\\n**Note:** When using the commit hash, it must be the full-length hash instead of a 7-character ...\"],[\"```\\n\\n### Filter files to download\\n\\n[`snapshot_download`] provides an easy way to download a reposito...\"],[\"```\\n\\n## Download file(s) to local folder\\n\\nThe recommended (and default) way to download files from t...\"],[\"However, in some cases you want to download files and move them to a specific folder. This is useful...\"],[\"Here is a table that summarizes the different options to help you choose the parameters that best su...\"],[\"**Note:** if you are on a Windows machine, you need to enable developer mode or run `huggingface_hub...\"],[\"```\\n\\nYou can download multiple files at once which displays a progress bar and returns the snapshot ...\"],[\"--\\n# For reference on dataset card metadata, see the spec: https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhub-docs\\u002fb...\"],[\"## Uses\\n\\n\\u003c!-- Address questions around how the dataset is intended to be used. --\\u003e\\n\\n### Direct Use\\n\\n...\"],[\"\\u003c!-- This section describes the people or systems who originally created the data. It should also in...\"],[\"{{ bias_risks_limitations | default(\\\"[More Information Needed]\\\", true)}}\\n\\n### Recommendations\\n\\n\\u003c!-- ...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\n## Upload a folder\\n\\nUse the [`upload_folder`] function to upload a local folder to an existing ...\"],[\"```\\n\\nBy default, the `.gitignore` file will be taken into account to know which files should be comm...\"],[\"```\\n\\nYou can also use the `delete_patterns` argument to specify files you want to delete from the re...\"],[\"```\\n\\n`local_path` and `path_in_repo` are optional and can be implicitly inferred. If `local_path` is...\"],[\"```\\n\\n\\u003cTip\\u003e\\n\\nBackground jobs are queued when using `run_as_future=True`. This means that you are guar...\"],[\"```\\n\\n### Upload a folder by chunks\\n\\n[`upload_folder`] makes it easy to upload an entire folder to th...\"],[\"```\\n\\nIf you want a better control on the upload strategy (i.e. the commits that are created), you ca...\"],[\"```py\\n\\u003e\\u003e\\u003e import json\\n\\u003e\\u003e\\u003e import uuid\\n\\u003e\\u003e\\u003e from pathlib import Path\\n\\u003e\\u003e\\u003e import gradio as gr\\n\\u003e\\u003e\\u003e from ...\"],[\"```\\n\\nAnd that's it! User input\\u002foutputs and feedback will be available as a dataset on the Hub. By us...\"],[\"#### Space persistence demo\\n\\nPersisting data from a Space to a Dataset on the Hub is the main use ca...\"],[\"# 2. Zip png files in a single archive\\n        with tempfile.TemporaryDirectory() as tmpdir:\\n       ...\"],[\"```\\n\\nWhen you overwrite `push_to_hub`, you have access to the attributes of [`CommitScheduler`] and ...\"],[\"- [`CommitOperationCopy`] copies a file within a repository. This operation accepts three arguments:...\"],[\"```\\n\\n2. Pass your operations to [`create_commit`]:\\n\\n```py\\n\\u003e\\u003e\\u003e api.create_commit(\\n...     repo_id=\\\"ly...\"],[\"```\\n\\nIn addition to [`upload_file`] and [`upload_folder`], the following functions also use [`create...\"],[\"\\u003c\\u002fTip\\u003e\\n\\nHere is a simple example illustrating how to pre-upload files:\\n\\n```py\\n\\u003e\\u003e\\u003e from huggingface_h...\"],[\"```\\n\\nFirst, we create the [`CommitOperationAdd`] objects one by one. In a real-world example, those ...\"],[\"- **Start small**: We recommend starting with a small amount of data to test your upload script. It'...\"],[\"\\u003cTip warning={true}\\u003e\\n\\nAlthough [`Repository`] is not formally deprecated, we recommend using the HTT...\"],[\"```\\n\\nYou should install this for each repository that has a very large file. Once installed, you'll ...\"],[\"```\\n\\nYou can check the status of your push with the `command_queue` method:\\n\\n```python\\n\\u003e\\u003e\\u003e last_comm...\"],[\"```\\n\\nHowever, if you aren't ready to push a file yet, you can use [`~Repository.git_add`] and [`~Rep...\"],[\"!---\\nCopyright 2020 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, V...\"],[\"There are many ways you can contribute to this client library:\\n* Fixing outstanding issues with the ...\"],[\"If your issue is well written we're already 80% of the way there by the time you post it!\\n\\n## Submit...\"],[\"```\\n\\n3. Create a new branch to hold your development changes, and do this for every new PR you work ...\"],[\"```\\n\\n   This command will update your code to comply with the standards of the `huggingface_hub` rep...\"],[\"```\\n\\n   Please write [good commit messages](https:\\u002f\\u002fchris.beams.io\\u002fposts\\u002fgit-commit\\u002f).\\n\\n   It is a g...\"],[\"```\\n\\n10. Once you are satisfied (**and the [checklist below](https:\\u002f\\u002fgithub.com\\u002fhuggingface\\u002fhuggingf...\"],[\"### Tests\\n\\nAn extensive test suite is included to test the library behavior and several examples. Li...\"],[\"```\\n\\nYou can specify a smaller set of tests in order to test only the feature you're working on.\\n\\nFo...\"],[\"# Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pled...\"],[\"## Enforcement\\n\\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\\nreported ...\"],[\"**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attri...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"```\\n\\nIf the CLI is correctly installed, you should see a list of all the options available in the CL...\"],[\"```\\n_|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      ...\"],[\"```\\n\\nFor more details about authentication, check out [this section](..\\u002fquick-start#authentication)....\"],[\"```\\n\\n### Download a single file\\n\\nTo download a single file from a repo, simply provide the repo_id a...\"],[\"```\\n\\n### Download multiple files\\n\\nYou can also download a subset of the files from a repository with...\"],[\"```\\n\\nThe other approach is to provide patterns to filter which files you want to download using `--i...\"],[\"```\\n\\n### Download to a local folder\\n\\nThe recommended (and default) way to download files from the Hu...\"],[\"```\\n\\n### Specify a token\\n\\nTo access private or gated repositories, you must use a token. By default,...\"],[\"```\\n\\nTo upload the current directory at the root of the repo, use:\\n\\n```bash\\n\\u003e\\u003e\\u003e huggingface-cli uplo...\"],[\"```\\n\\n### Upload multiple files\\n\\nTo upload multiple files from a folder at once without uploading the...\"],[\"```\\n\\n**Note:** if `revision` does not exist and `--create-pr` is not set, a branch will be created a...\"],[\"```\\n\\n### Specify a token\\n\\nTo upload files, you must use a token. By default, the token saved locally...\"],[\"```\\n\\n## huggingface-cli scan-cache\\n\\nScanning your cache directory is useful if you want to know whic...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli scan-cache\\nREPO ID                     REPO TYPE SIZE ON DISK NB FILES L...\"],[\"Done in 0.0s. Scanned 6 repo(s) for a total of 3.4G.\\nGot 1 warning(s) while scanning. Use -vvv to pr...\"],[\"```\\n\\nFor more details about how to scan your cache directory, please refer to the [Manage your cache...\"],[\"```bash\\n\\u003e\\u003e\\u003e huggingface-cli env\\n\\nCopy-and-paste the text below in your GitHub issue.\\n\\n- huggingface_...\"],[\"Running Tests\\n\\nTo run the test suite, please perform the following from the root directory of this r...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"\\u003e\\u003e\\u003e # Read a remote file \\n\\u003e\\u003e\\u003e with fs.open(\\\"datasets\\u002fmy-username\\u002fmy-dataset-repo\\u002fdata\\u002ftrain.csv\\\", \\\"r...\"],[\"```\\n\\nThe optional `revision` argument can be passed to run an operation from a specific commit such ...\"],[\"```\\n\\nThe same workflow can also be used for [Dask](https:\\u002f\\u002fdocs.dask.org\\u002fen\\u002fstable\\u002fhow-to\\u002fconnect-to...\"],[\"```\\n\\n* Using the Hub as an array store with [Zarr](https:\\u002f\\u002fzarr.readthedocs.io\\u002fen\\u002fstable\\u002ftutorial.ht...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"[[autodoc]] huggingface_hub.DeleteCacheStrategy\\n    - expected_freed_size_str\\n\\n## Exceptions\\n\\n### Co...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"],[\"Let's fetch the collection with, `\\\"TheBloke\\u002frecent-models-64f9a55bb3115b4f513ec026\\\"`:\\n\\n```py\\n\\u003e\\u003e\\u003e fro...\"],[\"```\\n\\nThe [`Collection`] object returned by [`get_collection`] contains:\\n- high-level metadata: `slug...\"],[\"```\\n\\n\\u003cTip warning={true}\\u003e\\n\\nWhen listing collections, the item list per collection is truncated to 4 ...\"],[\"```\\n\\nParameter `sort` must be one of  `\\\"last_modified\\\"`,  `\\\"trending\\\"` or `\\\"upvotes\\\"`. Parameter `it...\"],[\"```\\n\\n## Manage items in a collection\\n\\nNow that we have a [`Collection`], we want to add items to it ...\"],[\"```\\n\\nIf an item already exists in a collection (same `item_id`\\u002f`item_type` pair), an HTTP 409 error ...\"],[\"```\\n\\n### Reorder items\\n\\nItems in a collection are ordered. The order is determined by the `position`...\"],[\"```\\n\\n## Delete collection\\n\\nA collection can be deleted using [`delete_collection`].\\n\\n\\u003cTip warning={t...\"],[\"--\\nlanguage:\\n- en\\nlicense: mit\\nlibrary_name: pytorch-lightning\\ntags:\\n- pytorch\\n- image-classificatio...\"],[\"--\\n[]\\n---\\n\\n# invalid-card-data\\n\\nThis card should fail when trying to load it in because the card dat...\"],[\"his document covers all steps that need to be done in order to do a release of the `huggingface_hub`...\"],[\"```\\ngit checkout main\\n   ```\\n\\n9. Update the version to contain the `.dev0` suffix:\\n```\\n__version__ =...\"],[\"!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to...\"]],\"hovertemplate\":\"source=huggingface_hub\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"huggingface_hub, circle\",\"marker\":{\"color\":\"#19d3f3\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"huggingface_hub, circle\",\"showlegend\":true,\"x\":[3.9117503,3.68646,3.6522868,4.0330653,3.8519642,3.9066253,3.8141415,3.4783473,3.9896178,4.381889,3.9393914,2.8320842,2.306962,3.0809848,2.932874,2.986695,3.0802748,2.508988,2.0609994,2.934338,3.0304513,-5.4788013,0.39598283,-6.77736,-7.258185,2.3042817,3.1150215,3.358529,3.4016416,3.3169432,3.6283205,3.5725436,3.6727672,3.5278587,3.539085,3.6895502,2.1561856,3.6270134,3.389741,3.5077252,3.6420646,3.5246694,3.5531898,3.2409732,3.319657,3.4485152,3.51499,3.4520469,2.1169832,2.29065,2.7113993,3.2421033,3.152253,3.5261042,2.8667207,2.4310408,2.3975668,-7.4441977,-7.099341,-7.5402474,2.447447,2.3830912,2.2821035,2.2090716,3.0691204,3.0722811,2.9797163,2.9568577,2.722811,3.1467633,3.4686456,3.4130893,2.4907825,2.9435716,3.0127194,1.0874718,2.4372687,2.4053726,2.4977496,2.0741622,1.494437,2.5904448,1.817758,1.6960614,2.1180282,1.8860693,1.1783553,1.2934546,1.2337605,1.1802926,1.2399658,1.3613704,1.2635181,1.3985751,1.5030359,-1.0437738,0.6871671,2.961575,3.4042962,2.6185243],\"xaxis\":\"x\",\"y\":[0.21479799,0.2664404,0.5780986,0.24546337,0.23825993,0.08547126,-0.18185656,-2.2096555,0.2873104,0.23154235,0.22949019,-5.038176,-5.6641874,-5.19066,-5.1741843,-5.232903,-5.1838856,-5.3448405,-5.1379557,-4.7608843,-5.049179,-1.4432119,-6.7510195,-1.5458242,-1.7881702,-5.175179,-4.5437922,-4.8165245,-4.752369,-4.700645,-4.9419847,-5.073398,-5.143545,-5.0653825,-5.03005,-5.21759,-5.315736,-5.113556,-5.085388,-4.4622116,-5.1264524,-4.957506,-5.008428,-4.8990674,-5.0278234,-4.821688,-4.883447,-4.652255,-2.8042312,-3.4304554,-4.0121865,-3.8819134,-3.8160443,-4.1961274,-3.9057066,-3.4000833,-2.9335778,1.2152091,1.0839207,1.0872513,-4.27699,-4.2109466,-3.9389656,-3.9859898,-4.9949207,-5.0429125,-5.016261,-4.9962697,-4.4618483,-4.5314713,-5.039797,-4.446169,-4.14416,-4.807874,-4.6785746,-2.4721754,-4.225136,-4.141256,-3.4294815,-5.4735723,-5.79865,-5.340109,-5.524687,-4.6389627,-4.196611,-3.511303,-2.9023619,-2.8263354,-2.939984,-2.6603389,-2.6875117,-2.5347478,-2.7052379,-2.6248982,-2.639053,-6.1778173,-2.8058925,-3.6610186,-4.238601,-3.7184834],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"The tokenization pipeline\\n\\nWhen calling `Tokenizer.encode` or\\n`Tokenizer.encode_batch`, the input\\nte...\"],[\"## Normalization\\n\\nNormalization is, in a nutshell, a set of operations you apply to a raw\\nstring to ...\"],[\"You can manually test that normalizer by applying it to any string:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython...\"],[\"When building a `Tokenizer`, you can\\ncustomize its normalizer by just changing the corresponding att...\"],[\"An easy way to pre-tokenize inputs is to split on spaces and\\npunctuations, which is done by the\\n`pre...\"],[\"You can combine together any `PreTokenizer` together. For instance, here is a pre-tokenizer that wil...\"],[\"As we saw in the `quicktour`, you can\\ncustomize the pre-tokenizer of a `Tokenizer` by just changing ...\"],[\"The role of the model is to split your \\\"words\\\" into tokens, using the\\nrules it has learned. It's als...\"],[\"As we saw in the quick tour, we can customize the post processor of a\\n`Tokenizer` by setting the\\ncor...\"],[\"Let's put all those pieces together to build a BERT tokenizer. First,\\nBERT relies on WordPiece, so w...\"],[\"Then we know that BERT preprocesses texts by removing accents and\\nlowercasing. We also use a unicode...\"],[\"The pre-tokenizer is just splitting on whitespace and punctuation:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e...\"],[\"And the post-processing uses the template we saw in the previous\\nsection:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003c...\"],[\"We can use this tokenizer and train on it on wikitext like in the\\n`quicktour`:\\n\\n\\u003ctokenizerslangconte...\"],[\"The `decoder` will first convert the IDs back to tokens\\n(using the tokenizer's vocabulary) and remov...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"But by changing it to a proper decoder, we get:\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{...\"],[\"Quicktour\\n\\nLet's have a quick look at the ü§ó Tokenizers library features. The\\nlibrary provides an imp...\"],[\"```\\n\\n### Training the tokenizer\\n\\nIn this tour, we will build and train a Byte-Pair Encoding (BPE)\\nto...\"],[\"To train our tokenizer on the wikitext files, we will need to\\ninstantiate a [trainer]{.title-ref}, i...\"],[\"The order in which you write the special tokens list matters: here `\\\"[UNK]\\\"` will get the ID 0,\\n`\\\"[C...\"],[\"Now, we can just call the `Tokenizer.train` method with any list of files we want to use:\\n\\n\\u003ctokenize...\"],[\"This should only take a few seconds to train our tokenizer on the full\\nwikitext dataset! To save the...\"],[\"and you can reload your tokenizer from that file with the\\n`Tokenizer.from_file`\\n`classmethod`:\\n\\n\\u003ctok...\"],[\"### Using the tokenizer\\n\\nNow that we have trained a tokenizer, we can use it on any text we want\\nwit...\"],[\"This `Encoding` object then has all the\\nattributes you need for your deep learning model (or other)....\"],[\"Similarly, the `ids` attribute will\\ncontain the index of each of those tokens in the tokenizer's\\nvoc...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"and those are the indices that correspond to the emoji in the original\\nsentence:\\n\\n\\u003ctokenizerslangcon...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"Here is how we can set the post-processing to give us the traditional\\nBERT inputs:\\n\\n\\u003ctokenizerslangc...\"],[\"Lastly, we specify the special tokens we used and their IDs in our\\ntokenizer's vocabulary.\\n\\nTo check...\"],[\"To check the results on a pair of sentences, we just pass the two\\nsentences to `Tokenizer.encode`:\\n\\n...\"],[\"You can then check the type IDs attributed to each token is correct with\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cp...\"],[\"To get the full speed of the ü§ó Tokenizers library, it's best to\\nprocess your texts by batches by usi...\"],[\"To process a batch of sentences pairs, pass two lists to the\\n`Tokenizer.encode_batch` method: the\\nli...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n\\u003cliteralinclude\\u003e\\n{\\\"path\\\": \\\"..\\u002f..\\u002fbindings\\u002fpython\\u002ftests\\u002fdocumentatio...\"],[\"In this case, the `attention mask` generated by the\\ntokenizer takes the padding into account:\\n\\n\\u003ctoke...\"],[\"```\\n\\n### Importing a pretrained tokenizer from legacy vocabulary files\\n\\nYou can also import a pretra...\"],[\"p align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fhuggingface.co\\u002flanding\\u002fassets\\u002ftokenizers\\u002ftokenizers...\"],[\"## Bindings\\n\\nWe provide bindings to the following languages (more to come!):\\n  - [Rust](https:\\u002f\\u002fgith...\"],[\"```\\n\\nYou can customize how pre-tokenization (e.g., splitting into words) is done:\\n\\n```python\\nfrom to...\"],[\"`tokenizers-linux-arm64-musl`\\n\\nThis is the **aarch64-unknown-linux-musl** binary for `tokenizers`...\"],[\"div align=\\\"center\\\"\\u003e\\n\\n  \\u003ch1\\u003e\\u003ccode\\u003ewasm-pack-template\\u003c\\u002fcode\\u003e\\u003c\\u002fh1\\u003e\\n\\n  \\u003cstrong\\u003eA template for kick start...\"],[\"Be sure to check out [other `wasm-pack` tutorials online][tutorials] for other\\ntemplates and usages ...\"],[\"```\\ncargo generate --git https:\\u002f\\u002fgithub.com\\u002frustwasm\\u002fwasm-pack-template.git --name my-project\\ncd my-...\"],[\"Post-processors\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BertProcessing\\n\\n[[autodoc]] tokenizers.processo...\"],[\"Training from memory\\n\\nIn the [Quicktour](quicktour), we saw how to build and train a\\ntokenizer using...\"],[\"Easy, right? You can use anything working as an iterator here, be it a\\n`List`{.interpreted-text role...\"],[\"With our iterator ready, we just need to launch the training. In order\\nto improve the look of our pr...\"],[\"Models\\n\\n\\u003ctokenizerslangcontent\\u003e\\n\\u003cpython\\u003e\\n## BPE\\n\\n[[autodoc]] tokenizers.models.BPE\\n\\n## Model\\n\\n[[auto...\"]],\"hovertemplate\":\"source=tokenizers\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"tokenizers, circle\",\"marker\":{\"color\":\"#FF6692\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"tokenizers, circle\",\"showlegend\":true,\"x\":[-8.293993,-8.09159,-8.342575,-8.188123,-8.134419,-8.222991,-8.270407,-6.182992,-8.22532,-8.081975,-8.09627,-8.192045,-8.32576,-8.268611,-8.325108,-8.42072,-8.382341,-8.150155,-8.129527,-8.315011,-8.294809,-8.280764,-8.185779,-8.280926,-8.202926,-8.2968,-8.51823,-8.474369,-8.558707,-8.389037,-8.374633,-8.436549,-8.401326,-8.447418,-8.133126,-6.066061,-8.51039,-8.508927,-8.359953,-7.68882,-7.655542,-7.807529,-7.984656,2.1454885,2.5632899,2.430061,2.8864515,-8.200214,-8.232745,-0.22287667,-0.67808056,-7.9845934],\"xaxis\":\"x\",\"y\":[-8.733036,-8.458039,-8.786261,-8.602731,-8.534199,-8.67107,-8.7489195,-5.4429297,-8.59846,-8.347242,-8.351001,-8.513021,-8.711175,-8.770333,-8.695831,-8.800015,-8.753934,-8.758246,-8.667021,-8.786515,-8.759215,-8.781116,-8.76045,-8.774344,-8.744261,-8.750743,-8.85783,-8.857897,-8.926997,-8.781424,-8.753737,-8.820067,-8.798914,-8.827743,-8.696537,-7.0958056,-8.880083,-8.817381,-8.768246,-7.7803144,-8.124559,-8.592325,-8.378701,-2.2041605,-2.4730487,-2.2467964,-2.7765455,-8.721239,-8.762261,-6.702923,-6.0514975,-8.69002],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\n\\nYou can then provide the `generate_map` method as an argument to the `sm.ParallelRLEnv` class, ...\"],[\"How to contribute to simulate?\\n[![Contributor Covenant](https:\\u002f\\u002fimg.shields.io\\u002fbadge\\u002fContributor%20C...\"],[\"```\\n\\n3. Create a new branch to hold your development changes:\\n\\n\\t```bash\\n\\tgit checkout -b a-descripti...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"# Simulate with Godot\\n\\n### Install in Godot 4\\nThis integration has been developed for Godot 4.x. You...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\npip install --upgrade simulate\\n```\\nBefore you merge a PR, fix the style (we use `isort` + `black...\"],[\"```\\nimport simulate as sm\\n\\nscene = sm.Scene()\\nscene += sm.Plane() + sm.Sphere(position=[0, 1, 0], ra...\"],[\"```\\npython examples\\u002fbasic\\u002fobjects.py\\n```\\n\\u003cp align=\\\"center\\\"\\u003e\\n    \\u003cbr\\u003e\\n    \\u003cimg src=\\\"https:\\u002f\\u002fuser-imag...\"],[\"```\\n# Add two copy of the sphere to the scene as children of the root node (using list will add all ...\"],[\"```\\n\\n### Visualization engine\\n\\nA default vizualization engine is provided with the vtk backend of `p...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"```\\nscene = sm.Scene.create_from(\\\"simulate-tests\\u002fBox\\u002fglTF-Embedded\\u002fBox.gltf\\\")\\n```\\n\\nor, they can be t...\"],[\"!--Copyright 2022 The HuggingFace Team. All rights reserved.\\n\\nLicensed under the Apache License, Ver...\"],[\"\\u003cdiv class=\\\"mt-10\\\"\\u003e\\n  \\u003cdiv class=\\\"w-full flex flex-col space-y-4 md:space-y-0 md:grid md:grid-cols-2...\"],[\"\\u003ca class=\\\"!no-underline border dark:border-gray-700 p-5 rounded-lg shadow hover:shadow-lg\\\" href=\\\".\\u002fc...\"]],\"hovertemplate\":\"source=simulate\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"simulate, circle\",\"marker\":{\"color\":\"#B6E880\",\"size\":[4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"simulate, circle\",\"showlegend\":true,\"x\":[1.0071714,-0.5439971,2.306124,3.5667808,0.8991565,0.7009272,1.5967565,0.9106725,1.3780879,1.2184709,1.6376482,1.2508926,1.1155779,0.85567206,1.310429,1.2670313,0.77258813,4.184991,4.413407],\"xaxis\":\"x\",\"y\":[-0.9724438,-6.6572256,-3.673117,-4.2448835,-0.26584506,-0.64334905,-1.6469352,-1.0111812,-1.675067,-1.146253,-1.6826043,-1.3034147,-1.3696997,-0.2942345,-1.3807608,-1.5516434,-0.36633924,-0.46814576,-0.48522228],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"# How to release\\n\\n# Before the release\\n\\nSimple checklist on how to make releases for `safetensors`.\\n...\"],[\"# Rust\\n\\n- `safetensors` (rust, python & node) versions don't have to be in sync but it's\\n  very comm...\"],[\"# Python\\n\\n- Edit `bindings\\u002fpython\\u002fsetup.py` to reflect new version.\\n- Edit `bindings\\u002fpython\\u002fpy_src\\u002fs...\"],[\"# Node\\n\\n- Edit `bindings\\u002fnode\\u002fpackage.json` to reflect new version.\\n- Edit `CHANGELOG.md`:\\n    - Add...\"]],\"hovertemplate\":\"source=safetensors\\u003cbr\\u003esymbol=circle\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"safetensors, circle\",\"marker\":{\"color\":\"#FF97FF\",\"size\":[4,4,4,4],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"circle\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"safetensors, circle\",\"showlegend\":true,\"x\":[2.4462008,3.1545746,3.1214533,2.9668128],\"xaxis\":\"x\",\"y\":[-3.4236288,-3.3611398,-3.4306097,-3.483541],\"yaxis\":\"y\",\"type\":\"scattergl\"},{\"customdata\":[[\"How to create a pipeline object?\"]],\"hovertemplate\":\"source=User query\\u003cbr\\u003esymbol=star\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003esize_col=%{marker.size}\\u003cbr\\u003eextract=%{customdata[0]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"User query, star\",\"marker\":{\"color\":\"black\",\"size\":[100],\"sizemode\":\"area\",\"sizeref\":0.25,\"symbol\":\"diamond\",\"line\":{\"color\":\"DarkSlateGrey\",\"width\":0},\"opacity\":1},\"mode\":\"markers\",\"name\":\"User query, star\",\"showlegend\":true,\"x\":[-0.9278808],\"xaxis\":\"x\",\"y\":[4.2659693],\"yaxis\":\"y\",\"type\":\"scattergl\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eChunk source\\u003c\\u002fb\\u003e\"},\"tracegroupgap\":0,\"itemsizing\":\"constant\"},\"margin\":{\"t\":60},\"height\":700,\"width\":1000,\"title\":{\"text\":\"\\u003cb\\u003e2D Projection of Chunk Embeddings via PaCMAP\\u003c\\u002fb\\u003e\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ec98e91e-37f4-4bac-bfab-07fdc609f553');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "df = pd.DataFrame.from_dict(\n",
        "    [\n",
        "        {\n",
        "            \"x\": documents_projected[i, 0],\n",
        "            \"y\": documents_projected[i, 1],\n",
        "            \"source\": docs_processed[i].metadata[\"source\"].split(\"/\")[1],\n",
        "            \"extract\": docs_processed[i].page_content[:100] + \"...\",\n",
        "            \"symbol\": \"circle\",\n",
        "            \"size_col\": 4,\n",
        "        }\n",
        "        for i in range(len(docs_processed))\n",
        "    ]\n",
        "    + [\n",
        "        {\n",
        "            \"x\": documents_projected[-1, 0],\n",
        "            \"y\": documents_projected[-1, 1],\n",
        "            \"source\": \"User query\",\n",
        "            \"extract\": user_query,\n",
        "            \"size_col\": 100,\n",
        "            \"symbol\": \"star\",\n",
        "        }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# visualize the embedding\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x=\"x\",\n",
        "    y=\"y\",\n",
        "    color=\"source\",\n",
        "    hover_data=\"extract\",\n",
        "    size=\"size_col\",\n",
        "    symbol=\"symbol\",\n",
        "    color_discrete_map={\"User query\": \"black\"},\n",
        "    width=1000,\n",
        "    height=700,\n",
        ")\n",
        "fig.update_traces(marker=dict(opacity=1, line=dict(width=0, color=\"DarkSlateGrey\")), selector=dict(mode=\"markers\"))\n",
        "fig.update_layout(\n",
        "    legend_title_text=\"<b>Chunk source</b>\",\n",
        "    title=\"<b>2D Projection of Chunk Embeddings via PaCMAP</b>\",\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lkad_0yyBiCS",
        "outputId": "74382cef-2c8f-49dd-87bf-7e8c5091e1df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting retrieval for user_query='How to create a pipeline object?'...\n",
            "\n",
            "==================================Top document==================================\n",
            "```\n",
            "</tf>\n",
            "</frameworkcontent>\n",
            "\n",
            "## Pipeline\n",
            "\n",
            "<Youtube id=\"tiZFewofSLM\"/>\n",
            "\n",
            "The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
            "\n",
            "<Tip>\n",
            "\n",
            "For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).\n",
            "\n",
            "</Tip>\n",
            "==================================Metadata==================================\n",
            "{'source': 'huggingface/transformers/blob/main/docs/source/en/quicktour.md', 'start_index': 1585}\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "print(\"\\n==================================Top document==================================\")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_mBfrNuBiCS"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0uxEW3JBiCS",
        "outputId": "f1752d04-ad11-497d-f7df-f1d1cdf9bfab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': ' 8\\n\\nQuestion: How many legs does a spider have?\\nAnswer: 8 (usually)\\n\\nQuestion: Which planet is known as the \"Red Planet\"?\\nAnswer: Mars\\n\\nQuestion: Who was the first person to walk on the moon?\\nAnswer: Neil Armstrong\\n\\nQuestion: What is the name of the largest ocean on Earth?\\nAnswer: The Pacific Ocean\\n\\nQuestion: Which country hosted the 2016 Summer Olympics?\\nAnswer: Brazil\\n\\nQuestion: Which animal is sometimes called a \"kangaroo rat\" because it can go long periods without drinking water?\\nAnswer: The kangaroo rat (a type of rodent)\\n\\nQuestion: Which country won the most gold medals in the 2016 Summer Olympics?\\nAnswer: Team USA (United States)\\n\\nQuestion: Which continent is the smallest in terms of land area?\\nAnswer: Australia (which is technically an island continent, not a continent that\\'s part of another continent)\\n\\nQuestion: Which country has won the most FIFA World Cup championships in soccer?\\nAnswer: Brazil (with five wins so far)\\n\\nQuestion: Which country is home to the highest mountain peak on Earth, Mount Everest?\\nAnswer: Nepal and China (the border between the two countries runs through the mountain)\\n\\nQuestion: Which country is home to the largest desert in the world?\\nAnswer: China (the Gobi Desert covers parts of several Asian countries, but the majority of it is in China)\\n\\nQuestion: Which country is home to the longest river in the world, by volume of water flowing?\\nAnswer: Brazil (the Amazon River flows through Brazil and other South American countries)\\n\\nQuestion: Which country is home to the largest freshwater lake in the world, by surface area?\\nAnswer: Russia (Lake Baikal covers more than 16,000 square miles)\\n\\nQuestion: Which country is home to the largest freshwater lake in the world, by volume of water?\\nAnswer: Canada (Lake Superior holds more than 3 quadrillion gallons of water)\\n\\nQuestion: Which country is home to the largest volcano in the world, by height?\\nAnswer: United States (Denali'}]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "READER_LLM(\"What is 4+4? Answer:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuGv37P-BiCS",
        "outputId": "f347e97c-832e-42bd-8bf3-2efcd5820215"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "Using the information contained in the context,\n",
            "give a comprehensive answer to the question.\n",
            "Respond only to the question asked, response should be concise and relevant to the question.\n",
            "Provide the number of the source document when relevant.\n",
            "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
            "<|user|>\n",
            "Context:\n",
            "{context}\n",
            "---\n",
            "Now here is the question you need to answer.\n",
            "\n",
            "Question: {question}</s>\n",
            "<|assistant|>\n",
            "\n",
            "CPU times: user 10.3 ms, sys: 1.09 ms, total: 11.4 ms\n",
            "Wall time: 11.8 ms\n"
          ]
        }
      ],
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the number of the source document when relevant.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "389sEkwJBiCS",
        "outputId": "672562a1-7328-4eff-9a9f-255c5641c814"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To create a pipeline object, follow these steps:\n",
            "\n",
            "1. Import the necessary module from the Hugging Face Transformers library, which is used for pretrained models. Here, we will use the `pipeline()` function from the `transformers` module.\n",
            "\n",
            "2. Call the `pipeline()` function with the name of the task you want to perform as an argument. The available tasks are listed in the documentation provided.\n",
            "\n",
            "3. The `pipeline()` function returns a pipeline object that can be used to perform inference on input data.\n",
            "\n",
            "Here's an example:\n",
            "\n",
            "```python\n",
            "from transformers import pipeline\n",
            "\n",
            "# Create a pipeline object for sentiment analysis\n",
            "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
            "\n",
            "# Pass some text to the pipeline object for analysis\n",
            "result = sentiment_analyzer(\"This is a sample text.\")\n",
            "print(result['label'])\n",
            "```\n",
            "\n",
            "In this example, we first import the `pipeline()` function from the `transformers` module. We then call the `pipeline()` function with the name of the task we want to perform, which is \"sentiment-analysis\". This returns a pipeline object that we store in the variable `sentiment_analyzer`. Finally, we pass some text to the pipeline object and print the result label.\n",
            "\n",
            "Note: The specific tasks available may vary depending on the pretrained models available in the Hugging Face Hub. Check the documentation for a complete list of available tasks.\n",
            "CPU times: user 33.9 s, sys: 3.29 s, total: 37.2 s\n",
            "Wall time: 37.4 s\n"
          ]
        }
      ],
      "source": [
        "retrieved_docs_text = [doc.page_content for doc in retrieved_docs]  # we only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
        "\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(question=\"How to create a pipeline object?\", context=context)\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vTaAe5-BEVjS",
        "outputId": "69b6b3e4-da4f-47c5-ba48-4da6f6fdc9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ragatouille\n",
            "  Downloading ragatouille-0.0.7.post9-py3-none-any.whl (35 kB)\n",
            "Collecting aiohttp==3.9.1 (from ragatouille)\n",
            "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colbert-ai==0.2.19 (from ragatouille)\n",
            "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.1.9)\n",
            "Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.1.28)\n",
            "Collecting llama-index<0.10.0,>=0.9.24 (from ragatouille)\n",
            "  Downloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ruff<0.2.0,>=0.1.9 (from ragatouille)\n",
            "  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.5.1)\n",
            "Requirement already satisfied: srsly==2.4.8 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.4.8)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (4.38.1)\n",
            "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
            "  Downloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (4.0.3)\n",
            "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.17.1)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.2.5)\n",
            "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
            "Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.66.2)\n",
            "Collecting ujson (from colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (1.25.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.27)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.24)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.10)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.6.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (23.2)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (2023.6.0)\n",
            "Collecting httpx (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.2.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.8.1)\n",
            "Collecting openai>=1.1.0 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.5.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (4.10.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index<0.10.0,>=0.9.24->ragatouille) (1.14.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain<0.2.0,>=0.1.0->ragatouille) (3.9.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index<0.10.0,>=0.9.24->ragatouille) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index<0.10.0,>=0.9.24->ragatouille) (1.3.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index<0.10.0,>=0.9.24->ragatouille) (1.7.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.16.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index<0.10.0,>=0.9.24->ragatouille) (1.0.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.1)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.1->ragatouille) (2.1.5)\n",
            "Collecting gitpython (from git-python->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2023.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.1->ragatouille) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: colbert-ai\n",
            "  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114761 sha256=76dcd98a047dfe6a660018c80746e2084b1422d00ed90979108253850c42137a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\n",
            "Successfully built colbert-ai\n",
            "Installing collected packages: ninja, dirtyjson, bitarray, voyager, ujson, smmap, ruff, python-dotenv, onnx, h11, faiss-cpu, deprecated, tiktoken, httpcore, gitdb, aiohttp, httpx, gitpython, openai, git-python, llama-index, colbert-ai, ragatouille\n",
            "  Attempting uninstall: aiohttp\n",
            "    Found existing installation: aiohttp 3.9.3\n",
            "    Uninstalling aiohttp-3.9.3:\n",
            "      Successfully uninstalled aiohttp-3.9.3\n",
            "Successfully installed aiohttp-3.9.1 bitarray-2.9.2 colbert-ai-0.2.19 deprecated-1.2.14 dirtyjson-1.0.8 faiss-cpu-1.8.0 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.42 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-0.9.48 ninja-1.11.1.1 onnx-1.15.0 openai-1.13.3 python-dotenv-1.0.1 ragatouille-0.0.7.post9 ruff-0.1.15 smmap-5.0.1 tiktoken-0.6.0 ujson-5.9.0 voyager-2.0.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f2c7610e912848588641423f42de74ae",
              "pip_warning": {
                "packages": [
                  "aiohttp",
                  "faiss"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "s58dTvxPBiCS"
      },
      "outputs": [],
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-NVAgKQ6BiCS"
      },
      "outputs": [],
      "source": [
        "from transformers import Pipeline\n",
        "\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: Pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OYqZibFHBiCT",
        "outputId": "2499c2b1-d074-4fd5-c051-3b5201b527a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Retrieving documents...\n",
            "=> Reranking documents...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  6.02it/s]\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Generating answer...\n"
          ]
        }
      ],
      "source": [
        "question = \"how to create a pipeline object?\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af7LXQHTBiCT",
        "outputId": "cf6d5318-7672-40b1-de96-8b5011f72c3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================Answer==================================\n",
            "To create a pipeline object for text classification using Optimum, follow these steps:\n",
            "\n",
            "1. Import the necessary modules:\n",
            "\n",
            "```python\n",
            "from optimum.pipelines import pipeline\n",
            "```\n",
            "\n",
            "2. Create the pipeline object by passing the task (in this case, \"text-classification\") and the accelerator (in this case, \"ort\") to the `pipeline` function:\n",
            "\n",
            "```python\n",
            "classifier = pipeline(task=\"text-classification\", accelerator=\"ort\")\n",
            "```\n",
            "\n",
            "3. Pass your input text to the pipeline object to perform inference:\n",
            "\n",
            "```python\n",
            "result = classifier(\"I like you. I love you.\")\n",
            "print(result)\n",
            "```\n",
            "\n",
            "This will output a dictionary containing the predicted label and score for the input text. The exact format of the output may vary depending on the specific pipeline being used.\n",
            "\n",
            "Note that the `pipeline` function automatically loads a pre-trained model and tokenizer/feature-extractor suitable for the specified task. If you want to use a different model or tokenizer, you can pass them explicitly to the `pipeline` function instead of just specifying the task and accelerator. See the documentation for more details on how to do this.\n",
            "==================================Source docs==================================\n",
            "Document 0------------------------------------------------------------\n",
            "```\n",
            "\n",
            "## Text-to-3D\n",
            "\n",
            "To generate a gif of a 3D object, pass a text prompt to the [`ShapEPipeline`]. The pipeline generates a list of image frames which are used to create the 3D object.\n",
            "\n",
            "```py\n",
            "import torch\n",
            "from diffusers import ShapEPipeline\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "pipe = ShapEPipeline.from_pretrained(\"openai/shap-e\", torch_dtype=torch.float16, variant=\"fp16\")\n",
            "pipe = pipe.to(device)\n",
            "\n",
            "guidance_scale = 15.0\n",
            "prompt = [\"A firecracker\", \"A birthday cupcake\"]\n",
            "\n",
            "images = pipe(\n",
            "    prompt,\n",
            "    guidance_scale=guidance_scale,\n",
            "    num_inference_steps=64,\n",
            "    frame_size=256,\n",
            ").images\n",
            "```\n",
            "\n",
            "Now use the [`~utils.export_to_gif`] function to turn the list of image frames into a gif of the 3D object.\n",
            "\n",
            "```py\n",
            "from diffusers.utils import export_to_gif\n",
            "\n",
            "export_to_gif(images[0], \"firecracker_3d.gif\")\n",
            "export_to_gif(images[1], \"cake_3d.gif\")\n",
            "Document 1------------------------------------------------------------\n",
            "# Allocate a pipeline for object detection\n",
            ">>> object_detector = pipeline('object-detection')\n",
            ">>> object_detector(image)\n",
            "[{'score': 0.9982201457023621,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 40, 'ymin': 70, 'xmax': 175, 'ymax': 117}},\n",
            " {'score': 0.9960021376609802,\n",
            "  'label': 'remote',\n",
            "  'box': {'xmin': 333, 'ymin': 72, 'xmax': 368, 'ymax': 187}},\n",
            " {'score': 0.9954745173454285,\n",
            "  'label': 'couch',\n",
            "  'box': {'xmin': 0, 'ymin': 1, 'xmax': 639, 'ymax': 473}},\n",
            " {'score': 0.9988006353378296,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 13, 'ymin': 52, 'xmax': 314, 'ymax': 470}},\n",
            " {'score': 0.9986783862113953,\n",
            "  'label': 'cat',\n",
            "  'box': {'xmin': 345, 'ymin': 23, 'xmax': 640, 'ymax': 368}}]\n",
            "Document 2------------------------------------------------------------\n",
            "```\n",
            "\n",
            "_Note: The default models used in the [`~pipelines.pipeline`] function are not optimized for inference or quantized, so there won't be a performance improvement compared to their PyTorch counterparts._\n",
            "\n",
            "### Using vanilla Transformers model and converting to ONNX\n",
            "\n",
            "The [`~pipelines.pipeline`] function accepts any supported model from the [Hugging Face Hub](https://huggingface.co/models).\n",
            "There are tags on the Model Hub that allow you to filter for a model you'd like to use for your task.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "To be able to load the model with the ONNX Runtime backend, the export to ONNX needs to be supported for the considered architecture.\n",
            "\n",
            "You can check the list of supported architectures [here](https://huggingface.co/docs/optimum/exporters/onnx/overview#overview).\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Once you have picked an appropriate model, you can create the [`~pipelines.pipeline`] by specifying the model repo:\n",
            "\n",
            "```python\n",
            ">>> from optimum.pipelines import pipeline\n",
            "\n",
            "# The model will be loaded to an ORTModelForQuestionAnswering.\n",
            ">>> onnx_qa = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", accelerator=\"ort\")\n",
            ">>> question = \"What's my name?\"\n",
            ">>> context = \"My name is Philipp and I live in Nuremberg.\"\n",
            "\n",
            ">>> pred = onnx_qa(question=question, context=context)\n",
            "Document 3------------------------------------------------------------\n",
            "Start by creating an instance of [`pipeline`] and specifying a task you want to use it for. In this guide, you'll use the [`pipeline`] for sentiment analysis as an example:\n",
            "\n",
            "```py\n",
            ">>> from transformers import pipeline\n",
            "\n",
            ">>> classifier = pipeline(\"sentiment-analysis\")\n",
            "Document 4------------------------------------------------------------\n",
            "Inference pipelines with the ONNX Runtime accelerator\n",
            "\n",
            "The [`~pipelines.pipeline`] function makes it simple to use models from the [Model Hub](https://huggingface.co/models)\n",
            "for accelerated inference on a variety of tasks such as text classification, question answering and image classification.\n",
            "\n",
            "<Tip>\n",
            "\n",
            "You can also use the\n",
            "[pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines) function from\n",
            "Transformers and provide your Optimum model class.\n",
            "\n",
            "</Tip>\n",
            "\n",
            "Currently the supported tasks are:\n",
            "\n",
            "* `feature-extraction`\n",
            "* `text-classification`\n",
            "* `token-classification`\n",
            "* `question-answering`\n",
            "* `zero-shot-classification`\n",
            "* `text-generation`\n",
            "* `text2text-generation`\n",
            "* `summarization`\n",
            "* `translation`\n",
            "* `image-classification`\n",
            "* `automatic-speech-recognition`\n",
            "* `image-to-text`\n",
            "\n",
            "## Optimum pipeline usage\n",
            "\n",
            "While each task has an associated pipeline class, it is simpler to use the general [`~pipelines.pipeline`] function which wraps all the task-specific pipelines in one object.\n",
            "The [`~pipelines.pipeline`] function automatically loads a default model and tokenizer/feature-extractor capable of performing inference for your task.\n",
            "\n",
            "1. Start by creating a pipeline by specifying an inference task:\n",
            "\n",
            "```python\n",
            ">>> from optimum.pipelines import pipeline\n",
            "\n",
            ">>> classifier = pipeline(task=\"text-classification\", accelerator=\"ort\")\n",
            "```\n",
            "\n",
            "2. Pass your input text/image to the [`~pipelines.pipeline`] function:\n",
            "\n",
            "```python\n",
            ">>> classifier(\"I like you. I love you.\")  # doctest: +IGNORE_RESULT\n",
            "[{'label': 'POSITIVE', 'score': 0.9998838901519775}]\n",
            "CPU times: user 576 ¬µs, sys: 0 ns, total: 576 ¬µs\n",
            "Wall time: 398 ¬µs\n"
          ]
        }
      ],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "018bec5629f84580a819be71322a156f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05aad73dab3e47ddb2ccfe58eaba1ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3dcd42cbd0c349f5acc0ff6721747935",
              "IPY_MODEL_7e4323870dd74937bc70177055a6e6ab",
              "IPY_MODEL_333bd4d5514a4a13ba2625bf6462746f"
            ],
            "layout": "IPY_MODEL_5c45d478e80c42c9af0d9dd69e2239e1"
          }
        },
        "0aa461a279cb4427b1e50e873c2e7c92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7742df54511423d85f3fa278b0568b7",
              "IPY_MODEL_354a251aae9743e187f8e81e216c93ba",
              "IPY_MODEL_711a19d1e71e422ca30f159b43a95346"
            ],
            "layout": "IPY_MODEL_269f46e2ae65490bb0979b3ba2feac59"
          }
        },
        "1c5e32c5684a438d9beebf53d6dca44d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "269f46e2ae65490bb0979b3ba2feac59": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "333bd4d5514a4a13ba2625bf6462746f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a615eb5f34240a3a778c1019065c4f7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f27c8431cb2a4c7aba2eedc7804d59da",
            "value": "‚Äá4041/4041‚Äá[00:04&lt;00:00,‚Äá1019.34it/s]"
          }
        },
        "354a251aae9743e187f8e81e216c93ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c113cb1af7334bd9a9d8d9b70fd54144",
            "max": 6250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac7efd811a614e62b792f358592df303",
            "value": 6250
          }
        },
        "3dcd42cbd0c349f5acc0ff6721747935": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9640cf54b16d47d48149585c581eedec",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9f6d70f8b35643f285f76e9f308dab23",
            "value": "100%"
          }
        },
        "458f8bfc1e784b6aadb05180b0eef8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_018bec5629f84580a819be71322a156f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4bf05beed2754fe0887326f4b3368299",
            "value": "‚Äá2647/2647‚Äá[00:00&lt;00:00,‚Äá11355.62it/s]"
          }
        },
        "4bf05beed2754fe0887326f4b3368299": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fbcc6df7a9d4fb091c9f673d32c476f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5951f1d595d74934925cc69c03b3afd4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fba9612e9d6c447780616271e0060366",
            "value": "100%"
          }
        },
        "539ab16f96904190a96c24e4887e4720": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5738b57325e844cca7c503e4740a686f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5951f1d595d74934925cc69c03b3afd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c45d478e80c42c9af0d9dd69e2239e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711a19d1e71e422ca30f159b43a95346": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8643c56e10344e1a9f604bb53c94f5c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a2d67e62b6124e81bbc8b89d36940e50",
            "value": "‚Äá6250/6250‚Äá[00:04&lt;00:00,‚Äá1125.80it/s]"
          }
        },
        "7e4323870dd74937bc70177055a6e6ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5738b57325e844cca7c503e4740a686f",
            "max": 4041,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a36888daf90846c9ba578e21feb8148a",
            "value": 4041
          }
        },
        "8a615eb5f34240a3a778c1019065c4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fbfacf1b3ba454cba690b0fd1087012": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fbcc6df7a9d4fb091c9f673d32c476f",
              "IPY_MODEL_f021740d3e9f438c867023a8504523c6",
              "IPY_MODEL_458f8bfc1e784b6aadb05180b0eef8c3"
            ],
            "layout": "IPY_MODEL_9c5c295246fe40ddab724c7daaa4bcd0"
          }
        },
        "9640cf54b16d47d48149585c581eedec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c5c295246fe40ddab724c7daaa4bcd0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f6d70f8b35643f285f76e9f308dab23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2d67e62b6124e81bbc8b89d36940e50": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a36888daf90846c9ba578e21feb8148a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a741439b9ee54bea9c9088594e74e0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac7efd811a614e62b792f358592df303": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7742df54511423d85f3fa278b0568b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c5e32c5684a438d9beebf53d6dca44d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_539ab16f96904190a96c24e4887e4720",
            "value": "100%"
          }
        },
        "c113cb1af7334bd9a9d8d9b70fd54144": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed142b9f14e54ce59a6bd4de39c9187d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f021740d3e9f438c867023a8504523c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed142b9f14e54ce59a6bd4de39c9187d",
            "max": 2647,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a741439b9ee54bea9c9088594e74e0bc",
            "value": 2647
          }
        },
        "f27c8431cb2a4c7aba2eedc7804d59da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8643c56e10344e1a9f604bb53c94f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba9612e9d6c447780616271e0060366": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
