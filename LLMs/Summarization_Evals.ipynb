{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WClxySHIzi9G"
      },
      "source": [
        "![Cohere-Logo-Color-RGB.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplM9PSe8djM"
      },
      "source": [
        "# Evaluating summarization\n",
        "\n",
        "In this cookbook, we will be demonstrating an approach we use for evaluating summarization tasks using LLM evaluation.\n",
        "\n",
        "### Table of contents:\n",
        "1. [Get started](#start)\n",
        "2. [Construct the evaluation dataset](#dataset)\n",
        "3. [Build the evaluation framework](#eval-framework)\n",
        "5. [Run evaluations](#run-evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SfEhd3O74--"
      },
      "source": [
        "<a id=\"start\"></a>\n",
        "<a name=\"start\"></a>\n",
        "# Get Started\n",
        "\n",
        "You'll need a Cohere API key to run this notebook. If you don't have a key, head to https://cohere.com/ to generate your key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "H2TfKiPM3a6f"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "#!pip install \"cohere<5\" datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "J1HzGqnY74bj"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "from typing import List, Optional\n",
        "\n",
        "import cohere\n",
        "from getpass import getpass\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Set up Cohere client\n",
        "load_dotenv()\n",
        "\n",
        "co_model = \"command-r\"\n",
        "co = cohere.Client(api_key=os.getenv('COHERE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEN21GS913wo"
      },
      "source": [
        "As test data, we'll use transcripts from the [QMSum dataset](https://github.com/Yale-LILY/QMSum). Note that in addition to the transcripts, this dataset also contains reference summaries -- we will use only the transcripts as our approach is reference-free."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "1JfU70r113wo"
      },
      "outputs": [],
      "source": [
        "qmsum = load_dataset(\"MocktaiLEngineer/qmsum-processed\", split=\"validation\")\n",
        "transcripts = [x for x in qmsum[\"meeting_transcript\"] if x is not None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'PhD D: Sure because I I need a lot of time to to put the label or to do that \\nProfessor E: I mean we we know that there s noise There s there s continual noise from fans and so forth and there is more impulsive noise from taps and so forth and and something in between with paper rustling We know that all that s there and it s a g worthwhile thing to study but obviously it takes a lot of time to mark all of these things Whereas th i I would think that you we can study more or less as a distinct phenomenon the overlapping of people talking So Then you can get the Cuz you need If it s three hundred i i it sounds like you probably only have fifty or sixty or seventy events right now that are really And and you need to have a lot more than that to have any kind of even visual sense of of what s going on much less any kind of reasonable statistics \\nPhD C: Now why do you need to mark speaker overlap by hand if you can infer it from the relative energy in the \\nGrad G: Well that s That s what I was going to bring up \\nPhD C: I mean you should not need to do this p completely by hand \\nProfessor E: OK So let s back up because you were not here for an earlier conversation \\nPhD C: right ? I m sorry \\nProfessor E: So the idea was that what he was going to be doing was experimenting with different measures such as the increase in energy such as the energy in the LPC residuals such as I mean there s a bunch of things I mean increased energy is is sort of an obvious one \\nPhD C: Mm In the far field mike \\nProfessor E: and it s not obvious I mean you could you could do the dumbest thing and get get it ninety percent of the time But when you start going past that and trying to do better it s not obvious what combination of features is going to give you the you know the right detector So the idea is to have some ground truth first And so the i the idea of the manual marking was to say `` OK this i you know it s it s really here `` \\nPhD A: But I think Liz is saying why not get it out of the transcripts ? \\nPhD C: What I mean is pause get it from the close talking mikes A or ge get a first pass from those \\nProfessor E: We t we t w we t we talked about that \\nPhD C: and then go through sort of It would be a lot faster probably to \\nProfessor E: We we we talked about that s But so it s a bootstrapping thing and the thing is the idea was i we i i we thought it would be useful for him to look at the data anyway and and then whatever he could mark would be helpful and we could it s a question of what you bootstrap from You know do you bootstrap from a simple measurement which is right most of the time and then you g do better or do you bootstrap from some human being looking at it and then then do your simple measurements from the close talking mike I mean even with the close talking mike you are not going to get it right all the time \\nPhD C: Well that s what I wonder because or how bad it is be because that would be interesting \\nGrad G: I m working on a program to do that and \\nPhD C: especially because the bottleneck is the transcription Right ? I mean we ve got a lot more data than we have transcriptions for We have the audio data we have the close talking mike so I mean it seems like one kind of project that s not perfect but that you can get the training data for pretty quickly is you know if you infer form the close talking mikes where the on off points are of speech you know how can we detect that from a far field ? \\nGrad G: I ve I ve written a program to do that \\nPhD C: OK I m sorry I missed the \\nGrad G: and so but it s it s doing something very very simple It just takes a threshold based on on the volume \\nPhD F: Or you can set the threshold low and then weed out the false alarms by hand \\nGrad G: and then it does a median filter and then it looks for runs And it seems to work I ve I m sort of fiddling with the parameters to get it to actually generate something and I have not I do not what I m working on was working on was getting it to a form where we can import it into the user interface that we have pause into Transcriber And so I told I said it would take about a day I ve worked on it for about half a day so give me another half day and I we will have something we can play with \\nProfessor E: See this is where we really need the Meeting Recorder query stuff to be working because we ve had these meetings and we ve had this discussion about this and I m sort of remembering a little bit about what we decided \\nPhD C: Right I m sorry I just \\nProfessor E: but I could not remember all of it So I think it was partly that you know give somebody a chance to actually look at the data and see what these are like partly that we have e some ground truth to compare against you know when when he he gets his thing going \\nPhD C: Well it s definitely good to have somebody look at it I was just thinking as a way to speed up you know the amount of \\nProfessor E: That was that was exactly the notion that that that we discussed'"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transcripts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U58iKz213wo"
      },
      "source": [
        "<a id=\"dataset\"></a>\n",
        "<a name=\"dataset\"></a>\n",
        "# Construct the evaluation dataset\n",
        "\n",
        "We are interested in evaluating summarization in real-world, enterprise use cases, which typically have two distinguishing features as compared to academic summarization benchmarks:\n",
        "- Enterprise use cases often focus on specific summarization objectives, e.g. \"summarize action items\".\n",
        "- Enterprise use cases often feature specific instruction constraints, e.g. \"summarize in bullets with each bullet under 20 words\".\n",
        "\n",
        "Therefore, we must first create a dataset that contains diverse summarization prompts. We will do this programmatically by building prompts from their components, as defined below:\n",
        "- Prompt = text (e.g. transcript to be summarized) + instruction\n",
        "- Instruction = instruction objective (e.g. \"summarize action items\") + modifiers\n",
        "- Modifiers = format/length modifiers (e.g. \"use bullets\") + style/tone modifiers (e.g. \"do not mention names\") + ...\n",
        "\n",
        "First, we define the prompt that combines the text and instructions. Here, we use a very basic prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "2Bj2I0If13wp"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"## meeting transcript\n",
        "{transcript}\n",
        "\n",
        "## instructions\n",
        "{instructions}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc33XZLL13wp"
      },
      "source": [
        "Next, we build the instructions. Because each instruction may have a different objective and modifiers, we track them using metadata. This will later be required for evaluation (i.e. to know what the prompt is asking)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uMI5VXm13wp"
      },
      "outputs": [],
      "source": [
        "# In this cookbook, we will only consider the following 2 objectives and 4 format/length modifiers\n",
        "\n",
        "instruction_objectives = {\n",
        "    \"general_summarization\": \"Summarize the meeting based on the transcript.\",\n",
        "    \"action_items\": \"What are the follow-up items based on the meeting transcript?\",\n",
        "}\n",
        "\n",
        "# Note that not all format/length modifiers are compatible with all objectives. For example, action\n",
        "# items are best suited for bullet format.\n",
        "format_length_modifiers = {\n",
        "    \"paragraphs_short\": {\n",
        "        \"text\": \"In paragraph form, output your response. Use at least 10 words and at most 50 words in total.\",\n",
        "        \"objectives\": [\"general_summarization\"],\n",
        "        \"eval_metadata\": {\n",
        "            \"format\": \"paragraphs\",\n",
        "            \"min_length\": 10,\n",
        "            \"max_length\": 50,\n",
        "        },\n",
        "    },\n",
        "    \"paragraphs_medium\": {\n",
        "        \"text\": \"Return the answer in the form of paragraphs. Make sure your answer is between 50 and 200 words long.\",\n",
        "        \"objectives\": [\"general_summarization\"],\n",
        "        \"eval_metadata\": {\n",
        "            \"format\": \"paragraphs\",\n",
        "            \"min_length\": 50,\n",
        "            \"max_length\": 200,\n",
        "        },\n",
        "    },\n",
        "    \"bullets_short_3\": {\n",
        "        \"text\": \"Format your answer in the form of bullets. Use exactly 3 bullets. Each bullet should be at least 10 words and at most 20 words.\",\n",
        "        \"objectives\": [\"general_summarization\", \"action_items\"],\n",
        "        \"eval_metadata\": {\n",
        "            \"format\": \"bullets\",\n",
        "            \"number\": 3,\n",
        "            \"min_length\": 10,\n",
        "            \"max_length\": 20,\n",
        "        },\n",
        "    },\n",
        "    \"bullets_medium_2\": {\n",
        "        \"text\": \"In bullets, output your response. Make sure to use exactly 2 bullets. Make sure each bullet is between 20 and 80 words long.\",\n",
        "        \"objectives\": [\"general_summarization\", \"action_items\"],\n",
        "        \"eval_metadata\": {\n",
        "            \"format\": \"bullets\",\n",
        "            \"number\": 2,\n",
        "            \"min_length\": 20,\n",
        "            \"max_length\": 80,\n",
        "        },\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0dqBqxO13wq"
      },
      "source": [
        "Let's combine the objectives and format/length modifiers to finish building the instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMTxDOL713wq",
        "outputId": "e854d1bf-0ea7-41cd-b385-98294d19472e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"instruction\": \"Summarize the meeting based on the transcript. In paragraph form, output your response. Use at least 10 words and at most 50 words in total.\",\n",
            "        \"eval_metadata\": {\n",
            "            \"format\": \"paragraphs\",\n",
            "            \"min_length\": 10,\n",
            "            \"max_length\": 50\n",
            "        },\n",
            "        \"objective\": \"general_summarization\"\n",
            "    },\n",
            "    {\n",
            "        \"instruction\": \"Summarize the meeting based on the transcript. Return the answer in the form of paragraphs. Make sure your answer is between 50 and 200 words long.\",\n",
            "        \"eval_metadata\": {\n",
            "            \"format\": \"paragraphs\",\n",
            "            \"min_length\": 50,\n",
            "            \"max_length\": 200\n",
            "        },\n",
            "        \"objective\": \"general_summarization\"\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "instructions = []\n",
        "for obj_name, obj_text in instruction_objectives.items():\n",
        "    for mod_data in format_length_modifiers.values():\n",
        "        for mod_obj in mod_data[\"objectives\"]:\n",
        "            if mod_obj == obj_name:\n",
        "                instruction = {\n",
        "                        \"instruction\": f\"{obj_text} {mod_data['text']}\",\n",
        "                        \"eval_metadata\": mod_data[\"eval_metadata\"],\n",
        "                        \"objective\": obj_name,\n",
        "                    }\n",
        "                instructions.append(instruction)\n",
        "\n",
        "# View the first two instructions\n",
        "print(json.dumps(instructions[:2], indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MeNfW-m13wr"
      },
      "source": [
        "Finally, let's build the final prompts by semi-randomly pairing the instructions with transcripts from the QMSum dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'instruction': 'Summarize the meeting based on the transcript. In paragraph form, output your response. Use at least 10 words and at most 50 words in total.',\n",
              "  'eval_metadata': {'format': 'paragraphs',\n",
              "   'min_length': 10,\n",
              "   'max_length': 50},\n",
              "  'objective': 'general_summarization'},\n",
              " {'instruction': 'Summarize the meeting based on the transcript. Return the answer in the form of paragraphs. Make sure your answer is between 50 and 200 words long.',\n",
              "  'eval_metadata': {'format': 'paragraphs',\n",
              "   'min_length': 50,\n",
              "   'max_length': 200},\n",
              "  'objective': 'general_summarization'},\n",
              " {'instruction': 'Summarize the meeting based on the transcript. Format your answer in the form of bullets. Use exactly 3 bullets. Each bullet should be at least 10 words and at most 20 words.',\n",
              "  'eval_metadata': {'format': 'bullets',\n",
              "   'number': 3,\n",
              "   'min_length': 10,\n",
              "   'max_length': 20},\n",
              "  'objective': 'general_summarization'},\n",
              " {'instruction': 'Summarize the meeting based on the transcript. In bullets, output your response. Make sure to use exactly 2 bullets. Make sure each bullet is between 20 and 80 words long.',\n",
              "  'eval_metadata': {'format': 'bullets',\n",
              "   'number': 2,\n",
              "   'min_length': 20,\n",
              "   'max_length': 80},\n",
              "  'objective': 'general_summarization'},\n",
              " {'instruction': 'What are the follow-up items based on the meeting transcript? Format your answer in the form of bullets. Use exactly 3 bullets. Each bullet should be at least 10 words and at most 20 words.',\n",
              "  'eval_metadata': {'format': 'bullets',\n",
              "   'number': 3,\n",
              "   'min_length': 10,\n",
              "   'max_length': 20},\n",
              "  'objective': 'action_items'},\n",
              " {'instruction': 'What are the follow-up items based on the meeting transcript? In bullets, output your response. Make sure to use exactly 2 bullets. Make sure each bullet is between 20 and 80 words long.',\n",
              "  'eval_metadata': {'format': 'bullets',\n",
              "   'number': 2,\n",
              "   'min_length': 20,\n",
              "   'max_length': 80},\n",
              "  'objective': 'action_items'}]"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYTMTFAm13wr"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(instructions)\n",
        "\n",
        "# Randomly shuffle the top 25% of transcripts by length, then assign them to the data\n",
        "transcripts = sorted(transcripts, key=lambda x: len(x), reverse=True)[:int(len(transcripts) * 0.25)]\n",
        "random.seed(42)\n",
        "random.shuffle(transcripts)\n",
        "data[\"transcript\"] = transcripts[:len(data)]\n",
        "\n",
        "# Build the prompt\n",
        "data[\"prompt\"] = data.apply(lambda x: prompt_template.format(transcript=x[\"transcript\"], instructions=x[\"instruction\"]), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#transcripts[:len(data)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'PhD F: As opposed to the rest of us \\nPhD D: Well comment OK I I remind that me my first objective eh in the project is to to study difference parameters to to find a a good solution to detect eh the overlapping zone in eh speech recorded But eh tsk comment ehhh comment In that way comment I I I begin to to study and to analyze the ehn the recorded speech eh the different session to to find and to locate and to mark eh the the different overlapping zone And eh so eh I was eh I am transcribing the the first session and I I have found eh eh one thousand acoustic events eh besides the overlapping zones eh I I I mean the eh breaths eh aspiration eh eh talk eh eh clap eh comment I do not know what is the different names eh you use to to name the the pause n speech \\nGrad G: Oh I do not think we ve been doing it at that level of detail So \\nPhD D: Eh I I I do I do not need to to to mmm to m to label the the different acoustic but I prefer because eh I would like to to study if eh I I will find eh eh a good eh parameters eh to detect overlapping I would like to to to test these parameters eh with the another eh eh acoustic events to nnn to eh to find what is the ehm the false eh the false eh hypothesis eh nnn which eh are produced when we use the the ehm this eh parameter eh I mean pitch eh eh difference eh feature \\nPhD A: You know I think some of these that are the nonspeech overlapping events may be difficult even for humans to tell that there s two there I mean if it s a tapping sound you would not necessarily or you know something like that it would be it might be hard to know that it was two separate events \\nGrad G: Well You were not talking about just overlaps were you ? You were just talking about acoustic events \\nPhD D: I I I I t I t I talk eh about eh acoustic events in general but eh my my objective eh will be eh to study eh overlapping zone Eh ? comment n Eh in twelve minutes I found eh eh one thousand acoustic events \\nProfessor E: How many overlaps were there in it ? No no how many of them were the overlaps of speech though ? \\nPhD D: How many ? Eh almost eh three hundred eh in one session in five eh in forty five minutes Alm Three hundred overlapping zone With the overlapping zone overlapping speech speech what eh different duration \\nPostdoc B: Does this ? So if you had an overlap involving three people how many times was that counted ? \\nPhD D: three people two people Eh I would like to consider eh one people with difference noise eh in the background be \\nProfessor E: No no but I think what she s asking is pause if at some particular for some particular stretch you had three people talking instead of two did you call that one event ? \\nPhD D: Oh Oh I consider one event eh for th for that eh for all the zone This th I I I con I consider I consider eh an acoustic event the overlapping zone the period where three speaker or eh are talking together \\nGrad G: So let s say me and Jane are talking at the same time and then Liz starts talking also over all of us How many events would that be ? \\nPhD D: So I do not understand \\nGrad G: So two people are talking comment and then a third person starts talking Is there an event right here ? \\nPhD D: Eh no No no For me is the overlapping zone because because you you have s you have more one eh more one voice eh eh produced in a in in a moment \\nGrad G: So i if two or more people are talking \\nProfessor E: OK So I think We just wanted to understand how you are defining it So then in the region between since there there is some continuous region in between regions where there is only one person speaking And one contiguous region like that you are calling an event Is it Are you calling the beginning or the end of it the event or are you calling the entire length of it the event ? \\nPhD D: I consider the the nnn the nnn nnn eh the entirety eh eh all all the time there were the voice has overlapped This is the idea But eh I I do not distinguish between the the numbers of eh speaker I m not considering eh the the ehm eh the fact of eh eh for example what did you say ? Eh at first eh eh two talkers are eh speaking and eh eh a third person eh join to to that For me it s eh it s eh all overlap zone with eh several numbers of speakers is eh eh the same acoustic event Wi but without any mark between the zone of the overlapping zone with two speakers eh speaking together and the zone with the three speakers \\nPostdoc B: That would j just be one \\nPhD D: Eh with eh a beginning mark and the ending mark Because eh for me is the is the zone with eh some kind of eh distortion the spectral I do not mind By the moment by the moment \\nGrad G: Well but But you could imagine that three people talking has a different spectral characteristic than two \\nPhD D: I I do not but eh but eh I have to study comment What will happen in a general way \\nGrad G: So You had to start somewhere \\nPhD C: So there s a lot of overlap \\nPhD D: I I do not know what eh will will happen with the \\nGrad G: That s a lot of overlap \\nProfessor E: So again that s that s three three hundred in forty five minutes that are that are speakers just speakers \\nPostdoc B: But a a a th \\nProfessor E: So that s about eight per minute \\nPostdoc B: But a thousand events in twelve minutes that s \\nPhD C: But that can include taps \\nPostdoc B: Well but a thousand taps in eight minutes is a l in twelve minutes is a lot \\nPhD D: I I con I consider I consider acoustic events eh the silent too \\nGrad G: Silence starting or silence ending \\nPhD D: silent ground to bec to detect eh because I consider acoustic event all the things are not eh speech In ge in in in a general point of view \\nProfessor E: OK so how many of those thousand were silence ? \\nPhD F: Not speech not speech or too much speech \\nProfessor E: Right So how many of those thousand were silence silent sections ? \\nPhD D: silent I I I I do not I I have not the eh I I would like to to do a stylistic study'"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.iloc[0]['transcript']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "igP0-oHN13wr"
      },
      "outputs": [],
      "source": [
        "# (Optional) Let's also check the token lengths of our prompts.\n",
        "data[\"transcript_token_len\"] = [len(x) for x in co.batch_tokenize(data[\"transcript\"].tolist(), model=co_model)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbPVxiuV13wr",
        "outputId": "bc9e9cf6-31c0-4404-f43f-fdfb5679cd56"
      },
      "outputs": [],
      "source": [
        "# Let's examine one of the prompts\n",
        "#print(data[\"prompt\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDamVDyP13wr"
      },
      "source": [
        "<a id=\"eval-framework\"></a>\n",
        "<a name=\"eval-framework\"></a>\n",
        "# Build the evaluation framework\n",
        "\n",
        "We now setup the tools we will use for evaluation.\n",
        "\n",
        "We use three criteria that are graded using LLMs:\n",
        "- Completeness: checks if the summary includes all the important information from the original text that it should include\n",
        "- Correctness: checks if there are any hallucinations or factual inaccuracies in the summary\n",
        "- Conciseness: checks if the summary includes any unnecessary information or wordiness\n",
        "\n",
        "In this cookbook, we will use Command-R to grade the completions. However, note that in practice, we typically use an ensemble of multiple LLM evaluators to reduce any bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KGQQp2Yd13wr"
      },
      "outputs": [],
      "source": [
        "# LLM evaluation uses an LLM to grade the completion. We pass the original prompt, completion, and a\n",
        "# criteria to the LLM, and it returns a score between 1 and 5. We will use the following template as\n",
        "# the grading prompt.\n",
        "\n",
        "grading_prompt_template = \"\"\"You are an AI grader that given a prompt, a completion, and a criterion, grades the completion based on the prompt and criterion. Below is a prompt, a completion, and a criterion with which to grade the completion. You need to respond according to the criterion instructions.\n",
        "\n",
        "PROMPT:\n",
        "{prompt}\n",
        "\n",
        "COMPLETION:\n",
        "{completion}\n",
        "\n",
        "CRITERION:\n",
        "{criteria}\n",
        "\n",
        "Do not provide an explanation or respond with additional text. Only respond with the final score in the format SCORE: <final_score>\n",
        "\n",
        "SCORE:\"\"\"\n",
        "\n",
        "\n",
        "criteria_completeness = \"\"\"Please judge the above completion on whether or not all the completion fully addresses the task posed in the prompt. The completion should contain all the information that is asked for by the prompt while also upholding the constraints mentioned in the prompt. Please return a grade between 1 and 5, where 1 means the completion does not address the prompt at all, 2 means the completion mostly does not address the prompt, 3 is neutral, 4 means the completion mostly addresses the prompt, and 5 means the completion fully addresses the prompt.\"\"\"\n",
        "\n",
        "criteria_correctness = \"\"\"Please judge the above completion on whether or not all information was derived from the prompt and that no information is false. Return a grade between 1 and 5, where 1 means the answers are entirely untruthful relative to the prompt, 2 means the answers are somewhat untruthful relative to the prompt, 3 means neutral, 4 means the answers are somewhat truthful relative to the prompt, and 5 means the answers are entirely truthful relative to the prompt. Evaluate only the truthfulness of the answers, not whether or not they capture all the relevant information in the prompt.\"\"\"\n",
        "\n",
        "criteria_conciseness = \"\"\"Please judge the above completion on whether or not the completion contains any unnecessary information or wordiness that does not help answer the specific instruction given in the prompt. Return a grade between 1 and 5, where 1 means the completion contains many unnecessary details and wordiness that do not answer the specific instruction given in the prompt, 2 means the completion contains some unnecessary details or wordiness, 3 means neutral, 4 means the completion contains few unnecessary details or wordiness, and 5 means the completion contains only necessary details that answer the specific instruction given in the prompt.\"\"\"\n",
        "\n",
        "\n",
        "def score_llm(prompt: str, completion: str, criteria: str) -> int:\n",
        "    \"\"\"\n",
        "    Score a completion based on a prompt and a criterion using LLM Because we\n",
        "    grade all completions on a scale of 1-5, we will normalize the scores by 5 so that the final score\n",
        "    is between 0 and 1.\n",
        "    \"\"\"\n",
        "    grading_prompt = grading_prompt_template.format(\n",
        "        prompt=prompt, completion=completion, criteria=criteria\n",
        "    )\n",
        "    # Use Cohere to grade the completion\n",
        "    completion = co.chat(message=grading_prompt, model=co_model, temperature=0.2).text\n",
        "\n",
        "    ### Alternatively, use OpenAI to grade the completion (requires key)\n",
        "    # import openai\n",
        "    # completion = openai.OpenAI(api_key=\"INSERT OPENAI KEY HERE\").chat.completions.create(\n",
        "    #     model=\"gpt-4\",\n",
        "    #     messages=[{\"role\": \"user\", \"content\": grading_prompt}],\n",
        "    #     temperature=0.2,\n",
        "    # ).choices[0].message.content\n",
        "\n",
        "    # Extract the score from the completion\n",
        "    score = float(re.search(r\"[12345]\", completion).group()) / 5\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dEEsXt13wr"
      },
      "source": [
        "In addition, we have two criteria that are graded programmatically:\n",
        "- Format: checks if the summary follows the format (e.g. bullets) that was requested in the prompt\n",
        "- Length: checks if the summary follows the length that was requested in the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "i1niSoI-13wr"
      },
      "outputs": [],
      "source": [
        "# Non-LLM checks\n",
        "\n",
        "def score_format(completion: str, format_type: str) -> int:\n",
        "    \"\"\"\n",
        "    Returns 1 if the completion is in the correct format, 0 otherwise.\n",
        "    \"\"\"\n",
        "    if format_type == \"paragraphs\":\n",
        "        return int(_is_only_paragraphs(completion))\n",
        "    elif format_type == \"bullets\":\n",
        "        return int(_is_only_bullets(completion))\n",
        "    return 0\n",
        "\n",
        "def score_length(\n",
        "    completion: str,\n",
        "    format_type: str,\n",
        "    min_val: int,\n",
        "    max_val: int,\n",
        "    number: Optional[int] = None\n",
        ") -> int:\n",
        "    \"\"\"\n",
        "    Returns 1 if the completion has the correct length for the given format, 0 otherwise. This\n",
        "    includes both word count and number of items (optional).\n",
        "    \"\"\"\n",
        "    # Split into items (each bullet for bullets or each paragraph for paragraphs)\n",
        "    if format_type == \"bullets\":\n",
        "        items = _extract_markdown_bullets(completion, include_bullet=False)\n",
        "    elif format_type == \"paragraphs\":\n",
        "        items = completion.split(\"\\n\")\n",
        "\n",
        "    # Strip whitespace and remove empty items\n",
        "    items = [item for item in items if item.strip() != \"\"]\n",
        "\n",
        "    # Check number of items if provided\n",
        "    if number is not None and len(items) != number:\n",
        "        return 0\n",
        "\n",
        "    # Check length of each item\n",
        "    for item in items:\n",
        "        num_words = item.strip().split()\n",
        "        if min_val is None and len(num_words) > max_val:\n",
        "            return 0\n",
        "        elif max_val is None and len(num_words) < min_val:\n",
        "            return 0\n",
        "        elif not min_val <= len(num_words) <= max_val:\n",
        "            return 0\n",
        "    return 1\n",
        "\n",
        "\n",
        "def _is_only_bullets(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if text is only markdown bullets.\n",
        "    \"\"\"\n",
        "    bullets = _extract_markdown_bullets(text, include_bullet=True)\n",
        "\n",
        "    for bullet in bullets:\n",
        "        text = text.replace(bullet, \"\")\n",
        "\n",
        "    return text.strip() == \"\"\n",
        "\n",
        "\n",
        "def _is_only_paragraphs(text: str) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if text is only paragraphs (no bullets).\n",
        "    \"\"\"\n",
        "    bullets = _extract_markdown_bullets(text, include_bullet=True)\n",
        "\n",
        "    return len(bullets) == 0\n",
        "\n",
        "\n",
        "def _extract_markdown_bullets(text: str, include_bullet: bool = False) -> List[str]:\n",
        "    \"\"\"\n",
        "    Extracts markdown bullets from text as a list. If include_bullet is True, the bullet will be\n",
        "    included in the output. The list of accepted bullets is: -, *, +, •, and any number followed by\n",
        "    a period.\n",
        "    \"\"\"\n",
        "    if include_bullet:\n",
        "        return re.findall(r\"^[ \\t]*(?:[-*+•]|[\\d]+\\.).*\\w+.*$\", text, flags=re.MULTILINE)\n",
        "    return re.findall(r\"^[ \\t]*(?:[-*+•]|[\\d]+\\.)(.*\\w+.*)$\", text, flags=re.MULTILINE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBpk-GTe13ws"
      },
      "source": [
        "<a id=\"run-evals\"></a>\n",
        "<a name=\"run-evals\"></a>\n",
        "# Run evaluations\n",
        "\n",
        "Now that we have our evaluation dataset and defined our evaluation functions, let's run evaluations!\n",
        "\n",
        "First, we generate completions to be graded. We will use Cohere's [Command-R](https://huggingface.co/CohereForAI/c4ai-command-r-v01) model, boasting a context length of 128K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data[\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## meeting transcript\n",
            "PhD F: As opposed to the rest of us \n",
            "PhD D: Well comment OK I I remind that me my first objective eh in the project is to to study difference parameters to to find a a good solution to detect eh the overlapping zone in eh speech recorded But eh tsk comment ehhh comment In that way comment I I I begin to to study and to analyze the ehn the recorded speech eh the different session to to find and to locate and to mark eh the the different overlapping zone And eh so eh I was eh I am transcribing the the first session and I I have found eh eh one thousand acoustic events eh besides the overlapping zones eh I I I mean the eh breaths eh aspiration eh eh talk eh eh clap eh comment I do not know what is the different names eh you use to to name the the pause n speech \n",
            "Grad G: Oh I do not think we ve been doing it at that level of detail So \n",
            "PhD D: Eh I I I do I do not need to to to mmm to m to label the the different acoustic but I prefer because eh I would like to to study if eh I I will find eh eh a good eh parameters eh to detect overlapping I would like to to to test these parameters eh with the another eh eh acoustic events to nnn to eh to find what is the ehm the false eh the false eh hypothesis eh nnn which eh are produced when we use the the ehm this eh parameter eh I mean pitch eh eh difference eh feature \n",
            "PhD A: You know I think some of these that are the nonspeech overlapping events may be difficult even for humans to tell that there s two there I mean if it s a tapping sound you would not necessarily or you know something like that it would be it might be hard to know that it was two separate events \n",
            "Grad G: Well You were not talking about just overlaps were you ? You were just talking about acoustic events \n",
            "PhD D: I I I I t I t I talk eh about eh acoustic events in general but eh my my objective eh will be eh to study eh overlapping zone Eh ? comment n Eh in twelve minutes I found eh eh one thousand acoustic events \n",
            "Professor E: How many overlaps were there in it ? No no how many of them were the overlaps of speech though ? \n",
            "PhD D: How many ? Eh almost eh three hundred eh in one session in five eh in forty five minutes Alm Three hundred overlapping zone With the overlapping zone overlapping speech speech what eh different duration \n",
            "Postdoc B: Does this ? So if you had an overlap involving three people how many times was that counted ? \n",
            "PhD D: three people two people Eh I would like to consider eh one people with difference noise eh in the background be \n",
            "Professor E: No no but I think what she s asking is pause if at some particular for some particular stretch you had three people talking instead of two did you call that one event ? \n",
            "PhD D: Oh Oh I consider one event eh for th for that eh for all the zone This th I I I con I consider I consider eh an acoustic event the overlapping zone the period where three speaker or eh are talking together \n",
            "Grad G: So let s say me and Jane are talking at the same time and then Liz starts talking also over all of us How many events would that be ? \n",
            "PhD D: So I do not understand \n",
            "Grad G: So two people are talking comment and then a third person starts talking Is there an event right here ? \n",
            "PhD D: Eh no No no For me is the overlapping zone because because you you have s you have more one eh more one voice eh eh produced in a in in a moment \n",
            "Grad G: So i if two or more people are talking \n",
            "Professor E: OK So I think We just wanted to understand how you are defining it So then in the region between since there there is some continuous region in between regions where there is only one person speaking And one contiguous region like that you are calling an event Is it Are you calling the beginning or the end of it the event or are you calling the entire length of it the event ? \n",
            "PhD D: I consider the the nnn the nnn nnn eh the entirety eh eh all all the time there were the voice has overlapped This is the idea But eh I I do not distinguish between the the numbers of eh speaker I m not considering eh the the ehm eh the fact of eh eh for example what did you say ? Eh at first eh eh two talkers are eh speaking and eh eh a third person eh join to to that For me it s eh it s eh all overlap zone with eh several numbers of speakers is eh eh the same acoustic event Wi but without any mark between the zone of the overlapping zone with two speakers eh speaking together and the zone with the three speakers \n",
            "Postdoc B: That would j just be one \n",
            "PhD D: Eh with eh a beginning mark and the ending mark Because eh for me is the is the zone with eh some kind of eh distortion the spectral I do not mind By the moment by the moment \n",
            "Grad G: Well but But you could imagine that three people talking has a different spectral characteristic than two \n",
            "PhD D: I I do not but eh but eh I have to study comment What will happen in a general way \n",
            "Grad G: So You had to start somewhere \n",
            "PhD C: So there s a lot of overlap \n",
            "PhD D: I I do not know what eh will will happen with the \n",
            "Grad G: That s a lot of overlap \n",
            "Professor E: So again that s that s three three hundred in forty five minutes that are that are speakers just speakers \n",
            "Postdoc B: But a a a th \n",
            "Professor E: So that s about eight per minute \n",
            "Postdoc B: But a thousand events in twelve minutes that s \n",
            "PhD C: But that can include taps \n",
            "Postdoc B: Well but a thousand taps in eight minutes is a l in twelve minutes is a lot \n",
            "PhD D: I I con I consider I consider acoustic events eh the silent too \n",
            "Grad G: Silence starting or silence ending \n",
            "PhD D: silent ground to bec to detect eh because I consider acoustic event all the things are not eh speech In ge in in in a general point of view \n",
            "Professor E: OK so how many of those thousand were silence ? \n",
            "PhD F: Not speech not speech or too much speech \n",
            "Professor E: Right So how many of those thousand were silence silent sections ? \n",
            "PhD D: silent I I I I do not I I have not the eh I I would like to to do a stylistic study\n",
            "\n",
            "## instructions\n",
            "Summarize the meeting based on the transcript. In paragraph form, output your response. Use at least 10 words and at most 50 words in total.\n"
          ]
        }
      ],
      "source": [
        "print(data[\"prompt\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzbOQmWE13ws"
      },
      "outputs": [],
      "source": [
        "# completions = []\n",
        "# for prompt in data[\"prompt\"]:\n",
        "#     completion = co.chat(message=prompt, model=\"command-r\", temperature=0.2).text\n",
        "#     completions.append(completion)\n",
        "\n",
        "# data[\"completion\"] = completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'PhD D is transcribing recorded sessions to find and label overlapping speech zones. Other team members question their definition of an event, suggesting thousands of events in a short time is excessive. PhD D maintains their approach is valid.'"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "completion = co.chat(message=data['prompt'][0], model=\"command-r\", temperature=0.2).text\n",
        "completion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fWjuruK13ws"
      },
      "source": [
        "Let's grade the completions using our LLM and non-LLM checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Score Format\n",
        "score_format(completion, data[\"eval_metadata\"][0]['format'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Score Length\n",
        "score_length(\n",
        "        completion,\n",
        "        data[\"eval_metadata\"][0][\"format\"],\n",
        "        data[\"eval_metadata\"][0][\"min_length\"],\n",
        "        data[\"eval_metadata\"][0][\"max_length\"],\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['eval_metadata'][0]['min_length']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Score completeness\n",
        "score_llm(data[\"prompt\"][0], completion, criteria_completeness)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Score correctness\n",
        "score_llm(data[\"prompt\"][0], completion, criteria_correctness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # Score conciseness\n",
        "score_llm(data[\"prompt\"][0], completion, criteria_conciseness)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XesZHCN13ws"
      },
      "outputs": [],
      "source": [
        "# # Score format\n",
        "# data[\"format_score\"] = data.apply(\n",
        "#     lambda x: score_format(x[\"completion\"], x[\"eval_metadata\"][\"format\"]), axis=1\n",
        "# )\n",
        "\n",
        "# # Score length\n",
        "# data[\"length_score\"] = data.apply(\n",
        "#     lambda x: score_length(\n",
        "#         x[\"completion\"],\n",
        "#         x[\"eval_metadata\"][\"format\"],\n",
        "#         x[\"eval_metadata\"].get(\"min_length\"),\n",
        "#         x[\"eval_metadata\"].get(\"max_length\"),\n",
        "#     ),\n",
        "#     axis=1,\n",
        "# )\n",
        "\n",
        "# # Score completeness\n",
        "# data[\"completeness_score\"] = data.apply(\n",
        "#     lambda x: score_llm(x[\"prompt\"], x[\"completion\"], criteria_completeness), axis=1\n",
        "# )\n",
        "\n",
        "# # Score correctness\n",
        "# data[\"correctness_score\"] = data.apply(\n",
        "#     lambda x: score_llm(x[\"prompt\"], x[\"completion\"], criteria_correctness), axis=1\n",
        "# )\n",
        "\n",
        "# # Score conciseness\n",
        "# data[\"conciseness_score\"] = data.apply(\n",
        "#     lambda x: score_llm(x[\"prompt\"], x[\"completion\"], criteria_conciseness), axis=1\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "4gJypAVI13ws",
        "outputId": "67aae9c3-02eb-4a1f-efcb-ba6c4781d18c"
      },
      "outputs": [],
      "source": [
        "# Let's examine the final evaluation results\n",
        "#data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bOUjcce13ws"
      },
      "source": [
        "Finally, let's plot the average scores per critiera."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "J_MeQOWX13ws",
        "outputId": "8cedc547-6311-4700-c873-5dbbcbc6f52d"
      },
      "outputs": [],
      "source": [
        "# avg_scores = data[[\"format_score\", \"length_score\", \"completeness_score\", \"correctness_score\", \"conciseness_score\"]].mean()\n",
        "# ax = avg_scores.plot.bar(title=\"Average Scores\", xlabel=\"Criteria\", ylabel=\"Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
