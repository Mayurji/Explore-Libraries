{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc458b76",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c278fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from docling_core.transforms.chunker.hierarchical_chunker import (\n",
    "    ChunkingDocSerializer,\n",
    "    ChunkingSerializerProvider,\n",
    ")\n",
    "from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "from docling_core.transforms.serializer.markdown import MarkdownTableSerializer\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.data_structs import Node\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.schema import NodeWithScore, TransformComponent\n",
    "from llama_index.core.vector_stores import MetadataFilter, MetadataFilters\n",
    "from llama_index.core.vector_stores.types import VectorStoreQueryMode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.readers.elasticsearch import ElasticsearchReader\n",
    "from llama_index.vector_stores.opensearch import (\n",
    "    OpensearchVectorClient,\n",
    "    OpensearchVectorStore,\n",
    ")\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f7c67",
   "metadata": {},
   "source": [
    "### Check if GPU is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "print(f\"CUDA GPU is enabled: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358676d1",
   "metadata": {},
   "source": [
    "### Run Local OpenSearch Instance\n",
    "\n",
    "ðŸ’¡The version of the OpenSearch instance needs to be compatible with the version of the OpenSearch Python Client library, since this library is used by the LlamaIndex framework, which we leverage in this notebook.\n",
    "\n",
    "```\n",
    "docker run \\\n",
    "    -it \\\n",
    "    --pull always \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9600:9600 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e DISABLE_INSTALL_DEMO_CONFIG=true \\\n",
    "    -e DISABLE_SECURITY_PLUGIN=true \\\n",
    "    --name opensearch-node \\\n",
    "    -d opensearchproject/opensearch:3.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6f2f15",
   "metadata": {},
   "source": [
    "### Verify OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86683432",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"http://localhost:9200\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d7ded9",
   "metadata": {},
   "source": [
    "### Set up OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132b39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http endpoint for your cluster\n",
    "OPENSEARCH_ENDPOINT = \"http://localhost:9200\"\n",
    "# index to store the Docling document vectors\n",
    "OPENSEARCH_INDEX = \"docling-index\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8ac89",
   "metadata": {},
   "source": [
    "### Set Up Language Model (HuggingFace and Ollama)\n",
    "\n",
    "- Embedding Model: IBM's Granite Embedding 30M for embedding generation.\n",
    "- LLM: IBM's Granite 3.1 MoE for model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the embedding model\n",
    "EMBED_MODEL = HuggingFaceEmbedding(\n",
    "    model_name=\"ibm-granite/granite-embedding-30m-english\"\n",
    ")\n",
    "# maximum chunk size in tokens\n",
    "EMBED_MAX_TOKENS = 200\n",
    "# the generation model\n",
    "GEN_MODEL = Ollama(\n",
    "    model=\"granite3.1-moe\",\n",
    "    request_timeout=120.0,\n",
    "    # Manually set the context window to limit memory usage\n",
    "    context_window=8000,\n",
    "    # Set temperature to 0 for reproducibility of the results\n",
    "    temperature=0.0,\n",
    ")\n",
    "# a sample document\n",
    "SOURCE = \"https://arxiv.org/pdf/2408.09869\"\n",
    "\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"hi\"))\n",
    "print(f\"The embedding dimension is {embed_dim}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc84a52",
   "metadata": {},
   "source": [
    "### Process Data Using DoclingÂ¶\n",
    "\n",
    "A single PDF file is processed by Hybrid chunker to generate structured, hierarchical chunks suitable for downstream RAG tasks.\n",
    "\n",
    "We will convert the original PDF file into a DoclingDocument format using a DoclingReader object. We specify the JSON export type to retain the document hierarchical structure as an input for the next step (chunking the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f487f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Directory /Home/mayur/Downloads/2408.09869v5.pdf does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m tmp_dir_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Home/mayur/Downloads/2408.09869v5.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m reader \u001b[38;5;241m=\u001b[39m DoclingReader(export_type\u001b[38;5;241m=\u001b[39mDoclingReader\u001b[38;5;241m.\u001b[39mExportType\u001b[38;5;241m.\u001b[39mJSON)\n\u001b[0;32m----> 9\u001b[0m dir_reader \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleDirectoryReader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtmp_dir_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# load the PDF files\u001b[39;00m\n\u001b[1;32m     15\u001b[0m documents \u001b[38;5;241m=\u001b[39m dir_reader\u001b[38;5;241m.\u001b[39mload_data()\n",
      "File \u001b[0;32m~/rag_opensearch/lib/python3.10/site-packages/llama_index/core/readers/file/base.py:295\u001b[0m, in \u001b[0;36mSimpleDirectoryReader.__init__\u001b[0;34m(self, input_dir, input_files, exclude, exclude_hidden, exclude_empty, errors, recursive, encoding, filename_as_id, required_exts, file_extractor, num_files_limit, file_metadata, raise_on_error, fs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_dir:\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39misdir(input_dir):\n\u001b[0;32m--> 295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_dir \u001b[38;5;241m=\u001b[39m _Path(input_dir)\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude \u001b[38;5;241m=\u001b[39m exclude\n",
      "\u001b[0;31mValueError\u001b[0m: Directory /Home/mayur/Downloads/2408.09869v5.pdf does not exist."
     ]
    }
   ],
   "source": [
    "tmp_dir_path = Path(mkdtemp())\n",
    "req = requests.get(SOURCE)\n",
    "with open(tmp_dir_path / f\"{Path(SOURCE).name}.pdf\", \"wb\") as out_file:\n",
    "    out_file.write(req.content)\n",
    "\n",
    "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)\n",
    "dir_reader = SimpleDirectoryReader(\n",
    "    input_dir=tmp_dir_path,\n",
    "    file_extractor={\".pdf\": reader},\n",
    ")\n",
    "\n",
    "# load the PDF files\n",
    "documents = dir_reader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b44d7",
   "metadata": {},
   "source": [
    "### Load data into OpenSearch\n",
    "\n",
    "Before loading the data into open search, we have to transform the data.\n",
    "\n",
    "- DoclingNodeParser: It executes the document-based chunking with the hybrid chunker, which leverages the tokenizer of the embedding model to ensure that the resulting chunks fit within the model input text limit.\n",
    "\n",
    "- MetadataTransform: It is a custom transformation to ensure that generated chunk metadata is best formatted for indexing with OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e6dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hybrid chunker\n",
    "tokenizer = HuggingFaceTokenizer(\n",
    "    tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL.model_name),\n",
    "    max_tokens=EMBED_MAX_TOKENS,\n",
    ")\n",
    "chunker = HybridChunker(tokenizer=tokenizer)\n",
    "\n",
    "# create a Docling node parser\n",
    "node_parser = DoclingNodeParser(chunker=chunker)\n",
    "\n",
    "\n",
    "# create a custom transformation to avoid out-of-range integers\n",
    "class MetadataTransform(TransformComponent):\n",
    "    def __call__(self, nodes, **kwargs):\n",
    "        for node in nodes:\n",
    "            binary_hash = node.metadata.get(\"origin\", {}).get(\"binary_hash\", None)\n",
    "            if binary_hash is not None:\n",
    "                node.metadata[\"origin\"][\"binary_hash\"] = str(binary_hash)\n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46da8e9d",
   "metadata": {},
   "source": [
    "### Embed and Insert Data\n",
    "\n",
    "Using opensearch vector client, we embed the document into the index.\n",
    "\n",
    "The key action takes place in VectorStoreIndex:\n",
    "\n",
    "- Read document\n",
    "- Apply transformation\n",
    "- Generating embeddings and indexing them to vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed9938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpensearchVectorClient stores text in this field by default\n",
    "text_field = \"content\"\n",
    "# OpensearchVectorClient stores embeddings in this field by default\n",
    "embed_field = \"embedding\"\n",
    "\n",
    "client = OpensearchVectorClient(\n",
    "    endpoint=OPENSEARCH_ENDPOINT,\n",
    "    index=OPENSEARCH_INDEX,\n",
    "    dim=embed_dim,\n",
    "    engine=\"faiss\",\n",
    "    embedding_field=embed_field,\n",
    "    text_field=text_field,\n",
    ")\n",
    "\n",
    "vector_store = OpensearchVectorStore(client)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    transformations=[node_parser, MetadataTransform()],\n",
    "    storage_context=storage_context,\n",
    "    embed_model=EMBED_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fac343",
   "metadata": {},
   "source": [
    "### Building RAG\n",
    "\n",
    "Assemble RAG system, execute a query and get the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed98dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console(width=88)\n",
    "\n",
    "QUERY = \"Which are the main AI models in Docling?\"\n",
    "query_engine = index.as_query_engine(llm=GEN_MODEL)\n",
    "res = query_engine.query(QUERY)\n",
    "\n",
    "console.print(f\"ðŸ‘¤: {QUERY}\\nðŸ¤–: {res.response.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea878a1",
   "metadata": {},
   "source": [
    "### Custom Serializer\n",
    "\n",
    "Docling can extract the table content and process it for chunking, like other text elements.\n",
    "\n",
    "In the following example, the response is generated from a retrieved chunk containing a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db65df",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"What is the time to solution with the native backend on Intel?\"\n",
    "query_engine = index.as_query_engine(llm=GEN_MODEL)\n",
    "res = query_engine.query(QUERY)\n",
    "console.print(f\"ðŸ‘¤: {QUERY}\\nðŸ¤–: {res.response.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7ac2e",
   "metadata": {},
   "source": [
    "The result above was generated with the table serialized in a triplet format. Language models may perform better on complex tables if the structure is represented in a format that is widely adopted, like markdown.\n",
    "\n",
    "For this purpose, we can leverage a custom serializer that transforms tables in markdown format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568b4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDTableSerializerProvider(ChunkingSerializerProvider):\n",
    "    def get_serializer(self, doc):\n",
    "        return ChunkingDocSerializer(\n",
    "            doc=doc,\n",
    "            # configuring a different table serializer\n",
    "            table_serializer=MarkdownTableSerializer(),\n",
    "        )\n",
    "\n",
    "\n",
    "# clear the database from the previous chunks\n",
    "client.clear()\n",
    "vector_store.clear()\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    tokenizer=tokenizer,\n",
    "    max_tokens=EMBED_MAX_TOKENS,\n",
    "    serializer_provider=MDTableSerializerProvider(),\n",
    ")\n",
    "node_parser = DoclingNodeParser(chunker=chunker)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    transformations=[node_parser, MetadataTransform()],\n",
    "    storage_context=storage_context,\n",
    "    embed_model=EMBED_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba70424",
   "metadata": {},
   "source": [
    "**More accurate results after Custom Serialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(llm=GEN_MODEL)\n",
    "res = query_engine.query(QUERY)\n",
    "console.print(f\"ðŸ‘¤: {QUERY}\\nðŸ¤–: {res.response.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180893ff",
   "metadata": {},
   "source": [
    "### Filter Context Query\n",
    "\n",
    "Since we are storing chunks in hierarchical structure, we can leverage document structure using Docling to improve the RAG \\\n",
    "performance for both retrieval and for answering questions.\n",
    "\n",
    "For example, we can use chunk metadata with layout information to run queries in a filter context, for high retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d038ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_nodes(nodes):\n",
    "    res = []\n",
    "    for idx, item in enumerate(nodes):\n",
    "        doc_res = {\"k\": idx + 1, \"score\": item.score, \"text\": item.text, \"items\": []}\n",
    "        doc_items = item.metadata[\"doc_items\"]\n",
    "        for doc in doc_items:\n",
    "            doc_res[\"items\"].append({\"ref\": doc[\"self_ref\"], \"label\": doc[\"label\"]})\n",
    "        res.append(doc_res)\n",
    "    pprint(res, max_string=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d1426",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "QUERY = \"How does pypdfium perform?\"\n",
    "nodes = retriever.retrieve(QUERY)\n",
    "\n",
    "print(QUERY)\n",
    "display_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43747c8",
   "metadata": {},
   "source": [
    "**Restrict the retrieval to only those chunks containing tabular data to retrieve more quantitative information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af7ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = MetadataFilters(\n",
    "    filters=[MetadataFilter(key=\"doc_items.label\", value=\"table\")]\n",
    ")\n",
    "\n",
    "table_retriever = index.as_retriever(filters=filters, similarity_top_k=1)\n",
    "nodes = table_retriever.retrieve(QUERY)\n",
    "\n",
    "print(QUERY)\n",
    "display_nodes(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e1627",
   "metadata": {},
   "source": [
    "### Hybrid Search Retrieval with RRFÂ¶\n",
    "\n",
    "It combines keyword and semantic search to improve search relevance. To avoid relying on traditional score normalization techniques, the reciprocal rank fusion (RRF) feature on hybrid search can significantly improve the relevance of the retrieved chunks in our RAG system.\n",
    "\n",
    "*Reciprocal Rank Fusion (RRF) is a powerful algorithm that merges ranked search results from multiple retrieval methods (like keyword and semantic search) into a single, superior list by focusing on rank positions rather than raw scores.*\n",
    "\n",
    "First, create a search pipeline and specify RRF as technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb5cf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"{OPENSEARCH_ENDPOINT}/_search/pipeline/rrf-pipeline\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "body = {\n",
    "    \"description\": \"Post processor for hybrid RRF search\",\n",
    "    \"phase_results_processors\": [\n",
    "        {\"score-ranker-processor\": {\"combination\": {\"technique\": \"rrf\"}}}\n",
    "    ],\n",
    "}\n",
    "\n",
    "response = requests.put(url, json=body, headers=headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646b79b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_rrf = OpensearchVectorClient(\n",
    "    endpoint=OPENSEARCH_ENDPOINT,\n",
    "    index=f\"{OPENSEARCH_INDEX}-rrf\",\n",
    "    dim=embed_dim,\n",
    "    engine=\"faiss\",\n",
    "    embedding_field=embed_field,\n",
    "    text_field=text_field,\n",
    "    search_pipeline=\"rrf-pipeline\",\n",
    ")\n",
    "\n",
    "vector_store_rrf = OpensearchVectorStore(client_rrf)\n",
    "storage_context_rrf = StorageContext.from_defaults(vector_store=vector_store_rrf)\n",
    "index_hybrid = VectorStoreIndex.from_documents(\n",
    "    documents=documents,\n",
    "    transformations=[node_parser, MetadataTransform()],\n",
    "    storage_context=storage_context_rrf,\n",
    "    embed_model=EMBED_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa12d94",
   "metadata": {},
   "source": [
    "The first retriever, which entirely relies on semantic (vector) search, fails to catch the supporting chunk for the given question in the top 1 position. Note that we highlight few expected keywords for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700cbab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"Does Docling project provide a Dockerfile?\"\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes = retriever.retrieve(QUERY)\n",
    "exp = \"Docling also provides a Dockerfile\"\n",
    "start = \"[bold yellow]\"\n",
    "end = \"[/]\"\n",
    "for idx, item in enumerate(nodes):\n",
    "    console.print(\n",
    "        f\"*** k={idx + 1} ***\\n{item.text.strip().replace(exp, f'{start}{exp}{end}')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38594698",
   "metadata": {},
   "source": [
    "However, the retriever with the hybrid search pipeline effectively recognizes the key paragraph in the first position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43e9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_rrf = index_hybrid.as_retriever(\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID, similarity_top_k=3\n",
    ")\n",
    "nodes = retriever_rrf.retrieve(QUERY)\n",
    "for idx, item in enumerate(nodes):\n",
    "    console.print(\n",
    "        f\"*** k={idx + 1} ***\\n{item.text.strip().replace(exp, f'{start}{exp}{end}')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972fd8b",
   "metadata": {},
   "source": [
    "### Context Expansion\n",
    "\n",
    "**Small Chunks**\n",
    "\n",
    "- Increased retrieval precision\n",
    "- Answer question tightly focused.\n",
    "- Improves the accuracy, reduces hallucination, and speeds up inference.\n",
    "\n",
    "**But, it may overlook the contextual information**\n",
    "\n",
    "Docling by preserving the document structure, it enables us to employ various strategies to use context for more accurate RAG performance.\n",
    "\n",
    "For example, after identifying the most relevant chunk, you might include adjacent chunks from the same section as additional grounding material before generating the final answer.\n",
    "\n",
    "In the following example, the top retrieved chunks do not contain all the information that is required to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d381ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"According to the tests with arXiv and IBM Redbooks, which backend should I use if I have limited resources and complex tables?\"\n",
    "query_rrf = index_hybrid.as_query_engine(\n",
    "    vector_store_query_mode=VectorStoreQueryMode.HYBRID,\n",
    "    llm=GEN_MODEL,\n",
    "    similarity_top_k=3,\n",
    ")\n",
    "res = query_rrf.query(QUERY)\n",
    "console.print(f\"ðŸ‘¤: {QUERY}\\nðŸ¤–: {res.response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49817b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = retriever_rrf.retrieve(QUERY)\n",
    "for idx, item in enumerate(nodes):\n",
    "    console.print(\n",
    "        f\"*** k={idx + 1} ***\\n{item.text.strip().replace(exp, f'{start}{exp}{end}')}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e92f34",
   "metadata": {},
   "source": [
    "ðŸ’¡ In a production setting, it may be preferable to persist the parsed documents (i.e., DoclingDocument objects) as JSON in an object store or database and then fetch them when you need to traverse the document for contextâ€‘expansion scenarios. In this simplified example, however, we will query the OpenSearch index directly to obtain the required chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cb653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_headings = nodes[0].metadata[\"headings\"]\n",
    "top_text = nodes[0].text\n",
    "\n",
    "rdr = ElasticsearchReader(endpoint=OPENSEARCH_ENDPOINT, index=OPENSEARCH_INDEX)\n",
    "docs = rdr.load_data(\n",
    "    field=text_field,\n",
    "    query={\n",
    "        \"query\": {\n",
    "            \"terms_set\": {\n",
    "                \"metadata.headings.keyword\": {\n",
    "                    \"terms\": top_headings,\n",
    "                    \"minimum_should_match_script\": {\"source\": \"params.num_terms\"},\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")\n",
    "ext_nodes = []\n",
    "for idx, item in enumerate(docs):\n",
    "    if item.text == top_text:\n",
    "        ext_nodes.append(NodeWithScore(node=Node(text=item.text), score=1.0))\n",
    "        if idx > 0:\n",
    "            ext_nodes.append(\n",
    "                NodeWithScore(node=Node(text=docs[idx - 1].text), score=1.0)\n",
    "            )\n",
    "        if idx < len(docs) - 1:\n",
    "            ext_nodes.append(\n",
    "                NodeWithScore(node=Node(text=docs[idx + 1].text), score=1.0)\n",
    "            )\n",
    "        break\n",
    "\n",
    "synthesizer = get_response_synthesizer(llm=GEN_MODEL)\n",
    "res = synthesizer.synthesize(query=QUERY, nodes=ext_nodes)\n",
    "console.print(f\"ðŸ‘¤: {QUERY}\\nðŸ¤–: {res.response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa7b11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_opensearch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
