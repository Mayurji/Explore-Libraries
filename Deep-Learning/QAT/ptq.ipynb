{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68afb4e2",
   "metadata": {},
   "source": [
    "### Post Training Quantization\n",
    "\n",
    "PTQ is applied to a fully trained model without requiring any retraining. Itâ€™s simple and quick to implement, but may cause some degradation in model accuracy, especially when using aggressive quantization like int8.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Easy to integrate into existing workflows\n",
    "- No need to modify training code\n",
    "- Can dramatically reduce model size and inference cost\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- Accuracy may drop, especially for sensitive models or tasks\n",
    "- Works best on models that are already robust to small numeric changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e710628",
   "metadata": {},
   "source": [
    "**Loading CUDA Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43de0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_HOME=\"/usr/lib/cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5214e",
   "metadata": {},
   "source": [
    "**Import Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "246c6d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Device used: cuda\n",
      "Should skip CPU evaluations: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from packaging import version\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/pytorch/pytorch/issues/149829\n",
    "warnings.filterwarnings(\"ignore\", message=\".*TF32 acceleration on top of oneDNN is available for Intel GPUs. The current Torch version does not have Intel GPU Support.*\")\n",
    "\n",
    "# ignores irrelevant warning, see: https://github.com/tensorflow/tensorflow/issues/77293\n",
    "warnings.filterwarnings(\"ignore\", message=\".*erase_node(.*) on an already erased node.*\")\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device used: {device.type}\")\n",
    "\n",
    "skip_cpu = True # change to True to skip the slow checks on CPU\n",
    "print(f\"Should skip CPU evaluations: {skip_cpu}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7643a587",
   "metadata": {},
   "source": [
    "**Downloading and Preprocessing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54ded74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform),\n",
    "    batch_size=128, shuffle=True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform),\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "calibration_dataset = Subset(train_dataset, range(256))\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ead6e",
   "metadata": {},
   "source": [
    "**Download the ResNet18 Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93a8627d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet18_for_cifar10():\n",
    "    \"\"\"\n",
    "    Returns a ResNet-18 model adjusted for CIFAR-10:\n",
    "    - 3x3 conv with stride 1\n",
    "    - No max pooling\n",
    "    - 10 output classes\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=None, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model.to(device)\n",
    "\n",
    "model_to_quantize = get_resnet18_for_cifar10()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a7b04c",
   "metadata": {},
   "source": [
    "**Train and Evaluate Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b5fded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, epochs, lr=0.01, save_path=\"model.pth\", silent=False):\n",
    "    \"\"\"\n",
    "    Trains a model with SGD and cross-entropy loss.\n",
    "    Loads from save_path if it exists.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model.train()\n",
    "    except NotImplementedError:\n",
    "        torch.ao.quantization.move_exported_model_to_train(model)\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        if not silent:\n",
    "            print(f\"Model already trained. Loading from {save_path}\")\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "        return\n",
    "\n",
    "    # no saved model found. training from given model state\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if not silent:\n",
    "            print(f\"Epoch {epoch+1}: loss={loss.item():.4f}\")\n",
    "            evaluate(model, f\"Epoch {epoch+1}\")\n",
    "            try:\n",
    "                model.train()\n",
    "            except NotImplementedError:\n",
    "                torch.ao.quantization.move_exported_model_to_train(model)\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        if not silent:\n",
    "            print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "def evaluate(model, tag):\n",
    "    \"\"\"\n",
    "    Evaluates the model on test_loader and prints accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "    except NotImplementedError:\n",
    "        model = torch.ao.quantization.move_exported_model_to_eval(model)\n",
    "\n",
    "    model.to(device)\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Accuracy ({tag}): {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc644cd",
   "metadata": {},
   "source": [
    "**Utility Function**\n",
    "\n",
    "1. Switch between GPU and CPU evaluation\n",
    "2. Latency Estimation\n",
    "3. Determine Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e365ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    \"\"\"\n",
    "    A simple timer utility for measuring elapsed time in milliseconds.\n",
    "\n",
    "    Supports both GPU and CPU timing:\n",
    "    - If CUDA is available, uses torch.cuda.Event for accurate GPU timing.\n",
    "    - Otherwise, falls back to wall-clock CPU timing via time.time().\n",
    "\n",
    "    Methods:\n",
    "        start(): Start the timer.\n",
    "        stop(): Stop the timer and return the elapsed time in milliseconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.starter = torch.cuda.Event(enable_timing=True)\n",
    "            self.ender = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    def start(self):\n",
    "        if self.use_cuda:\n",
    "            self.starter.record()\n",
    "        else:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "    def stop(self):\n",
    "        if self.use_cuda:\n",
    "            self.ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            return self.starter.elapsed_time(self.ender)  # ms\n",
    "        else:\n",
    "            return (time.time() - self.start_time) * 1000  # ms\n",
    "\n",
    "def estimate_latency(model, example_inputs, repetitions=50):\n",
    "    \"\"\"\n",
    "    Returns avg and std inference latency (ms) over given runs.\n",
    "    \"\"\"\n",
    "    \n",
    "    timer = Timer()\n",
    "    timings = np.zeros((repetitions, 1))\n",
    "\n",
    "    # warm-up\n",
    "    for _ in range(5):\n",
    "        _ = model(example_inputs)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            timer.start()\n",
    "            _ = model(example_inputs)\n",
    "            elapsed = timer.stop()\n",
    "            timings[rep] = elapsed\n",
    "\n",
    "    return np.mean(timings), np.std(timings)\n",
    "\n",
    "def estimate_latency_full(model, tag, skip_cpu):\n",
    "    \"\"\"\n",
    "    Prints model latency on GPU and (optionally) CPU.\n",
    "    \"\"\"\n",
    "\n",
    "    # estimate latency on CPU\n",
    "    if not skip_cpu:\n",
    "        example_input = torch.rand(128, 3, 32, 32).cpu()\n",
    "        model.cpu()\n",
    "        latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "        print(f\"Latency ({tag}, on CPU): {latency_mu:.2f} Â± {latency_std:.2f} ms\")\n",
    "\n",
    "    # estimate latency on GPU\n",
    "    example_input = torch.rand(128, 3, 32, 32).cuda()\n",
    "    model.cuda()\n",
    "    latency_mu, latency_std = estimate_latency(model, example_input)\n",
    "    print(f\"Latency ({tag}, on GPU): {latency_mu:.2f} Â± {latency_std:.2f} ms\")\n",
    "\n",
    "def print_size_of_model(model, tag=\"\"):\n",
    "    \"\"\"\n",
    "    Prints model size (MB).\n",
    "    \"\"\"\n",
    "    \n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size_mb_full = os.path.getsize(\"temp.p\") / 1e6\n",
    "    print(f\"Size ({tag}): {size_mb_full:.2f} MB\")\n",
    "    os.remove(\"temp.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8d0bd3",
   "metadata": {},
   "source": [
    "**Train if model doesn't exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16e4286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model already trained. Loading from full_model.pth\n"
     ]
    }
   ],
   "source": [
    "train(model_to_quantize, train_loader, epochs=15, save_path=\"full_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d161c45",
   "metadata": {},
   "source": [
    "**Original Model: Size, Accuracy, Latency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "296477d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (full): 44.77 MB\n",
      "Accuracy (full): 80.47%\n",
      "Latency (full, on GPU): 19.00 Â± 0.29 ms\n"
     ]
    }
   ],
   "source": [
    "# get full model size\n",
    "print_size_of_model(model_to_quantize, \"full\")\n",
    "\n",
    "# evaluate full accuracy\n",
    "accuracy_full = evaluate(model_to_quantize, 'full')\n",
    "\n",
    "# estimate full model latency\n",
    "estimate_latency_full(model_to_quantize, 'full', skip_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6769fa7",
   "metadata": {},
   "source": [
    "### **Applying PTQ**\n",
    "\n",
    "- prepare_pt2e (to insert observers and fuse modules)\n",
    "     1) Fusing eligible layers (e.g., $\\text{Conv}+\\text{BN}$). 2) Inserting Observers (like $\\text{MinMaxObserver}$) at the output of layers to collect activation statistics (min/max values) during calibration. The result is prepared_model.\n",
    "- convert_pt2e (to replace observed modules with quantized equivalents)\n",
    "\n",
    "X86InductorQuantizer - It holds the quantization strategy (which layers to quantize, which data types to use, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "456c9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_pt2e import (\n",
    "  prepare_pt2e,\n",
    "  convert_pt2e,\n",
    ")\n",
    "\n",
    "import torch.ao.quantization.quantizer.x86_inductor_quantizer as xiq\n",
    "from torch.ao.quantization.quantizer.x86_inductor_quantizer import X86InductorQuantizer\n",
    "\n",
    "# batch of 128 images, each with 3 color channels and 32x32 resolution (CIFAR-10)\n",
    "example_inputs = (torch.rand(128, 3, 32, 32).to(device),)\n",
    "\n",
    "# export the model to a standardized format before quantization\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5\"): # for pytorch 2.5+\n",
    "    exported_model  = torch.export.export_for_training(model_to_quantize, example_inputs).module()\n",
    "else: # for pytorch 2.4\n",
    "    from torch._export import capture_pre_autograd_graph\n",
    "    exported_model = capture_pre_autograd_graph(model_to_quantize, example_inputs) \n",
    "\n",
    "# quantization setup for X86 Inductor Quantizer\n",
    "quantizer = X86InductorQuantizer()\n",
    "quantizer.set_global(xiq.get_default_x86_inductor_quantization_config())\n",
    "\n",
    "# preparing for PTQ by folding batch-norm into preceding conv2d operators, and inserting observers in appropriate places\n",
    "prepared_model = prepare_pt2e(exported_model, quantizer)\n",
    "\n",
    "# run inference on calibration data to collect activation stats needed for activation quantization\n",
    "def calibrate(model, data_loader):\n",
    "    torch.ao.quantization.move_exported_model_to_eval(model)\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            model(image.to(device))\n",
    "calibrate(prepared_model, calibration_loader)\n",
    "\n",
    "# converts calibrated model to a quantized model\n",
    "quantized_model = convert_pt2e(prepared_model)\n",
    "\n",
    "# export again to remove unused weights after quantization\n",
    "if version.parse(torch.__version__) >= version.parse(\"2.5\"): # for pytorch 2.5+\n",
    "    quantized_model = torch.export.export_for_training(quantized_model, example_inputs).module()\n",
    "else: # for pytorch 2.4\n",
    "    quantized_model = capture_pre_autograd_graph(quantized_model, example_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e24ca",
   "metadata": {},
   "source": [
    "### Results after quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73cec420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (quantized): 11.26 MB\n",
      "Accuracy (quantized): 80.51%\n",
      "Latency (quantized, on GPU): 35.77 Â± 0.37 ms\n"
     ]
    }
   ],
   "source": [
    "# get quantized model size\n",
    "print_size_of_model(quantized_model, \"quantized\")\n",
    "\n",
    "# evaluate quantized accuracy\n",
    "accuracy_full = evaluate(quantized_model, 'quantized')\n",
    "\n",
    "# estimate quantized model latency\n",
    "estimate_latency_full(quantized_model, 'quantized', skip_cpu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5e152e",
   "metadata": {},
   "source": [
    "### Optimizing Quantized Model\n",
    "\n",
    "It activates the C++ wrapper for TorchInductor. \\\n",
    "Instead of generating Python code to invoke the kernels and external kernels, the C++ wrapper generates C++ code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4f51ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable the use of the C++ wrapper for TorchInductor which reduces Python overhead\n",
    "import torch._inductor.config as config\n",
    "config.cpp_wrapper = True\n",
    "\n",
    "# compiles quantized model to generate optimized model\n",
    "with torch.no_grad():\n",
    "    optimized_model = torch.compile(quantized_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5fed11",
   "metadata": {},
   "source": [
    "### Results after Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b1199dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (optimized): 11.26 MB\n",
      "Accuracy (optimized): 79.29%\n",
      "Latency (optimized, on GPU): 32.55 Â± 0.38 ms\n"
     ]
    }
   ],
   "source": [
    "# get optimized model size\n",
    "print_size_of_model(optimized_model, \"optimized\")\n",
    "\n",
    "# evaluate optimized accuracy\n",
    "accuracy_full = evaluate(optimized_model, 'optimized')\n",
    "\n",
    "# estimate optimized model latency\n",
    "estimate_latency_full(optimized_model, 'optimized', skip_cpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0bab6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
