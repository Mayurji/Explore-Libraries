{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d3123c",
   "metadata": {},
   "source": [
    "### Code Chunker\n",
    "\n",
    "- Split code into chunks based on code structure\n",
    "\n",
    "- It splits code into chunks based on its structure, leveraging Abstract Syntax Trees (ASTs) to create contextually relevant segments.\n",
    "\n",
    "AST is a tree representation of the syntactic structure of source code, \\\n",
    "providing a hierarchial view of code elements like functions, variables, and control flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b02596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayur/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from chonkie import CodeChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = CodeChunker(\n",
    "    language=\"python\",                 # Specify the programming language\n",
    "    tokenizer_or_token_counter=\"gpt2\", # Tokenizer to use\n",
    "    chunk_size=512,                    # Maximum tokens per chunk\n",
    "    include_nodes=False                # Optionally include AST nodes in output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c69c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26293b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: \n",
      "mlflow.set_tracking_uri('http://localhost:8080')\n",
      "mlflow.set_experiment('Exp-1')\n",
      "\n",
      "with mlflow.start_run() as run:\n",
      "    x, y = make_regression(n_features=4, n_informative=2, random_state=42, shuffle=False)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
      "    params = {'max_depth': 1, 'random_state': 42}\n",
      "    model = RandomForestRegressor(**params)\n",
      "    model.fit(x_train, y_train)\n",
      "\n",
      "    y_pred = model.predict(x_test)\n",
      "    signature = infer_signature(x_test, y_pred)\n",
      "    \n",
      "    mlflow.log_params(params)\n",
      "    mlflow.log_metrics({'mse': mean_squared_error(y_test, y_pred)})\n",
      "\n",
      "    mlflow.sklearn.log_model(\n",
      "        sk_model=model,\n",
      "        artifact_path='sklearn-model-2',\n",
      "        signature=signature,\n",
      "        registered_model_name='sklearn-random-forest-reg'\n",
      "    )\n",
      "\n",
      "\n",
      "Token count: 351\n",
      "Language: None\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "mlflow.set_experiment('Exp-1')\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    x, y = make_regression(n_features=4, n_informative=2, random_state=42, shuffle=False)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    params = {'max_depth': 1, 'random_state': 42}\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    signature = infer_signature(x_test, y_pred)\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics({'mse': mean_squared_error(y_test, y_pred)})\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path='sklearn-model-2',\n",
    "        signature=signature,\n",
    "        registered_model_name='sklearn-random-forest-reg'\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "chunks = chunker.chunk(code)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Language: {chunk.language}\")\n",
    "    if chunk.nodes:\n",
    "        print(f\"Node count: {len(chunk.nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be8346",
   "metadata": {},
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce36038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=20,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba6d0d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: First paragraph about a Donkeys.\n",
      "Why donkey are colored black and white.\n",
      "\n",
      "Token count: 15\n",
      "Number of sentences: 2\n",
      "Chunk text: This paragraph is about Pigs.\n",
      "Pigs are pink and have curly tails.\n",
      "Token count: 14\n",
      "Number of sentences: 2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First paragraph about a Donkeys.\n",
    "Why donkey are colored black and white.\n",
    "This paragraph is about Pigs.\n",
    "Pigs are pink and have curly tails.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Number of sentences: {len(chunk.sentences)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce331e47",
   "metadata": {},
   "source": [
    "### SDPM Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62456335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SDPMChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SDPMChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                              # Similarity threshold (0-1)\n",
    "    chunk_size=50,                             # Maximum tokens per chunk\n",
    "    min_sentences=1,                            # Initial sentences per chunk\n",
    "    skip_window=1                               # Number of chunks to skip when looking for similarities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "489c53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: The neural network processes input data through layers.\n",
      "Training data is essential for model performance.\n",
      "GPUs accelerate neural network computations significantly.\n",
      "Quality training data improves model accuracy.\n",
      "TPUs provide specialized hardware for deep learning.\n",
      "\n",
      "Token count: 42\n",
      "Number of sentences: 5\n",
      "Chunk text: Data preprocessing is a crucial step in training.\n",
      "Token count: 12\n",
      "Number of sentences: 1\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The neural network processes input data through layers.\n",
    "Training data is essential for model performance.\n",
    "GPUs accelerate neural network computations significantly.\n",
    "Quality training data improves model accuracy.\n",
    "TPUs provide specialized hardware for deep learning.\n",
    "Data preprocessing is a crucial step in training.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Number of sentences: {len(chunk.sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f575d8",
   "metadata": {},
   "source": [
    "### Late Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd01dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "128b744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import LateChunker, RecursiveRules\n",
    "\n",
    "chunker = LateChunker(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    chunk_size=10,\n",
    "    rules=RecursiveRules(),\n",
    "    min_characters_per_chunk=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0196d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: First paragraph about a specific topic.\n",
      "\n",
      "Token count: 8\n",
      "Chunk text: Second paragraph continuing the same topic.\n",
      "\n",
      "Token count: 7\n",
      "Chunk text: Third paragraph switching to a different topic.\n",
      "\n",
      "Token count: 8\n",
      "Chunk text: Fourth paragraph expanding on the new topic.\n",
      "Token count: 9\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First paragraph about a specific topic.\n",
    "Second paragraph continuing the same topic.\n",
    "Third paragraph switching to a different topic.\n",
    "Fourth paragraph expanding on the new topic.\"\"\"\n",
    "\n",
    "chunks = chunker(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    \n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    #print(f\"Number of sentences: {len(chunk.sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3546d4",
   "metadata": {},
   "source": [
    "### Neural Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f72c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from chonkie import NeuralChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = NeuralChunker(\n",
    "    model=\"mirth/chonky_modernbert_base_1\",  # Default model\n",
    "    device_map=\"cuda\",                        # Device to run the model on ('cpu', 'cuda', etc.)\n",
    "    min_characters_per_chunk=10,             # Minimum characters for a chunk\n",
    "    return_type=\"chunks\"                     # Output type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905d0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: Limited context window - All LLMs have a limit on how much text they can process at once. This is referred to as the Context Window. Chunking helps in breaking down the large text document into processable tokens\n",
      "\n",
      "Token count: 48\n",
      "Start index: 0\n",
      "End index: 213\n",
      "Chunk text: Computational Efficiency - It is not possible to load a 100GB document every time you make a query. Attention mechanisms, even when optimized, are computationally expensive O(n). Chunking keeps things efficient and memory-friendly.\n",
      "\n",
      "Token count: 51\n",
      "Start index: 213\n",
      "End index: 445\n",
      "Chunk text: Better Representation - As mentioned earlier, chunks represent each idea as an independent entity. Not chunking your document will likely cause your model to conflate concepts and get confused. \n",
      "Representation models use lossy compression, so keeping chunks concise ensures the model understands the context better.\n",
      "\n",
      "Token count: 60\n",
      "Start index: 445\n",
      "End index: 761\n",
      "Chunk text: Reduced Hallucination - Feeding too much context at once causes the models to hallucinate. They start using irrelevant information to answer queries, and that's a big no-no. Smaller, focused chunks reduce this risk.\n",
      "Token count: 49\n",
      "Start index: 761\n",
      "End index: 976\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Limited context window - All LLMs have a limit on how much text they can process at once. This is referred to as the Context Window. Chunking helps in breaking down the large text document into processable tokens\n",
    "Computational Efficiency - It is not possible to load a 100GB document every time you make a query. Attention mechanisms, even when optimized, are computationally expensive O(n). Chunking keeps things efficient and memory-friendly.\n",
    "Better Representation - As mentioned earlier, chunks represent each idea as an independent entity. Not chunking your document will likely cause your model to conflate concepts and get confused. \n",
    "Representation models use lossy compression, so keeping chunks concise ensures the model understands the context better.\n",
    "Reduced Hallucination - Feeding too much context at once causes the models to hallucinate. They start using irrelevant information to answer queries, and that's a big no-no. Smaller, focused chunks reduce this risk.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\") # Note: token_count might be added post-hoc or not available depending on implementation\n",
    "    print(f\"Start index: {chunk.start_index}\")\n",
    "    print(f\"End index: {chunk.end_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bc669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
