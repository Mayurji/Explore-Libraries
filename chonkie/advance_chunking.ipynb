{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39d3123c",
   "metadata": {},
   "source": [
    "### Code Chunker\n",
    "\n",
    "- Split code into chunks based on code structure\n",
    "\n",
    "- It splits code into chunks based on its structure, leveraging Abstract Syntax Trees (ASTs) to create contextually relevant segments.\n",
    "\n",
    "AST is a tree representation of the syntactic structure of source code, \\\n",
    "providing a hierarchial view of code elements like functions, variables, and control flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20b02596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import CodeChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = CodeChunker(\n",
    "    language=\"python\",                 # Specify the programming language\n",
    "    tokenizer_or_token_counter=\"gpt2\", # Tokenizer to use\n",
    "    chunk_size=250,                    # Maximum tokens per chunk\n",
    "    include_nodes=False                # Optionally include AST nodes in output\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26293b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: \n",
      "mlflow.set_tracking_uri('http://localhost:8080')\n",
      "mlflow.set_experiment('Exp-1')\n",
      "\n",
      "with mlflow.start_run() as run:\n",
      "    x, y = make_regression(n_features=4, n_informative=2, random_state=42, shuffle=False)\n",
      "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
      "    params = {'max_depth': 1, 'random_state': 42}\n",
      "    model = RandomForestRegressor(**params)\n",
      "    model.fit(x_train, y_train)\n",
      "\n",
      "    y_pred = model.predict(x_test)\n",
      "    signature = infer_signature(x_test, y_pred)\n",
      "    \n",
      "    mlflow.log_params(params)\n",
      "    mlflow.log_metrics({'mse': mean_squared_error(y_test, y_pred)})\n",
      "\n",
      "    \n",
      "Token count: 222\n",
      "Language: None\n",
      "Chunk text: mlflow.sklearn.log_model(\n",
      "        sk_model=model,\n",
      "        artifact_path='sklearn-model-2',\n",
      "        signature=signature,\n",
      "        registered_model_name='sklearn-random-forest-reg'\n",
      "    )\n",
      "\n",
      "\n",
      "Token count: 85\n",
      "Language: None\n"
     ]
    }
   ],
   "source": [
    "code = \"\"\"\n",
    "mlflow.set_tracking_uri('http://localhost:8080')\n",
    "mlflow.set_experiment('Exp-1')\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    x, y = make_regression(n_features=4, n_informative=2, random_state=42, shuffle=False)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    params = {'max_depth': 1, 'random_state': 42}\n",
    "    model = RandomForestRegressor(**params)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    signature = infer_signature(x_test, y_pred)\n",
    "    \n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metrics({'mse': mean_squared_error(y_test, y_pred)})\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path='sklearn-model-2',\n",
    "        signature=signature,\n",
    "        registered_model_name='sklearn-random-forest-reg'\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "chunks = chunker.chunk(code)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Language: {chunk.language}\")\n",
    "    if chunk.nodes:\n",
    "        print(f\"Node count: {len(chunk.nodes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be8346",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "\n",
    "Split text into chunks based on semantic similarity\n",
    "\n",
    "The SemanticChunker splits text into chunks based on semantic similarity, ensuring that related content stays together in the same chunk. \\\n",
    "This approach is particularly useful for RAG applications where context preservation is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce36038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SemanticChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SemanticChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                               # Similarity threshold (0-1) or (1-100) or \"auto\"\n",
    "    chunk_size=20,                              # Maximum tokens per chunk\n",
    "    min_sentences=1                              # Initial sentences per chunk\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6d0d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: First paragraph about a Donkeys.\n",
      "Why donkey are colored black and white.\n",
      "\n",
      "Token count: 15\n",
      "Number of sentences: 2\n",
      "Chunk text: This paragraph is about Pigs.\n",
      "Pigs are pink and have curly tails.\n",
      "Token count: 14\n",
      "Number of sentences: 2\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First paragraph about a Donkeys.\n",
    "Why donkey are colored black and white.\n",
    "This paragraph is about Pigs.\n",
    "Pigs are pink and have curly tails.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Number of sentences: {len(chunk.sentences)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce331e47",
   "metadata": {},
   "source": [
    "### SDPM Chunker\n",
    "\n",
    "Split text using Semantic Double-Pass Merging for improved context preservation\n",
    "\n",
    "The SDPMChunker extends semantic chunking by using a double-pass merging approach. \\\n",
    "It first groups content by semantic similarity, then merges similar groups within a skip window, \\\n",
    "allowing it to connect related content that may not be consecutive in the text. This technique is \\\n",
    "particularly useful for documents with recurring themes or concepts spread apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62456335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chonkie import SDPMChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = SDPMChunker(\n",
    "    embedding_model=\"minishlab/potion-base-8M\",  # Default model\n",
    "    threshold=0.5,                              # Similarity threshold (0-1)\n",
    "    chunk_size=50,                             # Maximum tokens per chunk\n",
    "    min_sentences=1,                            # Initial sentences per chunk\n",
    "    skip_window=1                               # Number of chunks to skip when looking for similarities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "489c53e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: The neural network processes input data through layers.\n",
      "Training data is essential for model performance.\n",
      "\n",
      "Token count: 17\n",
      "Number of sentences: 2\n",
      "\n",
      "\n",
      "Chunk text: GPUs accelerate neural network computations significantly.\n",
      "Quality training data improves model accuracy.\n",
      "\n",
      "Token count: 16\n",
      "Number of sentences: 2\n",
      "\n",
      "\n",
      "Chunk text: TPUs provide specialized hardware for deep learning.\n",
      "\n",
      "Token count: 9\n",
      "Number of sentences: 1\n",
      "\n",
      "\n",
      "Chunk text: Data preprocessing is a crucial step in training.\n",
      "Token count: 12\n",
      "Number of sentences: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"The neural network processes input data through layers.\n",
    "Training data is essential for model performance.\n",
    "GPUs accelerate neural network computations significantly.\n",
    "Quality training data improves model accuracy.\n",
    "TPUs provide specialized hardware for deep learning.\n",
    "Data preprocessing is a crucial step in training.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print(f\"Number of sentences: {len(chunk.sentences)}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f575d8",
   "metadata": {},
   "source": [
    "### Late Chunker\n",
    "\n",
    "Split text into chunks based on a late-bound token count\n",
    "\n",
    "The LateChunker implements the late chunking strategy described in the Late Chunking paper. \\\n",
    "It builds on top of the RecursiveChunker and uses document-level embeddings to create more \\\n",
    "semantically rich chunk representations.\n",
    "\n",
    "Instead of generating embeddings for each chunk independently, the LateChunker first encodes \\\n",
    "the entire text into a single embedding. It then splits the text using recursive rules and derives \\\n",
    "each chunkâ€™s embedding by averaging relevant parts of the full document embedding. This allows \\\n",
    "each chunk to carry broader contextual information, improving retrieval performance in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fd01dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128b744d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 19:12:13.236055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750599733.739143   11165 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750599733.862928   11165 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750599734.913775   11165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750599734.913825   11165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750599734.913829   11165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750599734.913832   11165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-22 19:12:15.030072: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from chonkie import LateChunker, RecursiveRules\n",
    "\n",
    "chunker = LateChunker(\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    chunk_size=10,\n",
    "    rules=RecursiveRules(),\n",
    "    min_characters_per_chunk=24,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0196d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: First paragraph about a specific topic.\n",
      "\n",
      "Token count: 8\n",
      "\n",
      "\n",
      "Chunk text: Second paragraph continuing the same topic.\n",
      "\n",
      "Token count: 7\n",
      "\n",
      "\n",
      "Chunk text: Third paragraph switching to a different topic.\n",
      "\n",
      "Token count: 8\n",
      "\n",
      "\n",
      "Chunk text: Fourth paragraph expanding on the new topic.\n",
      "Token count: 9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"First paragraph about a specific topic.\n",
    "Second paragraph continuing the same topic.\n",
    "Third paragraph switching to a different topic.\n",
    "Fourth paragraph expanding on the new topic.\"\"\"\n",
    "\n",
    "chunks = chunker(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    \n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\")\n",
    "    print('\\n')\n",
    "    #print(f\"Number of sentences: {len(chunk.sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3546d4",
   "metadata": {},
   "source": [
    "### Neural Chunker\n",
    "\n",
    "Split text using a fine-tuned BERT model to detect semantic shifts\n",
    "\n",
    "The NeuralChunker leverages the power of deep learning! It uses a fine-tuned BERT \\\n",
    "model specifically trained to identify semantic shifts within text, allowing it to \\\n",
    "split documents at points where the topic or context changes significantly. This \\\n",
    "provides highly coherent chunks ideal for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f72c628",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "from chonkie import NeuralChunker\n",
    "\n",
    "# Basic initialization with default parameters\n",
    "chunker = NeuralChunker(\n",
    "    model=\"mirth/chonky_modernbert_base_1\",  # Default model\n",
    "    device_map=\"cuda\",                        # Device to run the model on ('cpu', 'cuda', etc.)\n",
    "    min_characters_per_chunk=10,             # Minimum characters for a chunk\n",
    "    return_type=\"chunks\"                     # Output type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "905d0792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0622 19:14:46.607000 11165 torch/_inductor/utils.py:1137] [1/0] Not enough SMs to use max_autotune_gemm mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk text: Limited context windowâ€Š-â€ŠAll LLMs have a limit on how much text they can process at once. This is referred to as the Context Window. Chunking helps in breaking down the large text document into processable tokens\n",
      "\n",
      "Token count: 48\n",
      "Start index: 0\n",
      "End index: 213\n",
      "\n",
      "\n",
      "Chunk text: Computational Efficiencyâ€Š-â€ŠIt is not possible to load a 100GB document every time you make a query. Attention mechanisms, even when optimized, are computationally expensive O(n). Chunking keeps things efficient and memory-friendly.\n",
      "\n",
      "Token count: 51\n",
      "Start index: 213\n",
      "End index: 445\n",
      "\n",
      "\n",
      "Chunk text: Better Representationâ€Š-â€ŠAs mentioned earlier, chunks represent each idea as an independent entity. Not chunking your document will likely cause your model to conflate concepts and get confused.Â \n",
      "Representation models use lossy compression, so keeping chunks concise ensures the model understands the context better.\n",
      "\n",
      "Token count: 60\n",
      "Start index: 445\n",
      "End index: 761\n",
      "\n",
      "\n",
      "Chunk text: Reduced Hallucinationâ€Š-â€ŠFeeding too much context at once causes the models to hallucinate. They start using irrelevant information to answer queries, and that's a big no-no. Smaller, focused chunks reduce this risk.\n",
      "Token count: 49\n",
      "Start index: 761\n",
      "End index: 976\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Limited context windowâ€Š-â€ŠAll LLMs have a limit on how much text they can process at once. This is referred to as the Context Window. Chunking helps in breaking down the large text document into processable tokens\n",
    "Computational Efficiencyâ€Š-â€ŠIt is not possible to load a 100GB document every time you make a query. Attention mechanisms, even when optimized, are computationally expensive O(n). Chunking keeps things efficient and memory-friendly.\n",
    "Better Representationâ€Š-â€ŠAs mentioned earlier, chunks represent each idea as an independent entity. Not chunking your document will likely cause your model to conflate concepts and get confused.Â \n",
    "Representation models use lossy compression, so keeping chunks concise ensures the model understands the context better.\n",
    "Reduced Hallucinationâ€Š-â€ŠFeeding too much context at once causes the models to hallucinate. They start using irrelevant information to answer queries, and that's a big no-no. Smaller, focused chunks reduce this risk.\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(f\"Chunk text: {chunk.text}\")\n",
    "    print(f\"Token count: {chunk.token_count}\") # Note: token_count might be added post-hoc or not available depending on implementation\n",
    "    print(f\"Start index: {chunk.start_index}\")\n",
    "    print(f\"End index: {chunk.end_index}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bc669",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
